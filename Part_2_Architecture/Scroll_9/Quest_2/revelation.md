# **Архитектура Искусственного Нейрона: Всесторонний Анализ Реализации Пользовательских Модулей в PyTorch**

## **Аннотация**

Переход от императивного, скриптового описания нейронных сетей к объектно-ориентированной архитектуре знаменует собой фундаментальный этап созревания инженера глубокого обучения (Deep Learning). Данный отчет представляет собой исчерпывающий технический анализ процесса создания пользовательского слоя Linear — "атома" глубокого обучения — в рамках фреймворка PyTorch. Инициированный теоретическими условиями задачи "Квест 9.2", данный документ выходит за рамки игровой легенды о "магических артефактах", предоставляя строгий инженерный разбор внутренней механики класса torch.nn.Module.

Мы детально деконструируем процесс инкапсуляции обучаемых параметров, анализируем архитектурные причины специфической раскладки памяти весовых матриц, исследуем статистическую теорию инициализации параметров и динамику выполнения прямого прохода (forward pass). Особое внимание уделяется механизмам автоматической регистрации параметров через метапрограммирование Python, что отличает "самосознающие" тензоры от простых контейнеров данных. Отчет служит фундаментальным руководством для архитекторов нейронных сетей, стремящихся не просто использовать готовые блоки, но и создавать собственные, математически корректные и оптимизированные вычислительные модули.

## ---

**1\. Введение: Парадигма Модульного Глубокого Обучения**

### **1.1 Эволюция от Скриптов к Объектам**

На ранних этапах развития вычислительных нейронаук и машинного обучения модели часто конструировались как монолитные скрипты. Операции определялись последовательно, а параметры управлялись как разрозненные коллекции массивов или тензоров. Такой подход, будучи приемлемым для простейших перцептронов, коллапсирует под тяжестью сложности современных архитектур, таких как Трансформеры или Глубокие Остаточные Сети (ResNet). Сдвиг в сторону объектно-ориентированного программирования (ООП) в таких фреймворках, как PyTorch, является не стилистическим выбором, а структурной необходимостью. Он обеспечивает **инкапсуляцию** — связывание состояния (обучаемых параметров, таких как веса и смещения) с поведением (математическими операциями, трансформирующими входные данные).1

В контексте запроса пользователя ("Квест 9.2"), создание собственного класса MyLinear представляет собой акт создания "Многоразового Артефакта". С инженерной точки зрения, это переход от процедурного кода к созданию переиспользуемых программных компонентов. Класс nn.Module в PyTorch служит базовым чертежом для всех нейросетевых модулей. Когда инженер наследуется от nn.Module, он не просто пишет класс; он подключается к сложной инфраструктуре, предназначенной для управления жизненным циклом нейронного компонента. Это включает в себя отслеживание градиентов, перемещение вычислений между устройствами (CPU и GPU), сохранение и загрузку словарей состояний (state_dict), а также интерфейс взаимодействия с алгоритмами оптимизации.3

### **1.2 Линейный Слой как Фундаментальный Блок**

Линейный слой (или полносвязный слой, Dense layer) является фундаментальным строительным блоком параметрического глубокого обучения. Математически он представляет собой аффинное преобразование. Несмотря на свою концептуальную простоту — $y \= xW^T \+ b$ — его реализация "с нуля" заставляет столкнуться с критически важными инженерными решениями:

- **Управление Состоянием:** Как фреймворк узнает, какие тензоры необходимо обновлять в процессе обратного распространения ошибки (backpropagation)?
- **Раскладка Памяти:** Почему матрицы весов хранятся в транспонированном виде $(Out, In)$, а не в алгебраически привычном $(In, Out)$?
- **Инициализация:** Каким образом задаются начальные значения "рун" (параметров), чтобы предотвратить затухание или взрыв градиентов?

Данный отчет использует реализацию класса MyLinear как призму для рассмотрения этих широких архитектурных принципов, отвечая на вопросы, поставленные в "легенде квеста", и углубляясь в технические детали, скрытые за абстракциями высокого уровня.

## ---

**2\. Анатомия nn.Module: Внутренняя Механика Регистрации**

Чтобы понять, как работает "простейший нейрон", необходимо сначала изучить контейнер, который его содержит. nn.Module часто описывают как "контейнер", но технически это динамическая система реестров, управляемая метапрограммированием.

### **2.1 Инициализация и Внутренние Реестры**

Когда пользовательский класс наследуется от nn.Module, он наследует набор внутренних словарей, которые отслеживают состояние объекта. Эти словари не видны в стандартном коде \_\_init\_\_, написанном пользователем, но они инициализируются вызовом super().\_\_init\_\_() — "ритуалом", который является обязательным для корректного функционирования модуля.1 Без этого вызова механизм отслеживания параметров не будет активирован, и добавление атрибутов вызовет ошибки AttributeError при попытке доступа к внутренним структурам.

Три критически важных внутренних реестра включают:

1. **\_parameters**: Словарь (OrderedDict), который хранит обучаемые веса и смещения. Именно эти тензоры возвращаются при вызове model.parameters(), что позволяет оптимизатору "видеть" их.
2. **\_modules**: Словарь, отслеживающий дочерние модули. Если слой Linear помещен внутри контейнера Sequential или является атрибутом другого сложного модуля (например, слоя энкодера), он регистрируется здесь. Это обеспечивает рекурсивную структуру нейронных сетей (дерево модулей).2
3. **\_buffers**: Словарь для тензоров, которые являются частью состояния модели, но не являются обучаемыми параметрами (например, бегущие средние и дисперсии в слоях Batch Normalization).5

### **2.2 Магия \_\_setattr\_\_: Автоматическая Регистрация**

Одним из самых глубоких "заклинаний" в PyTorch является перегрузка метода \_\_setattr\_\_ внутри класса nn.Module. В стандартном Python присваивание значения атрибуту (например, self.x \= 1\) просто сохраняет значение в \_\_dict\_\_ объекта. Однако nn.Module перехватывает каждое присваивание.5

В контексте кода из запроса пользователя:

Python

self.weight \= nn.Parameter(torch.randn(output_size, input_size))

Метод \_\_setattr\_\_ выполняет проверку типа присваиваемого объекта:

1. **Является ли значение экземпляром nn.Parameter?** Если да, оно автоматически добавляется в словарь \_parameters под ключом "weight". Эта автоматическая регистрация — именно то, что позволяет оптимизатору позже "найти" веса без явной передачи их пользователем.
2. **Является ли значение экземпляром nn.Module?** Если да, оно добавляется в \_modules.
3. **Является ли это буфером?** Если тензор зарегистрирован через register_buffer, он попадает в \_buffers.
4. **Является ли это обычным объектом (int, list, простой Tensor)?** Он сохраняется как стандартный атрибут и **игнорируется** методами parameters() и state_dict().7

Этот механизм объясняет ответ "Мастера" на вопрос "Техноманта" в условии задачи. Если бы пользователь присвоил обычный torch.Tensor атрибуту self.weight, \_\_setattr\_\_ воспринял бы его как обычный атрибут данных. Он не попал бы в \_parameters, и, следовательно, вызов model.parameters() вернул бы пустой итератор. Оптимизатор, который итерируется по параметрам модели, не получил бы ссылок на тензоры весов, и обучение (обновление весов) стало бы невозможным — Голем остался бы "безмозглым".8

### **2.3 Рекурсия и Управление Устройствами**

Иерархическая структура, управляемая через \_modules, позволяет выполнять мощные рекурсивные операции. Методы, такие как .to(device), .float(), .eval() и .apply(fn), полагаются на обход этого дерева.

Например, когда вызывается model.cuda(), команда не просто применяется к объекту верхнего уровня. Модуль итерируется по \_parameters, чтобы переместить собственные веса в память GPU, а затем рекурсивно вызывает .cuda() для каждого дочернего модуля в \_modules.3 Эта рекурсивная пропагация гарантирует, что сложные архитектуры, состоящие из вложенных блоков, могут управляться как единая сущность. В контексте пользовательского класса MyLinear, корректное наследование от nn.Module гарантирует, что созданный "Рунный Камень" будет совместим с этой экосистемой управления ресурсами.

## ---

**3\. Природа Обучаемых Параметров: torch.Tensor vs nn.Parameter**

Различие между torch.Tensor и torch.nn.Parameter является центральной темой в понимании архитектуры PyTorch и прямым ответом на вопрос "Техноманта" в условии задачи.

### **3.1 Семантическое и Функциональное Различие**

torch.Tensor — это многомерный массив, содержащий данные. nn.Parameter — это **подкласс** Tensor, который несет специфическую семантическую нагрузку в контексте Модуля.9

- **Поведенческое различие:** Как отмечено в анализе \_\_setattr\_\_, основное различие заключается в том, как nn.Module обрабатывает эти объекты. Параметры автоматически повышаются до статуса "обучаемых компонентов".
- **Атрибут requires_grad:** По умолчанию, при создании nn.Parameter, PyTorch устанавливает requires_grad=True. Хотя стандартный тензор также может иметь requires_grad=True, обертывание его в Parameter сигнализирует о намерении. Это декларация: "Этот тензор является переменной в задаче оптимизации".
- **Явность (Explicitness):** Использование Parameter обеспечивает разделение ответственности. Оно отделяет данные (входы/выходы, которые являются Тензорами) от модели (веса/смещения, которые являются Параметрами). Это улучшает читаемость и поддерживаемость кода.11

### **3.2 Градиентная История и Autograd**

Когда Параметр участвует в вычислении (прямой проход), движок Autograd в PyTorch строит динамический вычислительный граф. Каждая операция создает объект grad_fn (функцию градиента), который связывает выход обратно с входом.

Поскольку self.weight в классе MyLinear является Параметром с requires_grad=True, любой тензор, полученный в результате операции x @ self.weight.T, также будет требовать градиенты. Во время обратного прохода (loss.backward()), движок проходит по этому графу в обратном порядке, вычисляя частные производные с помощью цепного правила (Chain Rule) и аккумулируя их в атрибуте .grad соответствующего Параметра.

Если бы мы использовали обычный тензор без requires_grad=True, граф вычислений прервался бы на этом узле, и градиенты не вычислялись бы. Если бы мы использовали тензор с requires_grad=True, но не обернули его в Parameter, градиенты бы вычислялись, но оптимизатор (torch.optim.SGD или Adam) не знал бы о существовании этого тензора, так как он не был бы зарегистрирован в model.parameters().9 Таким образом, "магия" nn.Parameter заключается в связывании тензора с жизненным циклом модели и процессом оптимизации.

## ---

**4\. Архитектура Линейного Слоя: Дизайн и Реализация**

Перейдем к конкретной реализации класса MyLinear, требуемой в "Квесте", реплицирующей функциональность torch.nn.Linear.

### **4.1 Математическая Формулировка**

Линейный слой выполняет операцию:

$$y \= x A^T \+ b$$

Где:

- $x$ — входной тензор размерности $(N, \*, H\_{in})$.
- $A$ — матрица весов.
- $B$ — вектор смещения.
- $y$ — выходной тензор.

В коде, предоставленном в условии задачи, это выражается строкой:

Python

return x @ self.weight.T \+ self.bias

Символ @ является синтаксическим сахаром для матричного умножения (torch.matmul), а .T — операцией транспонирования.

### **4.2 Споры о Форме: $(Out, In)$ против $(In, Out)$**

Распространенным источником путаницы для практиков — и тонкой деталью в коде задачи — является форма матрицы весов.

- Математически, если вход $x$ является вектор-строкой $(1, H\_{in})$, мы ожидаем, что матрица весов $W$ будет иметь форму $(H\_{in}, H\_{out})$, чтобы удовлетворить правилам матричного умножения: $(1, H\_{in}) \\times (H\_{in}, H\_{out}) \= (1, H\_{out})$.
- Однако, PyTorch инициализирует веса с формой **$(H\_{out}, H\_{in})$**.13
- Следовательно, прямой проход требует транспонирования весов: x @ self.weight.T.

**Почему PyTorch хранит веса в транспонированном виде?**

Это решение продиктовано **раскладкой памяти и эффективностью оборудования**, в частности, когерентностью кэша и операциями SIMD (Single Instruction, Multiple Data) на CPU и GPU.

1. **Непрерывность Выходных Каналов:** В хранимом формате $(H\_{out}, H\_{in})$, веса для одного выходного нейрона хранятся непрерывно в памяти (предполагая хранение по строкам/row-major, что стандартно для C-backend PyTorch).
   - Строка $i$ матрицы self.weight содержит все веса, входящие в $i$-й выходной нейрон.
   - При вычислении значения $i$-го выходного нейрона процессор выполняет скалярное произведение входного вектора $x$ и строки $i$ матрицы весов.
   - Поскольку строка $i$ непрерывна в памяти, это позволяет осуществлять эффективный последовательный доступ к памяти, минимизируя промахи кэша (cache misses). Процессор загружает кэш-линию, содержащую веса для текущего нейрона, и обрабатывает их максимально быстро.14
2. **Оптимизация Ядер BLAS:** Базовые библиотеки линейной алгебры (такие как MKL для CPU или cuBLAS для GPU) высоко оптимизированы для конкретных раскладок памяти. Операция, выполняемая под капотом, часто является addmm (add matrix-matrix product). Хранение весов как $(Out, In)$ согласуется с конвенциями, ожидаемыми этими оптимизированными ядрами при выполнении вычисления $y \= x W^T$.
3. **Эффективность Обратного Прохода:** Существуют аргументы, что такая раскладка также оптимизирует обратный проход, где необходимо вычислять градиенты по отношению к входам. Градиенты часто требуют шаблонов доступа, которые выигрывают от такой специфической транспозиции.14

Таким образом, хотя запись x @ self.weight (где вес имеет форму $In \\times Out$) кажется более математически интуитивной, x @ self.weight.T (где вес $Out \\times In$) является вычислительно превосходящей из\-за физических реалий компьютерной памяти.

### **4.3 Реализация Метода forward**

Метод forward определяет граф вычислений. В реализации пользователя:

Python

def forward(self, x):  
 \# x @ self.weight.T \+ self.bias  
 return x @ self.weight.T \+ self.bias

Здесь используются механизмы **Broadcasting** (транслирования размеров):

- Логика должна обрабатывать входы произвольной размерности, а не только 2D матрицы. Если вход имеет форму $(Batch, Sequence, H\_{in})$, matmul корректно обрабатывает транслирование, применяя линейное преобразование к последнему измерению.
- Добавление self.bias также вызывает транслирование. Смещение формы $(H\_{out})$ добавляется к каждому элементу в батче.

Встроенный nn.Linear использует F.linear(x, self.weight, self.bias). Этот функциональный вызов немного эффективнее, чем явное x @ weight.T \+ bias, так как он может "сплавлять" (fuse) умножение и сложение в один запуск ядра (kernel launch), сокращая накладные расходы на чтение/запись памяти.13 Однако для целей обучения и понимания, явная запись через матричное умножение является корректной и предпочтительной.

## ---

**5\. Стратегии Инициализации: Наука Начальных Значений**

Условие "Квеста" использует torch.randn для инициализации. Хотя это функционально, простая гауссовская инициализация часто недостаточна для глубоких сетей. Профессиональный отчет должен исследовать, _почему_ инициализация важна, и каковы промышленные стандарты, которые следует применять при создании собственных слоев.

### **5.1 Проблема Дисперсии Сигнала**

Если веса инициализированы слишком большими значениями, активации могут насыщаться (в случае Tanh/Sigmoid) или взрываться (в случае ReLU/Linear). Если значения слишком малы, сигналы затухают по мере прохождения через слои ("Vanishing Gradients"). Цель правильной инициализации — сохранить **дисперсию активаций** неизменной при проходе через сеть.

### **5.2 Инициализация по Умолчанию в PyTorch**

Инициализация по умолчанию для nn.Linear в PyTorch **не является** стандартным нормальным распределением randn(0, 1). Она использует Равномерное распределение (Uniform), ограниченное диапазоном $\\pm \\frac{1}{\\sqrt{fan\\\_in}}$.18

Конкретно:

$$W \\sim \\mathcal{U}(-\\sqrt{k}, \\sqrt{k}), \\quad \\text{где } k \= \\frac{1}{\\text{in\\\_features}}$$  
Смещение (bias) также инициализируется из этого же равномерного распределения.19 Это отличается от распространенного совета инициализировать смещения нулем. PyTorch выбирает такую инициализацию, чтобы гарантировать, что начальное состояние нейрона нарушает симметрию и обеспечивает небольшой ненулевой сигнал, предотвращая появление "мертвых нейронов" (dead neurons) на самом старте обучения, особенно в контексте ReLU.

### **5.3 Продвинутые Инициализации: Xavier и Kaiming**

Для более глубоких сетей требуются более сложные стратегии, часто реализуемые через метод reset_parameters, который является стандартом де\-факто для пользовательских модулей.20

1. **Инициализация Xavier (Glorot):** Разработана для активаций типа Sigmoid/Tanh. Она масштабирует веса на основе как входных, так и выходных размерностей ($fan\\\_in \+ fan\\\_out$), чтобы поддерживать дисперсию постоянной как при прямом, так и при обратном проходе.
2. **Инициализация Kaiming (He):** Разработана для активаций ReLU. Поскольку ReLU зануляет половину входов (отрицательные значения), дисперсия уменьшается вдвое на каждом слое. Инициализация Kaiming компенсирует это, масштабируя веса коэффициентом $\\sqrt{2/fan\\\_in}$.20

Надежная реализация MyLinear, претендующая на статус "Артефакта", должна в идеале включать метод reset_parameters. В примере ниже мы покажем, как интегрировать это в класс.

## ---

**6\. Смещение (Bias) и Взаимодействие с Batch Normalization**

### **6.1 Роль Смещения**

Вектор смещения b позволяет гиперплоскости, представляемой линейным уравнением, смещаться относительно начала координат. Без смещения разделяющая плоскость должна строго проходить через точку $(0,0,\\dots,0)$. Это ограничивает способность модели аппроксимировать данные, которые не центрированы вокруг нуля.

### **6.2 Избыточность Смещения при Batch Norm**

Важным нюансом в проектировании современной архитектуры является взаимодействие между слоями Linear и Batch Normalization (BN).

Если слой Linear непосредственно предшествует слою BatchNorm, смещение линейного слоя становится избыточным.

$$BN(xW^T \+ b) \\equiv BN(xW^T)$$

Поскольку BatchNorm нормализует данные путем вычитания среднего значения, константа $b$ вычитается в процессе центрирования: $(xW^T \+ b) \- \\text{mean}(xW^T \+ b) \= xW^T \- \\text{mean}(xW^T)$. Обучаемый параметр $\\beta$ в слое BatchNorm затем действует как новое эффективное смещение.  
Поэтому, при комбинировании Linear и BatchNorm, стандартной практикой (и оптимизацией производительности) является установка bias=False в линейном слое для экономии параметров и памяти.25 В нашей реализации по умолчанию мы оставляем bias=True, как это сделано в стандартном nn.Linear, но понимание этой опции критично для "Архитектора".

## ---

**7\. Динамика Выполнения: \_\_call\_\_ против forward**

В условии задачи метод forward описывается как "главное заклинание". Однако пользователи редко вызывают .forward() напрямую. Они вызывают сам объект: model(x). Это различие критично.

### **7.1 Механизм \_\_call\_\_**

Класс nn.Module определяет метод \_\_call\_\_. Этот метод-обертка является точкой входа для выполнения. Он выполняет несколько критически важных административных задач до и после вызова определенного пользователем метода forward 28:

1. **Выполнение Хуков (Hooks):** Проверяет и выполняет любые зарегистрированные **forward pre-hooks**.
2. **Выполнение Forward:** Вызывает self.forward(\*input).
3. **Постобработка:** Выполняет **forward hooks** на выходе.

Если пользователь вызывает model.forward(x) напрямую, он обходит этот механизм. Хуки не сработают, и определенные механизмы отслеживания состояния (например, профилирование или трассировка JIT) могут отказать. Таким образом, лучшей практикой всегда является использование model(x).

### **7.2 Система Хуков**

Хуки — это функции, зарегистрированные в модуле, которые запускаются автоматически. Они необходимы для отладки и визуализации активаций без изменения кода модуля.3 В контексте "Квеста", если бы мы хотели "подслушать шепот рун" (проверить градиенты) или "посмотреть в кристалл" (визуализировать промежуточные данные), мы бы использовали именно хуки, которые работают только при правильном вызове через \_\_call\_\_.

## ---

**8\. Представление и Сериализация: extra_repr**

Когда пользователь печатает модуль (например, print(my_first_neuron)), PyTorch отображает строковое представление. По умолчанию nn.Module печатает только имя класса и рекурсивно печатает дочерние модули.

Чтобы отобразить параметры, такие как input_size=1, output_size=1, bias=True (как это делает стандартный nn.Linear), пользовательский модуль должен реализовать метод extra_repr.3

Python

def extra_repr(self):  
 return 'in_features={}, out_features={}, bias={}'.format(  
 self.in_features, self.out_features, self.bias is not None  
 )

Этот метод возвращает строку, которую базовая реализация \_\_repr\_\_ вставляет в вывод. Это жизненно важно для отладки, позволяя инженерам быстро проверять конфигурации слоев в больших логах. В "паспорте" нейрона, упомянутом в квесте, именно extra_repr отвечает за детализацию характеристик артефакта.

## ---

**9\. Комплексная Реализация Артефакта и Проведение Ритуала**

Синтезируя весь вышеприведенный анализ, мы представляем итоговую реализацию класса MyLinear и скрипт проверки ("Ритуал"), который полностью удовлетворяет требованиям квеста, но при этом снабжен комментариями уровня Senior DL Engineer.

Данный код включает не только требуемую функциональность, но и правильную обработку extra_repr, гибкую инициализацию и демонстрацию работы.

### **9.1 Полный Листинг Кода (Решение Квеста)**

Python

import torch  
import torch.nn as nn  
import math

class MyLinear(nn.Module):  
 """  
 MyLinear: Самодостаточный 'Рунный Камень', реализующий полносвязный слой.

    Этот класс демонстрирует фундаментальные принципы создания пользовательских
    модулей в PyTorch: наследование от nn.Module, использование nn.Parameter
    для регистрации весов и реализацию метода forward.
    """
    def \_\_init\_\_(self, input\_size, output\_size):
        \# 1\. Инициализация базового класса (Акт пробуждения nn.Module)
        \# Обязательный шаг для создания внутренних реестров (\_parameters, \_modules).
        super().\_\_init\_\_()

        \# Сохраняем размеры для использования в extra\_repr (для "паспорта")
        self.input\_size \= input\_size
        self.output\_size \= output\_size

        \# 2\. Сотворение Рун (Инициализация Параметров)
        \# Мы оборачиваем тензоры в nn.Parameter.
        \# Форма весов (out, in) выбрана для эффективности вычислений (см. раздел 4.2).
        \# Инициализация: используем Kaiming Uniform для лучшей сходимости,
        \# хотя в простейшем случае работает и randn.

        self.weight \= nn.Parameter(torch.Tensor(output\_size, input\_size))
        self.bias \= nn.Parameter(torch.Tensor(output\_size))

        \# Вызов метода сброса параметров для заполнения их начальными значениями.
        self.reset\_parameters()

    def reset\_parameters(self):
        """
        Инициализирует веса и смещения статистически обоснованными значениями.
        Это аналог 'наполнения рун магической силой' перед использованием.
        """
        \# Инициализация Kaiming Uniform (стандарт для ReLU сетей)
        nn.init.kaiming\_uniform\_(self.weight, a=math.sqrt(5))

        if self.bias is not None:
            \# Инициализация смещения на основе fan-in
            fan\_in, \_ \= nn.init.\_calculate\_fan\_in\_and\_fan\_out(self.weight)
            bound \= 1 / math.sqrt(fan\_in) if fan\_in \> 0 else 0
            nn.init.uniform\_(self.bias, \-bound, bound)

    def forward(self, x):
        """
        Главное заклинание действия: y \= x \* W^T \+ b.

        Args:
            x (torch.Tensor): Входной тензор формы (batch\_size, input\_size)

        Returns:
            torch.Tensor: Выходной тензор формы (batch\_size, output\_size)
        """
        \# Используем матричное умножение (@) и транспонирование (.T)
        \# Broadcasting автоматически применит bias ко всем элементам батча.
        return x @ self.weight.T \+ self.bias

    def extra\_repr(self):
        """
        Возвращает детали для строкового представления объекта ('паспорта').
        """
        return 'in\_features={}, out\_features={}, bias=True'.format(
            self.input\_size, self.output\_size
        )

\# \--- Запуск Ритуала (Main Execution Block) \---

if \_\_name\_\_ \== "\_\_main\_\_":  
 print("--- НАЧАЛО РИТУАЛА: Создание Артефакта \---\\n")

    \# 1\. Сотворение (Инстанцирование)
    \# Создаем нейрон с 1 входом и 1 выходом, как в условии задачи.
    print("Создаю 'Рунный Камень' (экземпляр класса MyLinear)...")
    my\_first\_neuron \= MyLinear(input\_size=1, output\_size=1)

    \# 2\. Проверка Паспорта (Repr)
    print("\\nРаспечатка 'паспорта' нашего Камня:")
    \# Благодаря extra\_repr, мы увидим размеры внутри скобок
    print(my\_first\_neuron)

    \# 3\. Инспекция Рун (Named Parameters)
    print("\\nСмотрим на его 'руны' (параметры):")
    \# Цикл по named\_parameters() доказывает, что \_\_setattr\_\_ сработал корректно
    \# и параметры зарегистрированы в \_parameters.
    for name, param in my\_first\_neuron.named\_parameters():
        print(f"  \- Руна '{name}':\\n    Значение: {param.data}\\n    Требует градиент: {param.requires\_grad}")

    \# 4\. Испытание (Forward Pass)
    input\_value \= 2.0
    input\_tensor \= torch.tensor(\[input\_value\])

    print(f"\\nКладу на 'Камень' гирьку весом: {input\_tensor.item()}")

    \# Вызываем объект как функцию (триггерит \_\_call\_\_ \-\> forward)
    output\_tensor \= my\_first\_neuron(input\_tensor)

    \# 5\. Результат
    print(f"Предсказание 'Рунного Камня': {output\_tensor.item():.4f}")

    \# Ручная проверка формулы для демонстрации корректности
    w\_val \= my\_first\_neuron.weight.item()
    b\_val \= my\_first\_neuron.bias.item()
    manual\_calc \= input\_value \* w\_val \+ b\_val
    print(f"Ручная проверка (2.0 \* {w\_val:.4f} \+ {b\_val:.4f}): {manual\_calc:.4f}")

    print("\\nРитуал завершен\! Ты выковал свой первый самодостаточный 'атом магии'.")

### **9.2 Анализ Результатов Выполнения**

При запуске вышеуказанного скрипта, консоль отобразит следующую структуру данных (конкретные числа будут отличаться из\-за случайной инициализации):

| Этап             | Описание                           | Ожидаемый вывод (пример)                                    |
| :--------------- | :--------------------------------- | :---------------------------------------------------------- |
| **Паспорт**      | Строковое представление объекта    | MyLinear(in_features=1, out_features=1, bias=True)          |
| **Руны**         | Значения тензоров весов и смещений | weight: tensor(\[\[-0.4521\]\]), bias: tensor(\[0.8912\])   |
| **Вход**         | Тестовые данные                    | 2.0                                                         |
| **Предсказание** | Результат forward                  | \-0.0130 (рассчитано как $2.0 \\times (-0.4521) \+ 0.8912$) |

Этот вывод подтверждает, что наш класс MyLinear ведет себя идентично стандартному nn.Linear, корректно хранит состояние и выполняет вычисления.

## ---

**10\. Заключение**

Создание пользовательского слоя Linear в рамках квеста — это не просто упражнение в написании кода; это археология фреймворка PyTorch. Раскопав механизмы \_\_setattr\_\_, мы поняли, как фреймворк "магически" отслеживает состояние. Исследовав форму весов $(Out, In)$, мы обнаружили глубокие связи между абстрактной алгеброй и иерархией кэша процессоров. Изучив инициализацию, мы признали статистическую тонкость, необходимую для обучения глубоких сетей.

Умение создавать собственные nn.Module — это фундамент, о котором говорилось в разделе "Бизнес-ценность". Любая современная архитектура, будь то GPT-4 или Diffusion Model, является, по сути, сложной композицией таких вот простых "Рунных Камней". Инкапсуляция логики и параметров в классы позволяет строить системы любой сложности, сохраняя код чистым, модульным и пригодным для повторного использования. "Артефакт", созданный в этом отчете, является первым шагом от пользователя библиотек к архитектору систем искусственного интеллекта.

#### **Источники**

1. Custom nn Modules — PyTorch Tutorials 2.9.0+cu128 documentation, дата последнего обращения: декабря 15, 2025, [https://docs.pytorch.org/tutorials/beginner/examples_nn/polynomial_module.html](https://docs.pytorch.org/tutorials/beginner/examples_nn/polynomial_module.html)
2. Build the Neural Network — PyTorch Tutorials 2.9.0+cu128 documentation, дата последнего обращения: декабря 15, 2025, [https://docs.pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html](https://docs.pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html)
3. Module — PyTorch 2.9 documentation, дата последнего обращения: декабря 15, 2025, [https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html](https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html)
4. Create Model using Custom Module in Pytorch \- GeeksforGeeks, дата последнего обращения: декабря 15, 2025, [https://www.geeksforgeeks.org/machine-learning/create-model-using-custom-module-in-pytorch/](https://www.geeksforgeeks.org/machine-learning/create-model-using-custom-module-in-pytorch/)
5. How does pytorch Module collect learnable parameters from modules in its attributes?, дата последнего обращения: декабря 15, 2025, [https://stackoverflow.com/questions/71093230/how-does-pytorch-module-collect-learnable-parameters-from-modules-in-its-attribu](https://stackoverflow.com/questions/71093230/how-does-pytorch-module-collect-learnable-parameters-from-modules-in-its-attribu)
6. Type hints on nn.Module.\_\_setattr\_\_ value · Issue \#120587 \- GitHub, дата последнего обращения: декабря 15, 2025, [https://github.com/pytorch/pytorch/issues/120587](https://github.com/pytorch/pytorch/issues/120587)
7. Learning PyTorch: Modules. This blog post is part of the series… | by Dagang Wei \- Medium, дата последнего обращения: декабря 15, 2025, [https://medium.com/@weidagang/demystifying-pytorch-modules-20fc1ddb7693](https://medium.com/@weidagang/demystifying-pytorch-modules-20fc1ddb7693)
8. nn.Module add new parameter, setattr() VS register_parameter() \- PyTorch Forums, дата последнего обращения: декабря 15, 2025, [https://discuss.pytorch.org/t/nn-module-add-new-parameter-setattr-vs-register-parameter/108238](https://discuss.pytorch.org/t/nn-module-add-new-parameter-setattr-vs-register-parameter/108238)
9. Difference between Parameter vs. Tensor in PyTorch \- Stack Overflow, дата последнего обращения: декабря 15, 2025, [https://stackoverflow.com/questions/56708367/difference-between-parameter-vs-tensor-in-pytorch](https://stackoverflow.com/questions/56708367/difference-between-parameter-vs-tensor-in-pytorch)
10. Differences with torch.nn.parameter.Parameter | MindSpore 2.4.1 documentation, дата последнего обращения: декабря 15, 2025, [https://www.mindspore.cn/docs/en/r2.4.1/note/api_mapping/pytorch_diff/Parameter.html](https://www.mindspore.cn/docs/en/r2.4.1/note/api_mapping/pytorch_diff/Parameter.html)
11. Understanding torch.nn.Parameter \- GeeksforGeeks, дата последнего обращения: декабря 15, 2025, [https://www.geeksforgeeks.org/deep-learning/understanding-torchnnparameter/](https://www.geeksforgeeks.org/deep-learning/understanding-torchnnparameter/)
12. Parameter vs tensor.requires_grad \= True \- autograd \- PyTorch Forums, дата последнего обращения: декабря 15, 2025, [https://discuss.pytorch.org/t/parameter-vs-tensor-requires-grad-true/60772](https://discuss.pytorch.org/t/parameter-vs-tensor-requires-grad-true/60772)
13. Why does PyTorch's Linear layer store the weight in shape (out, in) and transpose it in the forward pass? \- Stack Overflow, дата последнего обращения: декабря 15, 2025, [https://stackoverflow.com/questions/76949152/why-does-pytorchs-linear-layer-store-the-weight-in-shape-out-in-and-transpos](https://stackoverflow.com/questions/76949152/why-does-pytorchs-linear-layer-store-the-weight-in-shape-out-in-and-transpos)
14. Why does nn.Linear(in_features, out_features) use a weight matrix of shape (out_features, in_features) in PyTorch? \- Stack Overflow, дата последнего обращения: декабря 15, 2025, [https://stackoverflow.com/questions/78963755/why-does-nn-linearin-features-out-features-use-a-weight-matrix-of-shape-out](https://stackoverflow.com/questions/78963755/why-does-nn-linearin-features-out-features-use-a-weight-matrix-of-shape-out)
15. nn.Linear weight shape initialization confusion \- PyTorch Forums, дата последнего обращения: декабря 15, 2025, [https://discuss.pytorch.org/t/nn-linear-weight-shape-initialization-confusion/202891](https://discuss.pytorch.org/t/nn-linear-weight-shape-initialization-confusion/202891)
16. matrix \- PyTorch \- shape of nn.Linear weights \- Stack Overflow, дата последнего обращения: декабря 15, 2025, [https://stackoverflow.com/questions/53465608/pytorch-shape-of-nn-linear-weights](https://stackoverflow.com/questions/53465608/pytorch-shape-of-nn-linear-weights)
17. Applying linear layer but with weights transposed? \- PyTorch Forums, дата последнего обращения: декабря 15, 2025, [https://discuss.pytorch.org/t/applying-linear-layer-but-with-weights-transposed/79132](https://discuss.pytorch.org/t/applying-linear-layer-but-with-weights-transposed/79132)
18. Linear — PyTorch 2.9 documentation, дата последнего обращения: декабря 15, 2025, [https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html](https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html)
19. How are layer weights and biases initialized by default? \- PyTorch Forums, дата последнего обращения: декабря 15, 2025, [https://discuss.pytorch.org/t/how-are-layer-weights-and-biases-initialized-by-default/13073](https://discuss.pytorch.org/t/how-are-layer-weights-and-biases-initialized-by-default/13073)
20. What's the default initialization methods for layers? — PyTorch | by Hey Amit \- Medium, дата последнего обращения: декабря 15, 2025, [https://medium.com/data-scientists-diary/whats-the-default-initialization-methods-for-layers-pytorch-eb16cc45110d](https://medium.com/data-scientists-diary/whats-the-default-initialization-methods-for-layers-pytorch-eb16cc45110d)
21. Pytorch Linear Layer now automatically reshape the input? \- Stack Overflow, дата последнего обращения: декабря 15, 2025, [https://stackoverflow.com/questions/57138540/pytorch-linear-layer-now-automatically-reshape-the-input](https://stackoverflow.com/questions/57138540/pytorch-linear-layer-now-automatically-reshape-the-input)
22. Reset parameters of a neural network in pytorch \- Stack Overflow, дата последнего обращения: декабря 15, 2025, [https://stackoverflow.com/questions/63627997/reset-parameters-of-a-neural-network-in-pytorch](https://stackoverflow.com/questions/63627997/reset-parameters-of-a-neural-network-in-pytorch)
23. How do I initialize weights in PyTorch? | by why amit \- Medium, дата последнего обращения: декабря 15, 2025, [https://medium.com/@whyamit101/how-do-i-initialize-weights-in-pytorch-1dd37078d19b](https://medium.com/@whyamit101/how-do-i-initialize-weights-in-pytorch-1dd37078d19b)
24. In PyTorch how are layer weights and biases initialized by default? \- Stack Overflow, дата последнего обращения: декабря 15, 2025, [https://stackoverflow.com/questions/48529625/in-pytorch-how-are-layer-weights-and-biases-initialized-by-default](https://stackoverflow.com/questions/48529625/in-pytorch-how-are-layer-weights-and-biases-initialized-by-default)
25. Using Bias=False during batch norm \- vision \- PyTorch Forums, дата последнего обращения: декабря 15, 2025, [https://discuss.pytorch.org/t/using-bias-false-during-batch-norm/77661](https://discuss.pytorch.org/t/using-bias-false-during-batch-norm/77661)
26. Batch Normalization of Linear Layers \- PyTorch Forums, дата последнего обращения: декабря 15, 2025, [https://discuss.pytorch.org/t/batch-normalization-of-linear-layers/20989](https://discuss.pytorch.org/t/batch-normalization-of-linear-layers/20989)
27. Explanation of Karpathy tweet about common mistakes. \#5: "you didn't use bias=False for your Linear/Conv2d layer when using BatchNorm", дата последнего обращения: декабря 15, 2025, [https://datascience.stackexchange.com/questions/102659/explanation-of-karpathy-tweet-about-common-mistakes-5-you-didnt-use-bias-fa](https://datascience.stackexchange.com/questions/102659/explanation-of-karpathy-tweet-about-common-mistakes-5-you-didnt-use-bias-fa)
28. PyTorch module\_\_call\_\_() vs forward() \- Stephen Cow Chau \- Medium, дата последнего обращения: декабря 15, 2025, [https://stephencowchau.medium.com/pytorch-module-call-vs-forward-c4df3ff304b1](https://stephencowchau.medium.com/pytorch-module-call-vs-forward-c4df3ff304b1)
29. Is model.forward(x) the same as model.\_\_call\_\_(x)? \- PyTorch Forums, дата последнего обращения: декабря 15, 2025, [https://discuss.pytorch.org/t/is-model-forward-x-the-same-as-model-call-x/33460](https://discuss.pytorch.org/t/is-model-forward-x-the-same-as-model-call-x/33460)
30. PyTorch 101: Understanding Hooks \- DigitalOcean, дата последнего обращения: декабря 15, 2025, [https://www.digitalocean.com/community/tutorials/pytorch-hooks-gradient-clipping-debugging](https://www.digitalocean.com/community/tutorials/pytorch-hooks-gradient-clipping-debugging)
31. Extending PyTorch — PyTorch 2.9 documentation, дата последнего обращения: декабря 15, 2025, [https://docs.pytorch.org/docs/stable/notes/extending.html](https://docs.pytorch.org/docs/stable/notes/extending.html)
