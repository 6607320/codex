# **Демистификация Магии: Исчерпывающий Анализ Вычислительных Графов и Ручного Дифференцирования в Обучении Нейронных Сетей**

## **1\. Введение: Иллюзия Магии и Реальность Математики**

В современной эпохе искусственного интеллекта (ИИ) глубокое обучение часто воспринимается через призму абстракций высокого уровня, которые скрывают фундаментальные механизмы, управляющие процессом обучения. Разработчики и исследователи ежедневно используют мощные фреймворки, такие как PyTorch, вызывая методы вроде loss.backward() и optimizer.step(), словно заклинания, которые волшебным образом заставляют модель "учиться". Этот феномен порождает разрыв между практическим применением и теоретическим пониманием: сложные архитектуры строятся без глубокого осознания того, как именно градиенты протекают через вычислительный граф.  
Представленный исследовательский отчет посвящен детальному разбору "Квеста 9.1", который ставит перед собой фундаментальную задачу: доказать, что "магия" автоматического дифференцирования (Autograd) является всего лишь автоматизацией элементарных арифметических правил и законов математического анализа. Квест разделен на два "ритуала": Акт 1, использующий встроенные механизмы PyTorch, и Акт 2, требующий ручной реализации обратного распространения ошибки (backpropagation) для одного нейрона. Ожидаемый результат — полная идентичность выводов обоих скриптов — служит доказательством того, что внутри "черного ящика" фреймворка работают детерминированные математические алгоритмы.  
Цель данного документа — не просто сравнить два подхода, но предоставить исчерпывающий анализ механики обучения. Мы спустимся с уровня Python-абстракций до уровня C++ движка Autograd, разберем анатомию динамических вычислительных графов, проследим путь градиента от функции потерь до весов модели и сопоставим это с ручным применением цепного правила дифференцирования. Также будет рассмотрена роль оптимизаторов, проблема исчезающих градиентов и теоретические основы, которые делают возможным обучение нейронных сетей. Этот отчет предназначен для профессионалов, стремящихся понять глубинную суть процессов, происходящих при вызове метода .backward().

## **2\. Теоретический Фундамент: Анатомия Одного Нейрона**

Чтобы деконструировать процесс обучения, необходимо сначала определить математическую модель. В контексте Квеста 9.1 рассматривается простейшая архитектура: один нейрон, выполняющий линейную регрессию. Несмотря на свою простоту, этот нейрон содержит все ключевые компоненты глубоких сетей: веса, смещение (bias), операцию взвешенной суммы и функцию потерь.

### **2.1 Модель Прямого Распространения (Forward Pass)**

Один нейрон в контексте линейной регрессии аппроксимирует целевую переменную y на основе входного вектора x. Нейрон характеризуется двумя обучаемыми параметрами: вектором весов w и скаляром смещения b.  
Математическая формулировка прямого прохода выглядит следующим образом:  
Где:

- \\hat{y} (y_pred) — предсказанное значение.
- x — входной тензор.
- w — вес (коэффициент наклона в линейной функции).
- b — смещение (точка пересечения с осью ординат).

В рамках PyTorch, эта операция является не просто арифметическим умножением, а созданием узлов в вычислительном графе. Тензоры x, w и b являются листьями этого графа, а \\hat{y} — корнем (до момента вычисления ошибки). Значения, полученные на этом этапе, критически важны, так как они сохраняются в памяти для последующего использования при вычислении градиентов.

### **2.2 Функция Потерь: Среднеквадратичная Ошибка (MSE)**

Обучение нейрона заключается в минимизации расхождения между предсказанием \\hat{y} и истинным значением y. Для задач регрессии стандартным выбором является Среднеквадратичная Ошибка (Mean Squared Error, MSE). Она обладает важным свойством выпуклости, что гарантирует наличие единственного глобального минимума для линейных моделей, упрощая процесс оптимизации.  
Функция потерь L для одного обучающего примера определяется как:  
В некоторых реализациях используется множитель \\frac{1}{2} или усреднение по пакету данных (batch normalization), но для дифференцирования по одному примеру суть остается неизменной. Цель процесса обучения — найти такие значения параметров w и b, при которых градиент функции L стремится к нулю. Именно здесь начинается "магия" обратного распространения, которая на самом деле является строгим применением математического анализа.

## **3\. Акт 1: Автоматический Ритуал (PyTorch Autograd)**

В первом акте квеста используется скрипт quest_9_1.py, который делегирует вычисление градиентов движку PyTorch Autograd. Для пользователя этот процесс выглядит тривиально: определение переменных с флагом requires_grad=True, выполнение вычислений и вызов loss.backward(). Однако под капотом происходит сложная оркестрация памяти и вычислительных операций.

### **3.1 Архитектура Динамических Вычислительных Графов (DAG)**

В отличие от статических графов (характерных для ранних версий TensorFlow), PyTorch использует динамические ациклические графы (Directed Acyclic Graphs, DAG). Это означает, что граф строится "на лету" при каждом прямом проходе.

#### **Узлы и Ребра**

- **Листья (Leaf Nodes):** Это входные тензоры и параметры модели (w, b), которые создаются пользователем. Для параметров устанавливается атрибут requires_grad=True, сигнализирующий системе о необходимости отслеживать операции над ними.
- **Функциональные Узлы (Function Nodes):** Это внутренние узлы графа, представляющие операции. Например, операция умножения w \\cdot x порождает узел MulBackward, а сложение с b — узел AddBackward. Эти узлы хранят контекст операции, необходимый для вычисления производных.
- **Ребра (Edges):** Связывают тензоры с функциями, которые их породили (через атрибут .grad_fn), и функции с их входными тензорами.

Когда выполняется код y_pred \= w \* x \+ b, PyTorch не просто вычисляет число. Он создает объект MulBackward0 (для умножения), связывает его с w и x, затем создает объект AddBackward0 (для сложения), связывает его с результатом умножения и b. Результирующий тензор loss хранит ссылку на последнюю операцию в цепочке, формируя полный путь назад к параметрам.

### **3.2 Механика loss.backward(): Взгляд Изнутри**

Вызов loss.backward() запускает процесс обратного обхода графа. Этот процесс реализован на уровне C++ для максимальной производительности, но концептуально он следует строгим правилам:

1. **Топологическая сортировка:** Движок Autograd определяет порядок выполнения узлов, гарантируя, что все зависимости для вычисления градиента конкретного узла уже разрешены.
2. **Инициализация градиента:** Градиент на выходе (в корне графа, т.е. для loss) неявно инициализируется единицей (или тензором из единиц той же формы), так как \\frac{\\partial L}{\\partial L} \= 1\.
3. **Вызов метода apply:** Движок вызывает метод apply (или backward) для каждого функционального узла (grad_fn). Этот метод принимает входящий градиент (от вышестоящего узла, который ближе к выходу) и умножает его на локальную производную операции (Local Jacobian), следуя цепному правилу.
4. **Аккумуляция:** Вычисленные градиенты для листовых узлов (параметров w и b) не перезаписывают старые значения, а добавляются к ним. Это свойство, известное как "аккумуляция градиентов", критически важно для работы с мини-батчами и рекуррентными сетями, но требует ручного обнуления через zero_grad() перед новой итерацией.

### **3.3 Внутренняя Реализация Узлов Графа**

Интересно рассмотреть, как именно называются эти операции внутри PyTorch, так как это проливает свет на детерминированную природу процесса.

- При умножении w \* x создается узел типа MulBackward0.
- При сложении ... \+ b создается узел AddBackward0.
- При вычислении MSE (y_pred \- y)\*\*2 (или через функцию MSELoss) создается узел MSELossBackward или комбинация SubBackward и PowBackward.

Каждый из этих классов в C++ (torch/csrc/autograd/functions/) содержит жестко закодированные формулы производных. Например, MulBackward знает, что если z \= x \\cdot y, то \\frac{\\partial z}{\\partial x} \= y. Никакой магии — только заранее прописанные правила дифференцирования.

## **4\. Акт 2: Истинный Ручной Ритуал (Чистая Математика)**

Второй акт квеста требует отбросить костыли Autograd и реализовать тот же процесс вручную. Это упражнение, известное как "Manual Gradient Spell", является проверкой понимания фундаментального исчисления. Здесь мы выведем формулы, которые неявно использует PyTorch.

### **4.1 Дифференцирование Функции Потерь**

Наша цель — найти частные производные функции потерь L по параметрам w и b: \\frac{\\partial L}{\\partial w} и \\frac{\\partial L}{\\partial b}.  
Вспоминаем функцию потерь:  
И функцию прямого распространения:  
Используем **цепное правило (Chain Rule)**, которое гласит, что производная сложной функции является произведением производных её компонентов:

### **4.2 Шаг 1: Производная Внешней Функции (Градиент Ошибки)**

Сначала найдем производную потерь по предсказанию \\frac{\\partial L}{\\partial \\hat{y}}. Дифференцируем функцию квадрата (u)^2:  
Этот член 2(\\hat{y} \- y) часто называют "сигналом ошибки" или grad_output, который течет от функции потерь назад к слоям сети.

### **4.3 Шаг 2: Производные Внутренней Функции**

Теперь найдем частные производные \\hat{y} по параметрам w и b.  
**Для веса w:**  
Поскольку x и b константы относительно w, получаем:  
**Для смещения b:**  
Поскольку w и x константы относительно b, получаем:

### **4.4 Шаг 3: Сборка Формул (The Manual Spell)**

Объединяем полученные результаты согласно цепному правилу:

1. **Градиент для веса (w):** В коде Python это выражается как:  
   `grad_w = 2 * (y_pred - y) * x`

2. **Градиент для смещения (b):** В коде Python:  
   `grad_b = 2 * (y_pred - y)`

Именно эти формулы должны быть реализованы в quest_9_1_manual.py. Важно отметить, что если функция потерь определена как MSE \= \\frac{1}{N}\\sum (\\hat{y}-y)^2, то при одном примере N=1, и множитель 2 сохраняется. В некоторых фреймворках используют функцию \\frac{1}{2}(\\hat{y}-y)^2 для упрощения производной (чтобы двойка сократилась), но стандартная реализация MSE в PyTorch (torch.nn.MSELoss) сохраняет точную математику среднего квадрата, поэтому двойка должна присутствовать.

## **5\. Сравнительный Анализ: Почему Результаты Идентичны**

Ожидаемый результат квеста — полная идентичность выводов. Давайте разберем, почему это происходит и какие факторы могут (теоретически) вызвать расхождения.

### **5.1 Алгоритмическая Эквивалентность**

Когда Autograd выполняет обратный проход, он делает следующее:

1. Узел PowBackward (возведение в квадрат) берет вход val \= \\hat{y} \- y и возвращает градиент 2 \\cdot val.
2. Этот градиент передается в узел SubBackward (вычитание), который передает его дальше без изменений (производная разности равна 1).
3. Градиент доходит до узла AddBackward (сложение wx \+ b). Узел разветвляет градиент:
   - На ветку b он уходит как есть (умноженный на 1). Итог: 2(\\hat{y} \- y). Это точно совпадает с нашей ручной формулой grad_b.
   - На ветку wx он уходит тоже как есть.
4. Градиент попадает в узел MulBackward (умножение w \\cdot x). Этот узел умножает пришедший градиент 2(\\hat{y} \- y) на второй множитель операции, то есть на x. Итог: 2(\\hat{y} \- y) \\cdot x. Это точно совпадает с grad_w.

Таким образом, последовательность операций с плавающей точкой в обоих скриптах математически изоморфна. PyTorch не делает никакой аппроксимации (как, например, метод конечных разностей); он выполняет точное аналитическое дифференцирование.

### **5.2 Вопросы Точности Плавающей Точки (IEEE 754\)**

Теоретически, порядок операций может влиять на младшие биты чисел с плавающей точкой из\-за ошибок округления. Например, (a \\cdot b) \\cdot c может не быть побитово равным a \\cdot (b \\cdot c). Однако в данном простом примере (линейная регрессия, один слой) граф вычислений настолько короток, что порядок операций в Autograd и в ручном скрипте, скорее всего, полностью совпадает. Если инициализация весов (seed) одинакова, выводы будут идентичны вплоть до последнего знака.

### **5.3 Таблица Сравнения Операций**

Ниже приведена таблица, сопоставляющая шаги "Автоматического" и "Ручного" ритуалов, демонстрирующая их полную зеркальность.

| Этап                      | PyTorch (Автоматический)             | Ручной (Математический)         | Математическая Суть                                                                  |
| :------------------------ | :----------------------------------- | :------------------------------ | :----------------------------------------------------------------------------------- |
| **1\. Инициализация**     | w \= tensor(..., requires_grad=True) | w \= float(...)                 | Создание переменных                                                                  |
| **2\. Прямой проход**     | y_pred \= w \* x \+ b                | y_pred \= w \* x \+ b           | Линейная комбинация                                                                  |
| **3\. Ошибка**            | loss \= (y_pred \- y)\*\*2           | loss \= (y_pred \- y)\*\*2      | Вычисление невязки                                                                   |
| **4\. Начало Backprop**   | loss.backward()                      | (Начало вычислений производных) | Инициализация \\nabla L \= 1                                                         |
| **5\. Градиент потерь**   | (Внутри C++ PowBackward)             | grad_out \= 2 \* (y_pred \- y)  | \\frac{\\partial L}{\\partial \\hat{y}}                                              |
| **6\. Градиент веса**     | (Внутри C++ MulBackward)             | grad_w \= grad_out \* x         | \\frac{\\partial L}{\\partial w} \= \\frac{\\partial L}{\\partial \\hat{y}} \\cdot x |
| **7\. Градиент смещения** | (Внутри C++ AddBackward)             | grad_b \= grad_out \* 1         | \\frac{\\partial L}{\\partial b} \= \\frac{\\partial L}{\\partial \\hat{y}} \\cdot 1 |
| **8\. Обновление**        | optimizer.step()                     | w \-= lr \* grad_w              | Градиентный спуск (SGD)                                                              |
| **9\. Зачистка**          | optimizer.zero_grad()                | (Перезапись переменных в цикле) | Сброс аккумулятора                                                                   |

## **6\. Оптимизация: От Градиентов к Обновлению Параметров**

После того как градиенты вычислены (вручную или автоматически), наступает этап обновления параметров модели. В PyTorch это делегируется объекту optimizer. Понимание работы optimizer.step() является ключом к завершению картины обучения.

### **6.1 Стохастический Градиентный Спуск (SGD)**

В простейшем случае, который подразумевается в "ручном ритуале", используется алгоритм SGD без моментума. Формула обновления весов выглядит так:  
Где \\eta (эта) — скорость обучения (learning rate). В PyTorch вызов optimizer.step() проходит по списку параметров (model.parameters()), извлекает хранящееся в атрибуте .grad значение и применяет вычитание.  
Важно отметить, что PyTorch разделяет вычисление градиента (backward) и шаг оптимизации (step). Это архитектурное решение дает гибкость: можно накапливать градиенты от нескольких проходов перед шагом (gradient accumulation), изменять градиенты вручную (gradient clipping) или использовать сложные оптимизаторы, не меняя логику самой нейросети.

### **6.2 Усложненные Ритуалы: Оптимизатор Adam**

Хотя Квест 9.1 фокусируется на базовой арифметике, в реальных задачах часто используется оптимизатор Adam. Если бы нам пришлось реализовывать Adam вручную ("Акт 2" повышенной сложности), код стал бы значительно объемнее. Adam использует не только текущий градиент, но и историю градиентов (моментум и адаптивную скорость обучения).  
Формулы Adam включают:

1. **Скользящее среднее градиента (Первый момент m_t):**
2. **Скользящее среднее квадрата градиента (Второй момент v_t):**
3. **Коррекция смещения (Bias Correction):**
4. **Обновление параметров:**

Реализация этого вручную требует хранения дополнительных состояний (m и v) для каждого веса. Это демонстрирует мощь абстракции torch.optim: сложные математические конструкции скрыты за одним вызовом .step().

### **6.3 Роль zero_grad()**

В ручной реализации мы обычно вычисляем grad*w заново на каждой итерации, перезаписывая старое значение. В PyTorch, по умолчанию, градиенты *аккумулируются\_ (суммируются). Если не вызвать optimizer.zero_grad(), градиент текущего шага прибавится к градиенту предыдущего, что приведет к неверному направлению спуска и "взрыву" весов.  
Это поведение сделано намеренно для поддержки определенных архитектур (например, когда один вес используется в графе несколько раз, как в RNN, или при распределенном обучении). Однако для стандартных сетей прямого распространения вызов zero_grad() является обязательным ритуалом очистки перед новым вычислением.

## **7\. Проблемы Обучения: Взгляд Через Призму Математики**

Реализуя обратное распространение вручную, мы можем лучше понять фундаментальные проблемы глубокого обучения, такие как затухающие и взрывающиеся градиенты, которые часто скрыты за автоматикой фреймворков.

### **7.1 Почему Линейный Нейрон "Безопасен"**

В нашем квесте используется линейный нейрон (без функции активации или с тождественной активацией f(x)=x). Производная линейной функции — константа (в данном случае 1). При прохождении через множество линейных слоев градиент просто умножается на веса. Это защищает от проблемы исчезающего градиента, свойственной сигмоидальным функциям, где производная всегда меньше 0.25. В глубоких сетях перемножение множества чисел \<1 быстро превращает градиент в ноль, останавливая обучение. Линейная природа нашей задачи в Квесте 9.1 гарантирует, что сигнал ошибки доходит до весов без затухания, делая ручную реализацию стабильной и наглядной.

### **7.2 Взрывающиеся Градиенты**

С другой стороны, если веса w инициализированы большими числами или входные данные x не нормализованы, член 2(\\hat{y}-y)x в нашей формуле может стать огромным. В ручном коде это приведет к гигантскому шагу обновления w \-= lr \* huge_grad, что может выбросить параметры в бесконечность (NaN). В PyTorch существуют инструменты для борьбы с этим, например torch.nn.utils.clip_grad_norm\_, которые обрезают градиент до порогового значения. В ручной реализации "Акта 2" мы лишены этой страховки, что подчеркивает важность предварительной обработки данных и правильной инициализации весов.

## **8\. Глубинные Инсайты и Расширенное Понимание**

Выполнение Квеста 9.1 приводит к нескольким неочевидным выводам, которые выходят за рамки простого написания кода.

### **8.1 Autograd как Интерпретатор Производных**

Можно рассматривать Autograd как специализированный интерпретатор, который "компилирует" последовательность операций Python в последовательность операций дифференцирования. Когда мы пишем код модели, мы фактически пишем мета-код для генерации градиентов. Это объясняет, почему мы можем использовать условия if/else и циклы for внутри модели (динамический граф): Autograd просто запишет ту ветку исполнения, которая была пройдена, и продифференцирует именно её.

### **8.2 Понимание requires_grad как Менеджера Ресурсов**

Флаг requires_grad=True — это не просто переключатель "обучаемый/необучаемый". Это инструкция по выделению памяти. Для тензоров с этим флагом PyTorch обязан хранить промежуточные значения (активации) во время прямого прохода, чтобы использовать их при обратном проходе. Понимание этого механизма критично для оптимизации потребления памяти (например, использование torch.no_grad() при инференсе, чтобы отключить построение графа и сэкономить гигабайты VRAM).

### **8.3 Градиент как Вектор Направления**

Ручной расчет 2(\\hat{y}-y)x показывает физический смысл градиента.

- (\\hat{y}-y) — это величина и знак ошибки.
- x — это "вклад" данного входа в ошибку. Если вход x был большим, он сильно повлиял на результат, значит, и его вес нужно скорректировать сильнее. Если x был около нуля, то изменение веса w почти не повлияет на результат, поэтому градиент будет маленьким (умножение на x \\approx 0). Эта интуиция, полученная из простой формулы, верна для самых сложных сетей: веса обновляются пропорционально активности их входов.

## **9\. Заключение**

Квест 9.1, несмотря на свою кажущуюся простоту, является мощным педагогическим инструментом. Сравнивая "Автоматический Ритуал" PyTorch с "Ручным Ритуалом" чистой математики, мы приходим к неопровержимому выводу: за фасадом высокоуровневых API скрывается строгая, детерминированная логика исчисления.  
Наше исследование показало, что:

1. Метод loss.backward() — это алгоритмическая реализация цепного правила дифференцирования на динамическом графе.
2. Результаты ручного расчета и автоматического вычисления идентичны (с точностью до погрешности плавающей точки), что подтверждает надежность математического аппарата PyTorch.
3. Понимание внутренней механики (графы, аккумуляция градиентов, оптимизаторы) позволяет разработчику не просто "тренировать сети", а осознанно управлять процессом обучения, диагностировать проблемы сходимости и оптимизировать производительность.

В конечном итоге, "магия" глубокого обучения — это торжество математики, реализованной в эффективном коде. Способность спуститься на уровень ручных градиентов отличает инженера, умеющего использовать инструменты, от исследователя, понимающего суть явлений.

### **Поддерживающие Материалы и Исследования**

Анализ и выводы, представленные в этом отчете, базируются на технической документации PyTorch, академических курсах (CS230 Стэнфорд) и фундаментальных работах по теории оптимизации. Использованные концепции включают производные MSE , механику вычислительных графов Autograd , алгоритмы оптимизации SGD и Adam , а также анализ проблем градиентного спуска. Точное совпадение ручных и автоматических расчетов подтверждается теоретическими выкладками из.

#### **Источники**

1\. A Gentle Introduction to torch.autograd \- PyTorch, https://docs.pytorch.org/tutorials/beginner/blitz/autograd\_tutorial.html 2\. Section 2 (Week 2\) \- CS230 \- Stanford University, https://cs230.stanford.edu/section/2/ 3\. Neural Networks Backpropagation Made Easy | by Lihi Gur Arie, PhD \- Medium, https://medium.com/data-science/neural-networks-backpropagation-made-easy-27be67d8fdce 4\. Understanding Backpropagation \- QuantInsti Blog, https://blog.quantinsti.com/backpropagation/ 5\. Linear regression: Loss | Machine Learning \- Google for Developers, https://developers.google.com/machine-learning/crash-course/linear-regression/loss 6\. Understanding requires_grad, retain_grad, Leaf, and Non-leaf Tensors \- PyTorch, https://docs.pytorch.org/tutorials/beginner/understanding\_leaf\_vs\_nonleaf\_tutorial.html 7\. requires_grad relation to leaf nodes \- pytorch \- Stack Overflow, https://stackoverflow.com/questions/44913720/requires-grad-relation-to-leaf-nodes 8\. Demystifying loss.backward(): How PyTorch Autograd Actually Works | by Jimin Lee, https://medium.com/@jiminlee-ai/demystifying-loss-backward-how-pytorch-autograd-actually-works-e79b4b542b22 9\. How Computational Graphs are Constructed in PyTorch, https://pytorch.org/blog/computational-graphs-constructed-in-pytorch/ 10\. How Computational Graphs are Executed in PyTorch, https://pytorch.org/blog/how-computational-graphs-are-executed-in-pytorch/ 11\. Why Do We Need to Call zero_grad() in PyTorch? \- GeeksforGeeks, https://www.geeksforgeeks.org/deep-learning/why-do-we-need-to-call-zerograd-in-pytorch/ 12\. Why do we need to call zero_grad() in PyTorch? \- Stack Overflow, https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch 13\. Autograd Basics \- GitHub, https://github.com/pytorch/pytorch/wiki/Autograd-Basics 14\. Backpropagation In Neural Networks \- machine learning, http://vinhkhuc.github.io/2015/03/29/backpropagation.html 15\. Overview of PyTorch Autograd Engine, https://pytorch.org/blog/overview-of-pytorch-autograd-engine/ 16\. How to "manually" apply your gradients in Pytorch? \- Stack Overflow, https://stackoverflow.com/questions/74915682/how-to-manually-apply-your-gradients-in-pytorch 17\. Should gradient backwards() and optimizer.step() really be separate? : r/MLQuestions, https://www.reddit.com/r/MLQuestions/comments/1ikhkwx/should\_gradient\_backwards\_and\_optimizerstep/ 18\. Adam — PyTorch 2.9 documentation, https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html 19\. Adam vs SGD : What are the optimizers in neural network and when do we use? \- Medium, https://medium.com/@pumadd1227/adam-vs-sgd-what-are-the-optimizers-in-neural-network-and-when-do-we-use-238478a0eaea 20\. Zeroing out gradients in PyTorch, https://docs.pytorch.org/tutorials/recipes/recipes/zeroing\_out\_gradients.html 21\. Vanishing gradient problem \- Wikipedia, https://en.wikipedia.org/wiki/Vanishing\_gradient\_problem 22\. Vanishing and Exploding Gradients in Deep Neural Networks \- Analytics Vidhya, https://www.analyticsvidhya.com/blog/2021/06/the-challenge-of-vanishing-exploding-gradients-in-deep-neural-networks/ 23\. Vanishing/Exploding Gradients Problem | by Kushan Sharma \- Medium, https://medium.com/@kushansharma1/vanishing-exploding-gradients-problem-1901bb2db2b2 24\. Requires_grad and leaf nodes \- PyTorch Forums, https://discuss.pytorch.org/t/requires-grad-and-leaf-nodes/82881 25\. Understanding the Vanishing Gradient Problem in Deep Learning | by Piyush Kashyap, https://medium.com/@piyushkashyap045/understanding-the-vanishing-gradient-problem-in-deep-learning-840a9da6567f 26\. Stochastic gradient descent \- Wikipedia, https://en.wikipedia.org/wiki/Stochastic\_gradient\_descent
