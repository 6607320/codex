# **Отчёт: Глубинная архитектура и ручная сборка нейронных модулей — Проект «Башня Прозрения»**

**Тема:** Квест 10.2: Сборка «Башни Прозрения» вручную (Реализация CNN через кастомные Linear и Conv слои) для задачи классификации MNIST.

## **1\. Введение: Философия деконструкции в глубоком обучении**

### **1.1 Эпистемологический кризис абстракций**

Современный ландшафт глубокого обучения характеризуется беспрецедентным уровнем абстракции. Инструментарии, такие как PyTorch, TensorFlow и Keras, предоставляют разработчикам высокоуровневые API, которые инкапсулируют сложнейшие математические операции в лаконичные вызовы функций. Обыденная строка кода nn.Conv2d(32, 64, 3\) скрывает за собой колоссальный пласт инженерных и математических решений: от алгоритмов свертки (Winograd, GEMM) до стратегий инициализации весов и механизмов управления памятью GPU.  
Такая абстракция, безусловно, ускоряет процесс разработки и прототипирования, однако она порождает феномен «черного ящика». Исследователь, привыкший оперировать готовыми блоками, рискует утратить понимание фундаментальных принципов, управляющих обучением нейронных сетей. Когда модель не сходится, градиенты затухают или возникают ошибки несоответствия размерностей, поверхностное знание API оказывается недостаточным для диагностики проблемы.  
Проект «Башня Прозрения» (Tower of Insight) представляет собой радикальный ответ на этот эпистемологический вызов. Задача состоит не просто в обучении модели на классическом датасете MNIST, а в полной деконструкции нейронной архитектуры. Мы отказываемся от использования префабрикованных слоев torch.nn.Linear и torch.nn.Conv2d в пользу их ручной реализации. Это упражнение заставляет исследователя спуститься на уровень тензорного исчисления, управления жизненным циклом параметров и явного определения вычислительных графов.

### **1.2 Цели и задачи исследования**

Данный отчет документирует процесс создания сверточной нейронной сети (CNN) «с нуля» в экосистеме PyTorch, используя лишь базовый класс torch.nn.Module и функциональные примитивы. В ходе исследования решаются следующие задачи:

1. **Архитектурный анализ механизма nn.Module:** Исследование внутренних механизмов регистрации параметров, буферов и подмодулей. Как именно PyTorch «узнает» о созданных нами тензорах и включает их в процесс оптимизации?
2. **Реализация аффинных преобразований:** Создание слоя CustomLinear, включающее ручное управление матричными операциями, транспонированием весов и добавлением смещения (bias).
3. **Реализация пространственной фильтрации:** Разработка слоя CustomConv2d, требующая глубокого понимания размерностей 4D-тензоров, механики страйдов (stride), паддинга (padding) и дилатации (dilation).
4. **Стратегии инициализации:** Анализ влияния начального распределения весов на сходимость сети. Почему инициализация нулями или стандартным нормальным распределением губительна для глубоких сетей, и как математически обосновываются методы Kaiming (He) и Xavier (Glorot).
5. **Интеграция и обучение:** Сборка компонентов в единую архитектуру «Башня Прозрения» и её валидация на датасете рукописных цифр MNIST.

Это исследование не является простым руководством по написанию кода; это глубокий технический аудит фундаментальных блоков, из которых строятся современные системы искусственного интеллекта.

## **2\. Теоретический базис: Анатомия torch.nn.Module**

### **2.1 Метафизика объекта Module**

В основе всей экосистемы нейронных сетей PyTorch лежит класс torch.nn.Module. Понимание его внутренней работы критически важно для создания кастомных слоев. Module — это не просто контейнер для функций; это состояние-зависимая сущность, управляющая графом вычислений и памятью параметров.  
Ключевая магия происходит в методе \_\_setattr\_\_. В стандартном Python присваивание атрибута объекту — тривиальная операция. Однако nn.Module переопределяет этот метод, внедряя сложную логику перехвата. Когда мы пишем self.weight \=... внутри конструктора слоя, модуль проверяет тип присваиваемого объекта:

- Если объект является экземпляром torch.nn.Parameter, он автоматически регистрируется во внутреннем упорядоченном словаре \_parameters. Это критически важно: только тензоры, попавшие в этот словарь, будут возвращены методом .parameters() и, следовательно, только они будут обновляться оптимизатором (например, SGD или Adam).
- Если объект является экземпляром nn.Module, он регистрируется в словаре \_modules, что позволяет строить вложенные иерархические структуры.
- Если объект является обычным torch.Tensor, он сохраняется как простой атрибут и игнорируется при обратном распространении ошибки (backpropagation) в контексте обучения весов (хотя градиенты через него могут протекать, сам он не считается обучаемым параметром модели).

Эта дихотомия между Parameter и Tensor является первым камнем преткновения при ручной сборке слоев. Частая ошибка новичков — инициализация весов как простых тензоров (self.w \= torch.randn(...)), что приводит к «замороженной» модели, которая не обучается, так как оптимизатор не видит этих весов.

### **2.2 Роль nn.Parameter**

Класс torch.nn.Parameter является оберткой над torch.Tensor. Его единственная, но фундаментальная функция — служить маркером для nn.Module. При добавлении в атрибуты модуля, Parameter сигнализирует системе: «Я — обучаемая величина, сохраните меня в state_dict, передайте меня оптимизатору и отслеживайте мои градиенты».  
По умолчанию Parameter имеет атрибут requires_grad=True. Это означает, что при построении динамического вычислительного графа (Dynamic Computational Graph) во время прямого прохода (forward pass), любые операции с этим параметром будут записываться в историю вычислений, что позволит движку Autograd вычислить производные функции потерь по этому параметру во время обратного прохода.

### **2.3 Состояние и сериализация**

Еще один аспект, который мы берем под ручной контроль — это сериализация. Метод state_dict() рекурсивно обходит дерево модулей и собирает все зарегистрированные параметры. В нашем случае, создавая CustomLinear и CustomConv2d, мы обязаны гарантировать, что наши веса и смещения имеют корректные имена и типы, чтобы модель можно было сохранить и загрузить. Если мы используем некорректные типы данных (например, списки Python вместо nn.ParameterList), механизм сохранения состояния сломается.

## **3\. Компонент I: Аффинные преобразования (Слой Linear)**

### **3.1 Математическая формулировка**

Полносвязный (линейный) слой реализует аффинное преобразование пространства входных признаков. Пусть x — входной вектор размерности D\_{in}, W — матрица весов, а b — вектор смещения. Операция описывается уравнением:  
Здесь кроется важный нюанс реализации в PyTorch, отличающий её от стандартной записи в учебниках по линейной алгебре, где часто пишут y \= Wx. В глубоком обучении данные обычно обрабатываются батчами (пакетами). Входной тензор X имеет размерность (N, D\_{in}), где N — размер батча. Чтобы получить выходной тензор Y размерности (N, D\_{out}), мы должны умножить X на матрицу весов. Для корректного матричного умножения матрица весов W должна иметь размерность, позволяющую операцию (N, D\_{in}) \\times (D\_{in}, D\_{out}). Однако, в PyTorch принято хранить веса линейного слоя в форме (D\_{out}, D\_{in}). Это делается для упрощения внутренней логики и согласованности с другими слоями, где выходные каналы часто идут первыми. Следовательно, в методе forward мы обязаны транспонировать матрицу весов перед умножением, либо использовать функцию F.linear, которая делает это неявно.

### **3.2 Реализация CustomLinear: От теории к коду**

#### **3.2.1 Конструктор и инициализация памяти**

При создании класса CustomLinear, наследуемого от nn.Module, первым шагом является определение тензоров для весов и смещений.  
`class CustomLinear(nn.Module):`  
 `def __init__(self, in_features, out_features, bias=True):`  
 `super().__init__()`  
 `self.in_features = in_features`  
 `self.out_features = out_features`

        `# Регистрация параметров`
        `# Веса хранятся как (out_features, in_features)`
        `self.weight = nn.Parameter(torch.Tensor(out_features, in_features))`

        `if bias:`
            `self.bias = nn.Parameter(torch.Tensor(out_features))`
        `else:`
            `# Механизм регистрации отсутствующего параметра`
            `self.register_parameter('bias', None)`

        `# Критически важный шаг: инициализация значений`
        `self.reset_parameters()`

Вызов super().\_\_init\_\_() обязателен для инициализации внутренних структур данных nn.Module (тех самых словарей \_parameters). Без этого вызова попытка присвоить self.weight \= nn.Parameter(...) вызовет ошибку AttributeError: cannot assign parameter before Module.\_\_init\_\_() call.

#### **3.2.2 Стратегия инициализации весов**

Просто выделить память (torch.Tensor(...)) недостаточно; она будет содержать «мусор» (неинициализированные значения), которые могут быть бесконечно большими или NaN. Нам нужна строгая математическая стратегия инициализации.  
Стандартный слой nn.Linear в PyTorch использует модифицированную инициализацию Кайминга (Kaiming/He) для равномерного распределения. Границы распределения определяются как \\pm \\frac{1}{\\sqrt{D\_{in}}}.  
Эта формула выводится из условия сохранения дисперсии активаций при прохождении через слой. Если дисперсия весов слишком велика, сигналы будут экспоненциально расти (взрыв градиентов); если слишком мала — затухать.  
 `def reset_parameters(self):`  
 `# Инициализация Kaiming Uniform`  
 `init.kaiming_uniform_(self.weight, a=math.sqrt(5))`  
 `if self.bias is not None:`  
 `# Инициализация bias также зависит от fan_in`  
 `fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)`  
 `bound = 1 / math.sqrt(fan_in)`  
 `init.uniform_(self.bias, -bound, bound)`

Игнорирование этого шага — самая распространенная причина, по которой кастомные сети не обучаются. Инициализация нулями (zero initialization) приведет к тому, что все нейроны будут вычислять одну и ту же функцию, и симметрия весов никогда не будет нарушена (symmetry breaking problem).

#### **3.2.3 Прямой проход (Forward Pass)**

Метод forward определяет вычислительный граф. Здесь мы явно видим разницу между объектно-ориентированным и функциональным подходами.  
 `def forward(self, input):`  
 `# Вариант 1: Явное матричное умножение (наиболее "ручной" способ)`  
 `# input: (Batch, In)`  
 `# weight: (Out, In) -> weight.t(): (In, Out)`  
 `# result: (Batch, Out) + bias (broadcasting)`  
 `return torch.matmul(input, self.weight.t()) + self.bias`

        `# Вариант 2: Использование функционального API (более эффективно)`
        `# return F.linear(input, self.weight, self.bias)`

Использование torch.matmul демонстрирует понимание линейной алгебры. Операция сложения \+ self.bias использует механизм broadcasting (расширения размерностей): вектор смещения размера (Out) автоматически виртуально копируется для каждого элемента батча, чтобы соответствовать размерности (Batch, Out).

## **4\. Компонент II: Пространственная фильтрация (Слой Conv2d)**

### **4.1 Свертка или Корреляция?**

В терминологии глубокого обучения операция, называемая «сверткой», математически является взаимной корреляцией (cross-correlation). Классическая свертка требует зеркального отражения ядра относительно осей перед наложением. В нейронных сетях это отражение не выполняется, так как веса фильтра являются обучаемыми параметрами: сеть сама «выучит» перевернутое ядро, если это будет необходимо для минимизации ошибки. Это упрощение избавляет от лишних вычислений без потери выразительной способности.

### **4.2 Многомерная геометрия тензоров**

Реализация сверточного слоя на порядок сложнее линейного из\-за работы с 4D-тензорами и скользящим окном. Входной тензор имеет форму (N, C\_{in}, H\_{in}, W\_{in}). Выходной тензор — (N, C\_{out}, H\_{out}, W\_{out}).  
Самое важное — это форма тензора весов (ядра свертки). В PyTorch она определяется как:  
Где K_H, K_W — высота и ширина ядра (например, 3 \\times 3).  
Почему именно такой порядок?

1. **C\_{out} (первая размерность):** Это количество фильтров. Каждый фильтр ответственен за создание одной карты признаков (feature map) на выходе.
2. **C\_{in} (вторая размерность):** Каждый фильтр является трехмерным (в пространстве каналов). Он простирается на всю глубину входного объема. Если на входе RGB-изображение (C\_{in}=3), то каждый из C\_{out} фильтров имеет глубину 3\. Это позволяет фильтру искать паттерны, сочетающие информацию из красного, зеленого и синего каналов одновременно.

### **4.3 Реализация CustomConv2d**

#### **4.3.1 Определение гиперпараметров**

Конструктор должен принимать и сохранять параметры, определяющие геометрию свертки: kernel_size, stride, padding, dilation.  
`class CustomConv2d(nn.Module):`  
 `def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1):`  
 `super().__init__()`  
 `# Приведение аргументов к кортежам (для поддержки прямоугольных ядер)`  
 `self.kernel_size = _pair(kernel_size)`  
 `self.stride = _pair(stride)`  
 `self.padding = _pair(padding)`  
 `self.dilation = _pair(dilation)`

        `# Инициализация параметров`
        `self.weight = nn.Parameter(torch.Tensor(`
            `out_channels, in_channels, *self.kernel_size`
        `))`
        `self.bias = nn.Parameter(torch.Tensor(out_channels))`

        `self.reset_parameters()`

Здесь функция \_pair (аналог внутренней утилиты PyTorch) нужна для того, чтобы пользователь мог передать как одно число 3 (подразумевая 3 \\times 3), так и кортеж (3, 5).

#### **4.3.2 Магия im2col и эффективность вычислений**

Как реализовать forward? Наивная реализация с четырьмя вложенными циклами Python (по батчу, каналам, высоте, ширине) будет катастрофически медленной. В профессиональных реализациях используется подход **im2col (image to column)** или **Unfold**. Суть метода: локальные патчи входного изображения, на которые накладывается ядро, разворачиваются в векторы-столбцы. Ядра свертки также вытягиваются в строки. После этого операция свертки превращается в одно большое матричное умножение (GEMM \- General Matrix Multiply). Хотя PyTorch предоставляет nn.Unfold, для нашего кастомного слоя мы воспользуемся функциональным интерфейсом F.conv2d. Это не нарушает условия «ручной сборки», так как мы самостоятельно управляем параметрами и архитектурой слоя, делегируя лишь низкоуровневую математическую операцию оптимизированному C++/CUDA бэкенду.  
 `def forward(self, input):`  
 `return F.conv2d(input, self.weight, self.bias,`  
 `self.stride, self.padding, self.dilation)`

#### **4.3.3 Расчет выходных размерностей**

Для корректного построения архитектуры необходимо точно знать, как меняется размер пространственной карты признаков. Формула для расчета высоты выхода H\_{out}:  
Эта арифметика критически важна при переходе от сверточных слоев к полносвязным (операция Flatten). Если расчет неверен, мы получим ошибку несовпадения размерностей матриц.

## **5\. Архитектура: Возведение «Башни Прозрения»**

### **5.1 Контекст задачи: MNIST**

Датасет MNIST (Modified National Institute of Standards and Technology) является «дрозофилой» глубокого обучения. Он содержит 60,000 обучающих и 10,000 тестовых изображений рукописных цифр размером 28 \\times 28 пикселей в градациях серого (1 канал). Для человека эта задача тривиальна, но для машины она требует выделения иерархических признаков: от простых краев и линий до дуг и петель, формирующих цифры. Сверточные сети (CNN) идеально подходят для этого благодаря инвариантности к сдвигам и локальности восприятия.

### **5.2 Проектирование топологии сети**

Наша «Башня» будет состоять из двух основных блоков свертки и пулинга, за которыми следует классификатор (голова сети). Мы будем использовать следующую конфигурацию:  
**Таблица 1: Спецификация слоев «Башни Прозрения»**

| Слой        | Тип          | Входная форма    | Конфигурация                  | Выходная форма   | Комментарий                                |
| :---------- | :----------- | :--------------- | :---------------------------- | :--------------- | :----------------------------------------- |
| **Input**   | \-           | (N, 1, 28, 28\)  | \-                            | \-               | Grayscale image                            |
| **Conv1**   | CustomConv2d | (N, 1, 28, 28\)  | K=3\\times3, S=1, P=1, Out=32 | (N, 32, 28, 28\) | Padding=1 сохраняет размер 28\\times28     |
| **Relu1**   | Activation   | (N, 32, 28, 28\) | \-                            | (N, 32, 28, 28\) | Нелинейность                               |
| **Pool1**   | MaxPool2d    | (N, 32, 28, 28\) | K=2\\times2, S=2              | (N, 32, 14, 14\) | Уменьшение размерности в 2 раза            |
| **Conv2**   | CustomConv2d | (N, 32, 14, 14\) | K=3\\times3, S=1, P=1, Out=64 | (N, 64, 14, 14\) | Увеличение числа каналов                   |
| **Relu2**   | Activation   | (N, 64, 14, 14\) | \-                            | (N, 64, 14, 14\) | \-                                         |
| **Pool2**   | MaxPool2d    | (N, 64, 14, 14\) | K=2\\times2, S=2              | (N, 64, 7, 7\)   | Финальный размер карты признаков 7\\times7 |
| **Flatten** | Reshape      | (N, 64, 7, 7\)   | \-                            | (N, 3136\)       | 64 \\times 7 \\times 7 \= 3136             |
| **FC1**     | CustomLinear | (N, 3136\)       | Out=128                       | (N, 128\)        | Сжатие признаков                           |
| **Relu3**   | Activation   | (N, 128\)        | \-                            | (N, 128\)        | \-                                         |
| **FC2**     | CustomLinear | (N, 128\)        | Out=10                        | (N, 10\)         | Логиты для 10 классов цифр                 |

### **5.3 Интеграция кода «Башни»**

Класс TowerOfInsight объединяет наши кастомные модули.  
`class TowerOfInsight(nn.Module):`  
 `def __init__(self):`  
 `super(TowerOfInsight, self).__init__()`

        `# Блок 1: Свертка`
        `# Мы используем наши кастомные классы, а не nn.Conv2d`
        `self.conv1 = CustomConv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)`

        `# Блок 2: Свертка`
        `self.conv2 = CustomConv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)`

        `# Расчет размера входа для линейного слоя`
        `# Вход: 28x28 -> Pool(2) -> 14x14 -> Pool(2) -> 7x7`
        `# Каналы: 64`
        `# Flatten size = 64 * 7 * 7 = 3136`
        `self.flatten_size = 64 * 7 * 7`

        `# Полносвязные слои (MLP)`
        `self.fc1 = CustomLinear(in_features=self.flatten_size, out_features=128)`
        `self.fc2 = CustomLinear(in_features=128, out_features=10)`

    `def forward(self, x):`
        `# Проход через первый сверточный блок`
        `x = self.conv1(x)       # CustomConv2d`
        `x = F.relu(x)           # Functional ReLU (без состояния)`
        `x = F.max_pool2d(x, 2)  # Functional MaxPool`

        `# Проход через второй сверточный блок`
        `x = self.conv2(x)`
        `x = F.relu(x)`
        `x = F.max_pool2d(x, 2)`

        `# Выравнивание (Flattening)`
        `# x.size(0) - это размер батча N`
        `x = x.view(x.size(0), -1)`

        `# Проход через классификатор`
        `x = self.fc1(x)`
        `x = F.relu(x)`
        `x = self.fc2(x)`

        `return x # Возвращаем логиты, Softmax будет в функции потерь`

Использование x.view(x.size(0), \-1) — это стандартный паттерн для операции Flatten в PyTorch. Аргумент \-1 заставляет PyTorch автоматически вычислить оставшуюся размерность, что должно совпасть с нашим рассчитанным 3136\. Если расчеты выше неверны, view выбросит ошибку несоответствия количества элементов.

## **6\. Стратегии инициализации: Математика хаоса и порядка**

### **6.1 Проблема затухающих градиентов**

Почему мы уделили такое внимание методу reset_parameters в наших кастомных слоях? Рассмотрим глубокую сеть. При обратном распространении ошибки градиент для весов первого слоя вычисляется как произведение локальных градиентов всех последующих слоев (цепочечное правило). Если веса инициализированы слишком малыми значениями (например, из N(0, 0.01)), то при умножении множества малых чисел результат стремится к нулю. Градиент «затухает», и первые слои перестают обучаться. Если веса слишком велики, происходит обратное — «взрыв» градиентов, приводящий к нестабильности и переполнению чисел с плавающей точкой (NaN).

### **6.2 Xavier (Glorot) vs. Kaiming (He)**

Долгое время стандартом была инициализация **Xavier**, предложенная Глоротом. Она работает хорошо для линейных функций активации (как Tanh или Sigmoid в их линейной зоне). Однако для ReLU (Rectified Linear Unit), который зануляет половину активаций (все отрицательные значения), дисперсия сигнала падает вдвое на каждом слое. Кайминг Хе (Kaiming He) предложил поправку: чтобы компенсировать зануление половины нейронов, нужно увеличить дисперсию весов в 2 раза. Формула Kaiming Normal:  
Именно эту логику мы реализовали в наших слоях CustomConv2d и CustomLinear через init.kaiming_uniform\_ с параметром a=math.sqrt(5) (значение по умолчанию в PyTorch для Leaky ReLU, хотя для чистого ReLU часто используют просто \\sqrt{2}).

### **6.3 Значение Bias**

Инициализация векторов смещения (bias) часто недооценивается. В PyTorch для nn.Linear и nn.Conv2d смещения инициализируются не нулями, а равномерным распределением в диапазоне \\pm \\frac{1}{\\sqrt{fan\\\_in}}. Это делается для того, чтобы на старте обучения нейроны не были "мертвыми" и имели ненулевую вероятность активации, хотя на практике инициализация нулем также часто работает приемлемо для смещений (но не для весов\!).

## **7\. Экосистема обучения и валидации**

### **7.1 Цикл обучения (The Training Loop)**

Имея «Башню Прозрения», собранную вручную, мы помещаем её в стандартный цикл обучения PyTorch. Критически важно, что оптимизатор (torch.optim.Adam или SGD) принимает на вход model.parameters(). Поскольку мы корректно использовали nn.Parameter внутри наших кастомных классов, этот вызов вернет итератор по всем тензорам весов и смещений, которые мы создали. Если бы мы использовали просто self.w \= torch.tensor(...), список был бы пуст, и обучение не происходило бы.  
`# Гиперпараметры`  
`learning_rate = 0.001`  
`epochs = 5`  
`batch_size = 64`

`# Инициализация`  
`model = TowerOfInsight()`  
`criterion = nn.CrossEntropyLoss()`  
`optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)`  
`train_loader =... # DataLoader для MNIST`

`# Цикл`  
`for epoch in range(epochs):`  
 `model.train() # Переключение в режим обучения`  
 `for batch_idx, (data, target) in enumerate(train_loader):`  
 `# 1. Обнуление градиентов (накопителей)`  
 `optimizer.zero_grad()`

        `# 2. Прямой проход`
        `output = model(data)`

        `# 3. Вычисление ошибки`
        `loss = criterion(output, target)`

        `# 4. Обратный проход (Autograd строит граф от loss к параметрам)`
        `loss.backward()`

        `# 5. Шаг оптимизации (обновление весов)`
        `optimizer.step()`

### **7.2 Функция потерь CrossEntropy**

Мы используем nn.CrossEntropyLoss. Важно отметить, что этот класс в PyTorch ожидает на входе **логиты** (необработанные выходы линейного слоя), а не вероятности. Внутри он применяет LogSoftmax и затем NLLLoss. Если бы мы добавили Softmax в конец нашего TowerOfInsight, обучение было бы нестабильным из\-за численных погрешностей при взятии логарифма от малых вероятностей. Наш кастомный слой CustomLinear выдает именно то, что нужно — сырые проекции признаков.

## **8\. Проблемы производительности и экспорта**

### **8.1 Сравнение с нативной реализацией**

Насколько наша ручная сборка медленнее оптимизированных слоев C++? На этапе **выполнения** разница минимальна. Поскольку мы используем те же базовые операции (torch.matmul, F.conv2d), вычисления происходят на тех же ядрах CUDA/AVX. Накладные расходы возникают только на уровне интерпретатора Python (overhead), который должен вызывать эти функции. Для крупных моделей, где время вычисления свертки занимает 99% времени, эти накладные расходы незаметны. Однако, использование nn.Sequential и стандартных слоев позволяет применять продвинутые оптимизации JIT-компиляции (TorchScript) более эффективно.

### **8.2 ONNX и квантование**

Главный недостаток кастомных слоев проявляется при попытке экспортировать модель в формат ONNX (для запуска на мобильных устройствах или в браузере) или при квантовании (переводе весов в int8).

- **ONNX:** Экспортер может не распознать высокоуровневую семантику нашего CustomConv2d как единую операцию свертки, если мы добавим туда нестандартную логику. Хотя в данном случае, так как мы вызываем F.conv2d, трейсер (tracer) скорее всего корректно запишет это как узел Conv в графе.
- **Квантование:** Инструменты автоматического квантования PyTorch ищут известные паттерны (например, Conv2d \+ ReLU) для их слияния (fusion). Наши кастомные классы не будут распознаны этими паттернами, что сделает автоматическое квантование невозможным без написания дополнительных правил обработки.

## **9\. Заключение**

Проект «Башня Прозрения» успешно демонстрирует, что нейронная сеть — это не магический черный ящик, а строго детерминированная система тензорных операций. Мы убедились, что:

1. **Регистрация параметров** через nn.Parameter является единственным мостом между данными и оптимизатором.
2. **Геометрия тензоров** требует скрупулезного расчета размерностей, особенно при переходе от сверточных слоев к линейным.
3. **Инициализация весов** — это не просто случайные числа, а математически обоснованное распределение, критичное для динамики обучения.
4. **Ручная сборка** — мощнейший педагогический инструмент, дающий понимание того, как работает Autograd, как хранятся веса в памяти и как строятся вычислительные графы.

Хотя в промышленной разработке (production) следует предпочитать стандартные слои torch.nn ради поддержки экосистемы (ONNX, Quantization, Pruning), опыт ручной сборки дает инженеру необходимую интуицию для отладки сложных архитектур и разработки принципиально новых типов нейронных слоев. «Башня Прозрения» стоит прочно, потому что мы знаем каждый кирпич в её основании.

### **Приложение: Полный листинг архитектуры**

Для воспроизводимости результатов ниже приведена сводная таблица параметров тензоров, создаваемых нашей моделью (примерный вид model.state_dict()):  
**Таблица 2: Карта памяти параметров «Башни»**

| Имя параметра | Форма (Shape)   | Тип инициализации | Кол-во элементов         |
| :------------ | :-------------- | :---------------- | :----------------------- |
| conv1.weight  | (32, 1, 3, 3\)  | Kaiming Uniform   | 288                      |
| conv1.bias    | (32)            | Uniform           | 32                       |
| conv2.weight  | (64, 32, 3, 3\) | Kaiming Uniform   | 18,432                   |
| conv2.bias    | (64)            | Uniform           | 64                       |
| fc1.weight    | (128, 3136\)    | Kaiming Uniform   | 401,408                  |
| fc1.bias      | (128)           | Uniform           | 128                      |
| fc2.weight    | (10, 128\)      | Kaiming Uniform   | 1,280                    |
| fc2.bias      | (10)            | Uniform           | 10                       |
| **Итого**     |                 |                   | **\~421,642 параметров** |

Эта таблица наглядно показывает, что основная масса параметров (около 95%) сосредоточена в первом полносвязном слое fc1, что типично для архитектур типа LeNet/VGG и подчеркивает важность сверточных слоев для снижения количества параметров по сравнению с полносвязными сетями.

#### **Источники**

1\. Conv2d — PyTorch 2.9 documentation, https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv2d.html 2\. Maybe I found something strange on pytorch, which result in property setter not working, https://stackoverflow.com/questions/61116433/maybe-i-found-something-strange-on-pytorch-which-result-in-property-setter-not 3\. Why do we need to register parameters in pytorch when using nn modules?, https://discuss.pytorch.org/t/why-do-we-need-to-register-parameters-in-pytorch-when-using-nn-modules/5996 4\. nn.Module add new parameter, setattr() VS register_parameter() \- PyTorch Forums, https://discuss.pytorch.org/t/nn-module-add-new-parameter-setattr-vs-register-parameter/108238 5\. Creating Custom Layers and Loss Functions in PyTorch \- MachineLearningMastery.com, https://machinelearningmastery.com/creating-custom-layers-loss-functions-pytorch/ 6\. What is the difference between nn.Linear and nn.functional.Linear ? And which one should be used in which case ? : r/pytorch \- Reddit, https://www.reddit.com/r/pytorch/comments/j6mikd/what\_is\_the\_difference\_between\_nnlinear\_and/ 7\. Linear — PyTorch 2.9 documentation, https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html 8\. Clarity on default initialization in pytorch, https://discuss.pytorch.org/t/clarity-on-default-initialization-in-pytorch/84696 9\. Weight Initialization Techniques in Neural Networks \- Pinecone, https://www.pinecone.io/learn/weight-initialization/ 10\. 7.3. Padding and Stride — Dive into Deep Learning 1.0.3 documentation, https://d2l.ai/chapter\_convolutional-neural-networks/padding-and-strides.html 11\. Conv2d layer dimension order using shape \- PyTorch Forums, https://discuss.pytorch.org/t/conv2d-layer-dimension-order-using-shape/114054 12\. torch.nn.functional.conv2d — PyTorch 2.9 documentation, https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.conv2d.html 13\. Can't find the input order (kernel) for F.conv2d in doc and I am confused with passage in the book \- fastai, https://forums.fast.ai/t/cant-find-the-input-order-kernel-for-f-conv2d-in-doc-and-i-am-confused-with-passage-in-the-book/80345 14\. MaxPool2d — PyTorch 2.9 documentation, https://docs.pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html 15\. How to calculate the output size after Conv2d in pytorch?, https://discuss.pytorch.org/t/how-to-calculate-the-output-size-after-conv2d-in-pytorch/20405 16\. PyTorch Neural Network and Dataset Tutorial Using MNIST \- GitHub, https://github.com/mpirrall/pytorch-neural-network-tutorial-using-MNIST 17\. PyTorch neural network parameters and tensor shapes, https://discuss.pytorch.org/t/pytorch-neural-network-parameters-and-tensor-shapes/54689 18\. Maxpool2d output size inconsistent with formula \- PyTorch Forums, https://discuss.pytorch.org/t/maxpool2d-output-size-inconsistent-with-formula/86609 19\. Demystifying Weight Initialization Methods in Neural Networks | by Shaoni Mukherjee, https://medium.com/@shaomukherjee/demystifying-weight-initialization-methods-in-neural-networks-96042d2447f1 20\. Don't Trust PyTorch to Initialize Your Variables | Aditya Rana Blog, https://adityassrana.github.io/blog/theory/2020/08/26/Weight-Init.html 21\. How do I initialize weights in PyTorch? | by why amit | Medium, https://medium.com/@whyamit101/how-do-i-initialize-weights-in-pytorch-1dd37078d19b 22\. Learning PyTorch: The Basic Program Structure | by Dagang Wei \- Medium, https://medium.com/@weidagang/learning-pytorch-the-basic-program-structure-ed5723118b67 23\. Efficient Deep Learning Model Training in PyTorch \- Artificial Intelligence in Plain English, https://ai.plainenglish.io/efficient-deep-learning-model-training-in-pytorch-f597a7dc7ba0 24\. A Primer on Functional PyTorch \- Medium, https://medium.com/data-scientists-diary/a-primer-on-functional-pytorch-89f1fb6d7673 25\. Do we have lower performance and accuracy than when not using \`pytorch.nn.Sequnetial\` and if yes, why? \- Stack Overflow, https://stackoverflow.com/questions/67357795/do-we-have-lower-performance-and-accuracy-than-when-not-using-pytorch-nn-sequne 26\. What are the limitations of ONNX models compared to PyTorch models? \- Massed Compute, https://massedcompute.com/faq-answers/?question=What%20are%20the%20limitations%20of%20ONNX%20models%20compared%20to%20PyTorch%20models? 27\. Edge AI: TensorFlow Lite vs. ONNX Runtime vs. PyTorch Mobile \- DZone, https://dzone.com/articles/edge-ai-tensorflow-lite-vs-onnx-runtime-vs-pytorch 28\. When a quantized model is exported using onnx.export, the convolution result has discrepency with the original quantized model. · Issue \#146541 · pytorch/pytorch \- GitHub, https://github.com/pytorch/pytorch/issues/146541 29\. ONNX export of simple quantized model fails \- PyTorch Forums, https://discuss.pytorch.org/t/onnx-export-of-simple-quantized-model-fails/191406
