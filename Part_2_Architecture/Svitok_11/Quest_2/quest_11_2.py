# === quest_11_2.py ===
# Имя этого пергамента, хранящего ритуал сборки "Совета Мудрецов".
# Квест: 11.2 - Сборка "Многогранного Фокуса"
# Каноническое имя Квеста, как оно записано в Великом Кодексе.
# Цель: Реализовать с нуля Multi-Head Attention. Это ключевой механизм,
# Священная цель нашего ритуала.
# который позволяет Трансформеру "смотреть" на предложение под разными
# Детальное описание цели.
# "углами" одновременно, улавливая разные типы связей.
# Продолжение детального описания.

# --- Легенда Квеста: Совет Мудрецов ---
# Здесь начинается Легенда, объясняющая суть магии, которую мы сейчас сотворим.
# Self-Attention - это один мудрец, смотрящий на мир.
# Толкование первого элемента Легенды.
# Multi-Head Attention - это Совет из нескольких мудрецов. Мы "разрезаем"
# Толкование второго элемента Легенды.
# "душу" (эмбеддинг) каждого слова на части и раздаем каждому мудрецу
# Продолжение толкования.
# по фрагменту. Каждый мудрец анализирует мир через свою, уникальную призму.
# Продолжение толкования.
# Затем мы собираем их выводы вместе, получая невероятно богатое,
# Продолжение толкования.
# многогранное понимание контекста.
# Финальное толкование Легенды.

# Мы призываем гримуар математики `math` для вычисления корня.
import math

# --- Акт 1: Подготовка Гримуаров ---
# Начинается первый акт: мы призываем все необходимые знания и инструменты.
# Мы призываем `torch` — наш Источник Маны.
import torch
# Мы призываем `torch.nn` (с псевдонимом `nn`) — главу с чертежами базовых блоков.
import torch.nn as nn

# --- Акт 2: Чертеж Нашего "Совета Мудрецов" ---
# Начинается второй, самый важный акт: мы создаем "чертеж" для нашего артефакта.


# Мы начинаем создание "чертежа" (класса) для нашего артефакта, наследуя базовую магию от `nn.Module`.
class MultiHeadAttention(nn.Module):

    # Мы определяем заклинание Инициализации (`__init__`), которое срабатывает при сотворении артефакта.
    # Его задача — создать все необходимые "внутренние механизмы".
    def __init__(self, d_model, num_heads):
        # Мы произносим обязательное заклинание, пробуждающее магию родительского класса.
        super().__init__()

        # Мы проверяем, делится ли "глубина смысла" нацело на количество "мудрецов".
        assert d_model % num_heads == 0, "d_model must be divisible by num_heads"

        # --- Сохраняем ключевые параметры ---
        # Мы сохраняем общую "глубину смысла" ауры, например, 512.
        self.d_model = d_model
        # Мы сохраняем количество "мудрецов" (голов), например, 8.
        self.num_heads = num_heads
        # Мы вычисляем "глубину смысла" для каждого отдельного мудреца, например, 512 / 8 = 64.
        self.d_k = (
            d_model // num_heads
        )

        # --- Создаем внутренние "Рунные Камни" (Linear слои) ---
        # Эти три камня берут исходную "ауру" и создают из нее три проекции:
        # Запрос, Ключ и Значение.
        # Мы создаем "Рунный Камень" для Запросов (Queries).
        self.W_q = nn.Linear(d_model, d_model)
        # Мы создаем "Рунный Камень" для Ключей (Keys).
        self.W_k = nn.Linear(d_model, d_model)
        # Мы создаем "Рунный Камень" для Значений (Values).
        self.W_v = nn.Linear(d_model, d_model)

        # Этот финальный камень соберет "просветленные" ауры от всех мудрецов
        # и вынесет итоговое, объединенное суждение.
        self.W_o = nn.Linear(d_model, d_model)

    # Это наш старый знакомый ритуал из Квеста 11.1.
    # Теперь он является "внутренним заклинанием" нашего артефакта.
    def scaled_dot_product_attention(self, Q, K, V):
        # Мы вычисляем "симпатию" между Запросами и Ключами.
        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        # Мы превращаем "симпатию" в "проценты внимания".
        attn_probs = torch.softmax(attn_scores, dim=-1)
        # Мы смешиваем "коктейль", взвешивая Значения на "проценты внимания".
        output = torch.matmul(attn_probs, V)
        # Мы возвращаем "сфокусированную" ауру.
        return output

    # Мы определяем заклинание "Разделения Смысла".
    def split_heads(self, x):
        # Мы получаем размеры входящей "ауры".
        batch_size, seq_length, d_model = x.size()
        # Мы "нарезаем" последнюю, 512-мерную ось на `num_heads` (8) частей
        # по `d_k` (64) измерений в каждой. Затем меняем оси местами,
        # чтобы у каждого мудреца была своя стопка "смыслов".
        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)

    # Мы определяем заклинание "Великого Единения".
    def combine_heads(self, x):
        # Мы получаем размеры от "мудрецов".
        batch_size, _, seq_length, d_k = x.size()
        # Мы собираем "нарезанные" части обратно в единую, целостную "ауру".
        # `.contiguous()` — это техническая руна, которая упорядочивает память для `view`.
        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)

    # Мы определяем главное заклинание артефакта, которое выполняется при его вызове.
    def forward(self, Q, K, V):
        # Шаг 1: Мы пропускаем исходные ауры через три "Камня", чтобы получить Q, K, V.
        Q, K, V = self.W_q(Q), self.W_k(K), self.W_v(V)

        # Шаг 2: Мы "разделяем смысл" — нарезаем Q, K, V на части для каждого "мудреца".
        Q, K, V = self.split_heads(Q), self.split_heads(K), self.split_heads(V)

        # Шаг 3: Мы запускаем ритуал "Фокуса" одновременно для всех "мудрецов".
        attention_output = self.scaled_dot_product_attention(Q, K, V)

        # Шаг 4: Мы "склеиваем" просветленные выводы от всех мудрецов в единую ауру.
        output = self.combine_heads(attention_output)
        # Шаг 5: Мы пропускаем эту единую, обогащенную ауру через финальный "Камень".
        output = self.W_o(output)

        # Мы возвращаем финальную, обогащенную ауру.
        return output


# --- Акт 3: Испытание "Совета Мудрецов" ---
# Начинается третий акт: мы испытываем наш артефакт в деле.

# Мы задаем параметры нашего испытания.
# Мы устанавливаем общую "глубину смысла" каждой ауры.
d_model = 512
# Мы постановляем собрать совет из 8 "мудрецов".
num_heads = 8
# Мы определяем, что наше условное предложение состоит из 10 слов.
seq_length = 10
# Мы определяем, что будем анализировать 1 предложение за раз.
batch_size = 1

# Мы создаем случайный тензор, имитирующий "ауры" нашего предложения.
x = torch.randn(batch_size, seq_length, d_model)

# Мы оглашаем на кристалл о начале испытания.
print("--- Испытание 'Многогранного Фокуса' ---")
# Мы сотворяем наш артефакт по чертежу.
multi_head_attention = MultiHeadAttention(d_model, num_heads)
# Мы запускаем ритуал. Для Self-Attention мы подаем один и тот же тензор 'x'
# на все три входа: Q, K, и V.
output = multi_head_attention(x, x, x)

# Мы оглашаем на кристалл форму входного тензора.
print("Форма входного тензора (1 предложение, 10 слов, 512 измерений смысла):")
# `.shape` — это заклинание, показывающее размеры тензора.
print(x.shape)
# Мы оглашаем на кристалл форму выходного тензора.
print("\nФорма выходного, 'просветленного' тензора:")
# Мы печатаем форму выходного тензора.
print(output.shape)
# Мы оглашаем на кристалл, что ритуал завершен успешно.
print(
    "\nРитуал завершен! Размеры входа и выхода совпадают, значит, наш артефакт работает!"
)
