# === quest_11_2.py ===
# Квест: 11.2 - Сборка "Многогранного Фокуса"
# Цель: Реализовать с нуля Multi-Head Attention. Это ключевой механизм,
# который позволяет Трансформеру "смотреть" на предложение под разными
# "углами" одновременно, улавливая разные типы связей.

# --- Легенда Квеста: Совет Мудрецов ---
# Self-Attention - это один мудрец, смотрящий на мир.
# Multi-Head Attention - это Совет из нескольких мудрецов. Мы "разрезаем"
# "душу" (эмбеддинг) каждого слова на части и раздаем каждому мудрецу
# по фрагменту. Каждый мудрец анализирует мир через свою, уникальную призму.
# Затем мы собираем их выводы вместе, получая невероятно богатое,
# многогранное понимание контекста.

import math  # Призываем гримуар математики для вычисления корня

# --- Акт 1: Подготовка Гримуаров ---
import torch
import torch.nn as nn

# --- Акт 2: Чертеж Нашего "Совета Мудрецов" ---


# Создаем "чертеж" для нашего артефакта, наследуя базовую магию от nn.Module.
class MultiHeadAttention(nn.Module):

    # Заклинание Инициализации (__init__): срабатывает при сотворении артефакта.
    # Его задача - создать все необходимые "внутренние механизмы".
    def __init__(self, d_model, num_heads):
        super().__init__()  # Пробуждаем магию родительского класса.

        # Проверяем, делится ли "глубина смысла" нацело на количество "мудрецов".
        assert d_model % num_heads == 0, "d_model must be divisible by num_heads"

        # --- Сохраняем ключевые параметры ---
        self.d_model = d_model  # Общая "глубина смысла" ауры, например, 512.
        self.num_heads = num_heads  # Количество "мудрецов" (голов), например, 8.
        self.d_k = (
            d_model // num_heads
        )  # "Глубина смысла" для каждого мудреца, например, 512 / 8 = 64.

        # --- Создаем внутренние "Рунные Камни" (Linear слои) ---
        # Эти три камня берут исходную "ауру" и создают из нее три проекции:
        # Запрос, Ключ и Значение.
        self.W_q = nn.Linear(d_model, d_model)  # Камень для Запросов (Queries)
        self.W_k = nn.Linear(d_model, d_model)  # Камень для Ключей (Keys)
        self.W_v = nn.Linear(d_model, d_model)  # Камень для Значений (Values)

        # Этот финальный камень соберет "просветленные" ауры от всех мудрецов
        # и вынесет итоговое, объединенное суждение.
        self.W_o = nn.Linear(d_model, d_model)

    # Это наш старый знакомый ритуал из Квеста 11.1.
    # Теперь он является "внутренним заклинанием" нашего артефакта.
    def scaled_dot_product_attention(self, Q, K, V):
        # Вычисляем "симпатию" между Запросами и Ключами.
        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        # Превращаем "симпатию" в "проценты внимания".
        attn_probs = torch.softmax(attn_scores, dim=-1)
        # Смешиваем "коктейль", взвешивая Значения на "проценты внимания".
        output = torch.matmul(attn_probs, V)
        return output

    # Заклинание "Разделения Смысла".
    def split_heads(self, x):
        # Получаем размеры входящей "ауры".
        batch_size, seq_length, d_model = x.size()
        # "Нарезаем" последнюю, 512-мерную ось на `num_heads` (8) частей
        # по `d_k` (64) измерений в каждой. Затем меняем оси местами,
        # чтобы у каждого мудреца была своя стопка "смыслов".
        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)

    # Заклинание "Великого Единения".
    def combine_heads(self, x):
        # Получаем размеры от "мудрецов".
        batch_size, _, seq_length, d_k = x.size()
        # Собираем "нарезанные" части обратно в единую, целостную "ауру".
        # .contiguous() - техническая руна, которая упорядочивает память для view.
        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)

    # Главное заклинание артефакта, которое выполняется при его вызове.
    def forward(self, Q, K, V):
        # 1. Пропускаем исходные ауры через три "Камня", чтобы получить Q, K, V.
        Q, K, V = self.W_q(Q), self.W_k(K), self.W_v(V)

        # 2. "Разделяем смысл" - нарезаем Q, K, V на части для каждого "мудреца".
        Q, K, V = self.split_heads(Q), self.split_heads(K), self.split_heads(V)

        # 3. Запускаем ритуал "Фокуса" одновременно для всех "мудрецов".
        attention_output = self.scaled_dot_product_attention(Q, K, V)

        # 4. "Склеиваем" просветленные выводы от всех мудрецов в единую ауру.
        output = self.combine_heads(attention_output)
        # 5. Пропускаем эту единую, обогащенную ауру через финальный "Камень".
        output = self.W_o(output)

        return output


# --- Акт 3: Испытание "Совета Мудрецов" ---

# Задаем параметры нашего испытания.
d_model = 512  # Общая "глубина смысла" каждой ауры.
num_heads = 8  # Собираем совет из 8 "мудрецов".
seq_length = 10  # Наше условное предложение состоит из 10 слов.
batch_size = 1  # Мы анализируем 1 предложение за раз.

# Создаем случайный тензор, имитирующий "ауры" нашего предложения.
x = torch.randn(batch_size, seq_length, d_model)

print("--- Испытание 'Многогранного Фокуса' ---")
# Сотворяем наш артефакт по чертежу.
multi_head_attention = MultiHeadAttention(d_model, num_heads)
# Запускаем ритуал. Для Self-Attention мы подаем один и тот же тензор 'x'
# на все три входа: Q, K, и V.
output = multi_head_attention(x, x, x)

print("Форма входного тензора (1 предложение, 10 слов, 512 измерений смысла):")
# .shape показывает размеры тензора.
print(x.shape)
print("\nФорма выходного, 'просветленного' тензора:")
print(output.shape)
print(
    "\nРитуал завершен! Размеры входа и выхода совпадают, значит, наш артефакт работает!"
)
