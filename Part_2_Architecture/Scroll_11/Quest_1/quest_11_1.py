# === quest_11_1.py (Финальная Версия с Полными Комментариями) ===
# Имя этого пергамента, хранящего ритуал визуализации магии Внимания.
# Квест: 11.1 - Создание заклинания "Фокуса"
# Каноническое имя Квеста, как оно записано в Великом Кодексе.
# Цель: Визуализировать магию Attention, чтобы интуитивно понять,
# Священная цель нашего ритуала.
# как слова в предложении "смотрят" друг на друга и находят смысловые связи.
# Детальное описание цели.

# --- Легенда Квеста: Эмпатический Разговор ---
# Здесь начинается Легенда, объясняющая суть магии, которую мы сейчас сотворим.
# Представь, что 4 мага ("Король", "сел", "принц", "упал") сидят в комнате.
# Толкование первого элемента Легенды.
# У каждого есть своя "аура" (вектор).
# Толкование второго элемента Легенды.
# Attention - это ритуал, который позволяет КАЖДОМУ магу "почувствовать" ауру
# Толкование третьего элемента Легенды.
# всех остальных и понять, кто из них ему "ближе по духу".
# Продолжение толкования.
# Наша тепловая карта - это визуализация этого "эмпатического поля".
# Указание на финальный артефакт, который мы создадим.

# Мы призываем "Художника" `matplotlib.pyplot` (с псевдонимом `plt`) для
# рисования нашей карты.
import matplotlib.pyplot as plt

# Мы призываем `numpy` (с псевдонимом `np`) для математических операций
# (вычисления корня).
import numpy as np

# --- Акт 1: Подготовка Гримуаров ---
# Начинается первый акт: мы призываем все необходимые знания и инструменты.
# Мы призываем наш главный силовой гримуар `PyTorch`.
import torch

# Мы призываем функциональную часть `PyTorch` (с псевдонимом `F`) для
# заклинания Softmax.
import torch.nn.functional as F

# --- Акт 2: Призыв "Магов" и их "Аур" ---
# Начинается второй акт: мы создаем "ауры" наших магов.
# Мы создаем "ауры" (эмбеддинги) для 4-х условных слов.
# Мы специально делаем ауры "Короля" и "принца" похожими
#  (оба вектора "смотрят" вверх и вправо).
# Мы специально делаем ауры "сел" и "упал" похожими (смотрят вниз).
# `torch.tensor([...])` — это заклинание, создающее тензор из наших данных.
qkv = torch.tensor(
    # Начало списка аур.
    [
        # Первая аура (вектор) для мага "Король".
        [1.0, 1.0, 0.0],
        # Вторая аура для мага "сел".
        [0.0, -1.0, 0.0],
        # Третья аура для мага "принц" (похожа на Ауру 1).
        [0.9, 0.9, 0.0],
        # Четвертая аура для мага "упал" (похожа на Ауру 2).
        [0.1, -0.9, 0.0],
        # Конец списка аур.
    ]
    # Мы добавляем "батч" измерение, чтобы тензор имел форму (1, 4, 3).
).unsqueeze(0)

# Мы оглашаем на кристалл (консоль) заголовок для исходных данных.
print("--- Исходные 'Ауры' наших Магов ---")
# Мы печатаем исходные "ауры", чтобы видеть, с чего мы начали.
print(qkv)
# В ритуале Self-Attention ("внимание к себе") один и тот же тензор
# используется для всех трех ролей: Запроса (Query), Ключа (Key) и
# Значения (Value).

# --- Акт 3: Ритуал "Фокуса Мысли" ---
# Начинается третий, кульминационный акт: мы вычисляем "Карту Внимания".
# Этот акт вычисляет "Карту Внимания", показывающую, кто на кого "смотрит".

# Шаг 3.1: "Оценка Похожести" (Матричное Умножение).
# Мы умножаем матрицу "аур" саму на себя (транспонированную), чтобы получить
# "карту симпатий". `@` — это оператор матричного умножения.
# `.transpose(-2, -1)` — меняет местами последние два измерения (строки и столбцы).
attention_scores = qkv @ qkv.transpose(-2, -1)

# Шаг 3.2: "Магическая Нормализация" (Масштабирование).
# `qkv.size(-1)` — это заклинание, получающее "глубину смысла"
#  нашей ауры (в данном случае, 3).
d_k = qkv.size(-1)
# Мы делим все "симпатии" на корень из этой глубины. Это делает обучение
# более стабильным в настоящих Трансформерах.
scaled_scores = attention_scores / np.sqrt(d_k)

# Шаг 3.3: "Ритуал Сосредоточения" (Softmax).
# `F.softmax` — это заклинание, которое берет любые числа и превращает их
# в вероятности (числа от 0 до 1), сумма которых по строке равна 1.
# Это и есть наша финальная "Карта Внимания" в процентах.
attention_weights = F.softmax(scaled_scores, dim=-1)

# Мы оглашаем на кристалл заголовок для нашей "Карты Внимания".
print("\n--- Карта Внимания (в процентах, округлено) ---")
# Мы даем пояснение, как читать эту карту.
print("Кто (строки) на кого (столбцы) 'смотрит' больше всего:")
# `torch.round(...)` — это заклинание, округляющее результат для наглядности.
print(torch.round(attention_weights * 100))

# --- Акт 4: Визуализация Эмпатического Поля ---
# Начинается четвертый акт: мы рисуем нашу "Карту Внимания".
# Мы оглашаем на кристалл о начале ритуала визуализации.
print("\nСоздаю визуализацию 'Эмпатического Поля' (Attention Map)...")

# `.squeeze(0)` — убирает "батч" измерение.
#  `.detach()` — "открепляет" от графов вычислений.
# `.numpy()` — превращает в массив numpy, понятный "Художнику" matplotlib.
heatmap_data = attention_weights.squeeze(0).detach().numpy()
# Мы создаем список подписей для осей нашей карты.
labels = ["Король", "сел", "принц", "упал"]

# Мы создаем "холст" (`ax`) и "рамку" (`fig`) для нашей картины.
fig, ax = plt.subplots()
# `ax.imshow(...)` — главное заклинание рисования. Оно рисует наш массив
# как "тепловую карту", используя красивую цветовую схему 'viridis'.
im = ax.imshow(heatmap_data, cmap="viridis")

# Мы настраиваем подписи на осях X и Y.
ax.set_xticks(np.arange(len(labels)), labels=labels)
# Мы настраиваем подписи на оси Y.
ax.set_yticks(np.arange(len(labels)), labels=labels)
# Мы поворачиваем подписи на оси X для лучшей читаемости.
plt.setp(ax.get_xticklabels(), rotation=45, ha="right", rotation_mode="anchor")

# Мы запускаем ритуальный цикл, чтобы в каждую ячейку нашей карты
#  вписать ее значение в процентах.
# Мы начинаем цикл по строкам.
for i in range(len(labels)):
    # Мы начинаем вложенный цикл по столбцам.
    for j in range(len(labels)):
        # Мы создаем текстовый объект, который будет вписан в ячейку.
        text = ax.text(
            # Координата X.
            j,
            # Координата Y.
            i,
            # Сам текст (значение в процентах).
            f"{heatmap_data[i, j]*100:.0f}%",
            # Горизонтальное выравнивание по центру.
            ha="center",
            # Вертикальное выравнивание по центру.
            va="center",
            # Цвет текста — белый.
            color="w",
        )

# Мы добавляем название всей картине.
ax.set_title("Эмпатическое Поле (Карта Внимания)")
# Мы добавляем подпись к оси X.
ax.set_xlabel("На кого 'смотрит' (Key)")
# Мы добавляем подпись к оси Y.
ax.set_ylabel("Кто 'смотрит' (Query)")
# Мы автоматически подгоняем размеры, чтобы все поместилось.
fig.tight_layout()

# Мы сохраняем нашу "карту" в виде файла-артефакта.
plt.savefig("attention_map.png")
# Мы оглашаем, что ритуал успешно завершен.
print("Магия визуализирована! Открой файл 'attention_map.png'.")

# --- Акт 5: Расшифровка Визуального Результата ---
# Финальный акт: мы даем наставление, как правильно истолковать результат.
# Мы оглашаем на кристалл заголовок для нашей расшифровки.
print("\n--- Что означает эта карта? ---")
# Это финальные инструкции для ученика, объясняющие, как интерпретировать
# результат.
print("Посмотри на картинку 'attention_map.png':")
# Мы указываем на первую важную связь, которую нашла магия Внимания.
print(
    "1. Найди строку 'Король'. Самая яркая (горячая) ячейка в "
    "этой строке - в столбце 'принц'."
)
# Мы даем толкование этой связи.
print(
    "   Это значит, что маг 'Король' 'почувствовал' самое сильное родство с 'принцем'."
)
# Мы указываем на вторую важную связь.
print("2. Теперь найди строку 'сел'. Самая яркая ячейка - в столбце 'упал'.")
# Мы даем толкование второй связи.
print("   Маг 'сел' 'почувствовал' наибольшую симпатию к магу 'упал'.")
# Мы подводим итог, объясняя суть магии Внимания.
print(
    "Это и есть магия Attention: он автоматически находит смысловые связи в данных!"
)
