"""Квест 11.2: Сборка "Многогранного Фокуса".

Этот пергамент — кульминация "Свитка Фокуса Мысли". Его главная цель
(МАКРО-контекст) — не просто реализовать `Multi-Head Attention` с нуля,
а **визуализировать** его работу, чтобы доказать, что разные "мудрецы"
(головы) находят **разные смысловые связи** в одном и том же предложении.

Мы создаем самодостаточный "Рунный Камень" `MultiHeadAttention`, который
является сердцем всех Трансформеров. Затем мы "скармливаем" ему условные
данные и рисуем "карты внимания" для каждой из его "голов". Этот ритуал
наглядно демонстрирует, что "многогранность" — это способность
анализировать один и тот же факт с множества разных точек зрения
одновременно.
"""

# Мы призываем "математический" гримуар `math` для вычисления корня.
import math

# Мы призываем "Художника" `matplotlib.pyplot` (с псевдонимом `plt`) для
# рисования карт.
import matplotlib.pyplot as plt

# Мы призываем `numpy` (с псевдонимом `np`) для удобной работы с массивами
# для рисования.
import numpy as np

# --- Акт 1: Подготовка Гримуаров ---
# Начинается первый акт: мы призываем все необходимые знания и инструменты.
# Мы призываем `torch` — наш Источник Маны.
import torch

# Мы призываем `torch.nn` (с псевдонимом `nn`) — главу с чертежами базовых
# блоков.
import torch.nn as nn


# --- Акт 2: Чертеж "Совета Мудрецов" ---
# Начинается второй, самый важный акт: мы создаем "чертеж" для нашего артефакта.
# Мы начинаем создание "чертежа" (класса) для нашего "Совета Мудрецов".
class MultiHeadAttention(nn.Module):
    # Мы определяем заклинание Инициализации, которое создает все внутренние
    # механизмы.
    def __init__(self, d_model, num_heads):
        # Мы произносим обязательное заклинание, пробуждающее магию родительского
        # класса.
        super().__init__()
        # Мы проверяем, что общую "глубину смысла" можно поровну разделить между
        # "мудрецами".
        assert (
            d_model % num_heads == 0
        ), "d_model must be divisible by num_heads"

        # Мы сохраняем ключевые параметры артефакта.
        self.d_model = d_model
        # Мы сохраняем количество "мудрецов".
        self.num_heads = num_heads
        # Мы вычисляем "глубину смысла" для каждого отдельного мудреца.
        self.d_k = d_model // num_heads

        # Мы создаем "Рунные Камни" (Linear слои) для создания Q, K, V.
        self.W_q = nn.Linear(d_model, d_model)
        # Мы создаем "Рунный Камень" для Ключей (K).
        self.W_k = nn.Linear(d_model, d_model)
        # Мы создаем "Рунный Камень" для Значений (V).
        self.W_v = nn.Linear(d_model, d_model)
        # Мы создаем финальный "Рунный Камень" для объединения выводов всех
        # "мудрецов".
        self.W_o = nn.Linear(d_model, d_model)

    # Мы определяем внутреннее заклинание "Фокуса Мысли".
    def scaled_dot_product_attention(self, Q, K, V):
        # Мы вычисляем "симпатию" и масштабируем ее.
        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(
            self.d_k
        )
        # Мы превращаем "симпатию" в "проценты внимания" (вероятности).
        attn_probs = torch.softmax(attn_scores, dim=-1)
        # СОХРАНЯЕМ ВЕСА ДЛЯ ВИЗУАЛИЗАЦИИ: мы "запоминаем" эту карту внимания.
        self.last_attn_weights = attn_probs
        # Мы смешиваем "коктейль" из Значений (V), взвешенных на проценты
        # внимания.
        output = torch.matmul(attn_probs, V)
        # Мы возвращаем "сфокусированную" ауру.
        return output

    # Мы определяем внутреннее заклинание "Разделения Смысла".
    def split_heads(self, x):
        # Мы получаем размеры входящей ауры.
        batch_size, seq_length, d_model = x.size()
        # Мы "нарезаем" и "переставляем" оси, чтобы у каждого мудреца была своя
        # стопка смыслов.
        return x.view(
            batch_size, seq_length, self.num_heads, self.d_k
        ).transpose(1, 2)

    # Мы определяем внутреннее заклинание "Великого Единения".
    def combine_heads(self, x):
        # Мы получаем размеры.
        batch_size, _, seq_length, d_k = x.size()
        # Мы "склеиваем" выводы мудрецов обратно в единую, целостную ауру.
        return (
            x.transpose(1, 2)
            .contiguous()
            .view(batch_size, seq_length, self.d_model)
        )

    # Мы определяем главное заклинание артефакта.
    def forward(self, Q, K, V):
        # Шаг 1: Мы создаем Q, K, V, пропуская исходные ауры через "Рунные
        # Камни".
        Q, K, V = self.W_q(Q), self.W_k(K), self.W_v(V)
        # Шаг 2: Мы "нарезаем" их для каждого мудреца.
        Q, K, V = self.split_heads(Q), self.split_heads(K), self.split_heads(V)
        # Шаг 3: Каждый мудрец проводит свой ритуал "Фокуса".
        attention_output = self.scaled_dot_product_attention(Q, K, V)
        # Шаг 4: Мы "склеиваем" их выводы.
        output = self.combine_heads(attention_output)
        # Шаг 5: Мы пропускаем результат через финальный "Рунный Камень".
        output = self.W_o(output)
        # Мы возвращаем финальную, обогащенную ауру.
        return output


# --- Акт 3: Испытание на "Осмысленном" Предложении ---
# Начинается третий акт: мы испытываем наш артефакт в деле.

# Мы задаем параметры: общая "глубина смысла" — 128, количество "мудрецов" — 8.
d_model, num_heads = 128, 8
# Мы создаем "ауры" для 5 условных слов, заполняя их случайными числами.
x = torch.randn(1, 5, d_model)

# Мы оглашаем на кристалл о начале испытания.
print("--- Испытание 'Многогранного Фокуса' ---")
# Мы сотворяем наш артефакт по чертежу.
multi_head_attention = MultiHeadAttention(d_model, num_heads)
# Мы запускаем ритуал, передавая одни и те же ауры на все три входа
# (Self-Attention).
output = multi_head_attention(x, x, x)

# --- Акт 4: Визуализация Мыслей "Совета Мудрецов" ---
# Начинается четвертый акт: мы рисуем коллективный разум нашего "Совета".
# Мы оглашаем на кристалл о начале ритуала визуализации.
print("\nСоздаю визуализацию мыслей каждого 'мудреца'...")

# Мы извлекаем "Карты Внимания", которые мы предусмотрительно сохранили.
attention_maps = (
    multi_head_attention.last_attn_weights.squeeze(0).detach().numpy()
)

# Мы создаем список подписей для осей нашей карты.
labels = ["Старый", "король", "дал", "корону", "принцу"]

# Мы создаем большую "картину", разделенную на 8 "холстов" (2 строки, 4
# столбца).
fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(12, 6))

# Мы запускаем ритуальный цикл, который пройдет по каждому из 8 "холстов".
# `axes.flat` — "расплющивает" нашу сетку 2x4 в простой список из 8 холстов.
for i, ax in enumerate(axes.flat):
    # Мы берем карту внимания для i-го мудреца.
    heatmap_data = attention_maps[i]
    # Мы рисуем ее на текущем холсте `ax`.
    im = ax.imshow(heatmap_data, cmap="viridis")

    # Мы настраиваем подписи на осях текущего холста.
    ax.set_xticks(np.arange(len(labels)), labels=labels, fontsize=8)
    # Мы настраиваем подписи на оси Y.
    ax.set_yticks(np.arange(len(labels)), labels=labels, fontsize=8)
    # Мы поворачиваем подписи на оси X для читаемости.
    plt.setp(
        ax.get_xticklabels(), rotation=45, ha="right", rotation_mode="anchor"
    )

    # Мы даем каждому "портрету" свое имя.
    ax.set_title(f"Мысли Мудреца #{i+1}")

# Мы даем название всей нашей большой картине.
fig.suptitle("Эмпатическое Поле 'Совета Мудрецов'")
# Мы автоматически подгоняем расположение, чтобы ничего не накладывалось.
fig.tight_layout()
# Мы сохраняем финальный артефакт.
plt.savefig("multi_head_attention_map.png")

# Мы оглашаем, что ритуал успешно завершен.
print("Магия визуализирована! Открой файл 'multi_head_attention_map.png'.")

# --- Акт 5: Расшифровка Визуального Результата ---
# Финальный акт: мы даем наставление, как правильно истолковать результат.
# Мы оглашаем на кристалл заголовок для нашей расшифровки.
print("\n--- Что означает эта карта? ---")
# Это финальные инструкции для ученика, объясняющие, как интерпретировать
# результат.
print(
    "Открой картинку. Ты видишь 8 разных 'Карт Внимания' - "
    "по одной от каждого 'мудреца'."
)
# Мы объясняем, почему карты разные.
print(
    "Они все разные! Это потому, что каждый мудрец 'смотрит' на мир через свою призму."
)
# Мы приводим пример, какую связь мог бы найти первый мудрец.
print(
    "- Мудрец #1 мог научиться находить связи 'кто-кому'. "
    "Он бы показал яркую связь между 'король' и 'принцу'."
)
# Мы приводим пример для второго мудреца.
print(
    "- Мудрец #2 мог научиться находить глаголы. Он бы 'смотрел' только на слово 'дал'."
)
# Мы приводим пример для третьего мудреца.
print(
    "- Мудрец #3 мог научиться связывать прилагательные с "
    "существительными. Он бы 'смотрел' от 'Старый' на 'король'."
)
# Мы подводим итог, объясняя силу "Совета Мудрецов".
print(
    "Вместе, выводы всех восьми мудрецов дают невероятно богатое "
    "и полное понимание предложения."
)
