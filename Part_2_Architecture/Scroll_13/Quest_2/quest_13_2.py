# === quest_13_2.py ===
# Имя этого пергамента, хранящего ритуал "Перегонки Знаний".
# Квест: 13.2 - Перегонка знаний
# Каноническое имя Квеста, как оно записано в Великом Кодексе.
# Цель: Реализовать Knowledge Distillation. Мы обучим маленькую "модель-ученика"
# Священная цель нашего ритуала.
# имитировать "мысли" (soft labels) большой "модели-учителя", передавая
# Детальное описание цели.
# ей знания более эффективно, чем при обучении на "жестких" метках.
# Указание на ключевую идею.

# --- Акт 1: Подготовка Гримуаров ---
# Начинается первый акт: мы призываем все необходимые знания и инструменты.
# Мы призываем наш главный силовой гримуар `PyTorch`.
import torch

# Мы призываем `torch.nn` (с псевдонимом `nn`) — главу с чертежами базовых
# блоков для моделей.
import torch.nn as nn

# Мы призываем `torch.nn.functional` (с псевдонимом `F`) — гримуар с
# "функциональными" заклинаниями.
import torch.nn.functional as F

# Мы призываем `torch.optim` (с псевдонимом `optim`) — гримуар с
# "инструментами для исправления ошибок".
import torch.optim as optim

# Мы призываем "Библиотеку" с нашим "учебником" MNIST и гримуар трансформаций.
from torchvision import datasets, transforms

# Мы призываем наш верный "индикатор прогресса".
from tqdm import tqdm

# --- Акт 2: Подготовка "Учебника" и Чертежей ---
# Начинается второй акт: мы готовим "учебник" и чертежи для наших Големов.

# Мы создаем конвейер "магических линз" для подготовки изображений.
transform = transforms.Compose(
    # Начало списка трансформаций.
    [
        # Первая "линза": превращает картинку из формата PIL/numpy в Тензор
        # PyTorch.
        transforms.ToTensor(),
        # Вторая "линза": "нормализует" яркость пикселей, приводя их
        # к стандартному распределению. Это помогает модели учиться быстрее.
        transforms.Normalize((0.1307,), (0.3081,)),
        # Конец списка трансформаций.
    ]
)
# Мы загружаем "учебник" MNIST. `download=True` скачает его, если нужно.
train_dataset = datasets.MNIST(
    "./data", train=True, download=True, transform=transform
)
# Мы создаем "подносчик" данных, который будет подавать нам "учебник"
# пачками по 64 картинки.
train_loader = torch.utils.data.DataLoader(
    train_dataset, batch_size=64, shuffle=True
)


# --- Чертеж "Магистра" (Teacher): Большая и сложная CNN ---
# Мы создаем чертеж для нашей большой, мудрой модели-учителя.
class TeacherModel(nn.Module):
    # Мы определяем ритуал сотворения Магистра.
    def __init__(self):
        # Мы произносим обязательное заклинание, пробуждающее дух предка.
        super().__init__()
        # Мы создаем первый этаж: 32 "гнома".
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        # Мы создаем второй этаж: 64 "гнома".
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        # Мы создаем первый "зал раздумий".
        self.fc1 = nn.Linear(9216, 128)
        # Мы создаем второй, финальный "зал".
        self.fc2 = nn.Linear(128, 10)

    # Мы определяем путь мысли через разум Магистра.
    def forward(self, x):
        # Мы прогоняем образ через 1-й этаж и "переключатель" ReLU.
        x = F.relu(self.conv1(x))
        # Мы прогоняем образ через 2-й этаж, "переключатель" и "уменьшитель".
        x = F.relu(F.max_pool2d(self.conv2(x), 2))
        # Мы "сплющиваем" карты признаков в один длинный вектор.
        x = torch.flatten(x, 1)
        # Мы пропускаем вектор через 1-й "зал".
        x = F.relu(self.fc1(x))
        # Мы получаем финальные "мысли"-логиты из 2-го "зала".
        x = self.fc2(x)
        # Мы возвращаем эти "мысли".
        return x


# --- Чертеж "Подмастерья" (Student): Маленькая и простая CNN ---
# Мы создаем чертеж для нашей маленькой, экономной модели-ученика.
class StudentModel(nn.Module):
    # Мы определяем ритуал сотворения Подмастерья.
    def __init__(self):
        # Мы произносим обязательное заклинание, пробуждающее дух предка.
        super().__init__()
        # У него всего один этаж с 16-ю "гномами".
        self.conv1 = nn.Conv2d(1, 16, 3, 1)
        # И всего один "зал раздумий".
        self.fc1 = nn.Linear(2704, 10)

    # Мы определяем путь мысли через разум Подмастерья.
    def forward(self, x):
        # Он проделывает те же шаги, что и Магистр, но в меньшем масштабе.
        x = F.relu(F.max_pool2d(self.conv1(x), 2))
        # Он "сплющивает" свои карты.
        x = torch.flatten(x, 1)
        # Он пропускает вектор через свой "зал".
        x = self.fc1(x)
        # Он возвращает свои "мысли".
        return x


# --- Акт 3: Обучение "Магистра" ---
# Начинается третий акт: мы обучаем нашу модель-учителя.
# Мы оглашаем на кристалл о начале обучения Магистра.
print("Обучаю 'Магистра', чтобы он стал мудрым...")
# Мы сотворяем Магистра и отправляем его на Кристалл Маны.
teacher = TeacherModel().to("cuda")
# Мы готовим "Волшебный Ключ" для его обучения.
optimizer_teacher = optim.Adam(teacher.parameters(), lr=0.01)
# Мы готовим "Рулетку" для измерения его ошибок.
criterion = nn.CrossEntropyLoss()

# Мы переводим Магистра в режим обучения.
teacher.train()
# Мы проводим один быстрый "учебный год" (эпоху).
for data, target in tqdm(train_loader, desc="Обучение Магистра (1 эпоха)"):
    # Мы отправляем данные на Кристалл Маны.
    data, target = data.to("cuda"), target.to("cuda")
    # Мы стираем старые ошибки.
    optimizer_teacher.zero_grad()
    # Магистр выносит вердикт.
    output = teacher(data)
    # Мы измеряем ошибку.
    loss = criterion(output, target)
    # Мы вычисляем, как исправиться.
    loss.backward()
    # Мы исправляемся.
    optimizer_teacher.step()
# Мы оглашаем, что Магистр готов.
print("Магистр обучен и готов делиться мудростью.")

# --- Акт 4: Ритуал "Перегонки Знаний" ---
# Начинается четвертый, кульминационный акт: мы обучаем "Подмастерье".
# Мы оглашаем на кристалл о начале ритуала.
print("\nНачинаю ритуал 'Перегонки Знаний' для 'Подмастерья'...")
# Мы сотворяем Подмастерье.
student = StudentModel().to("cuda")
# Мы готовим "Волшебный Ключ" для него.
optimizer_student = optim.Adam(student.parameters(), lr=0.01)

# --- Настройка Магии "Перегонки" ---
# Мы устанавливаем "Температуру", смягчающую "шепот" Магистра.
distillation_temp = 5.0
# Мы устанавливаем "Баланс" между мудростью Магистра и сухими фактами из
# учебника.
alpha = 0.5

# Мы переводим Ученика в режим обучения.
student.train()
# Мы переводим Магистра в режим "экзаменатора" (он больше не учится).
teacher.eval()

# Мы начинаем урок для Подмастерья.
for data, target in tqdm(train_loader, desc="Обучение Подмастерья (1 эпоха)"):
    # Мы отправляем данные на Кристалл Маны.
    data, target = data.to("cuda"), target.to("cuda")
    # Мы стираем старые ошибки Ученика.
    optimizer_student.zero_grad()

    # Мы получаем "мысли" Подмастерья.
    student_logits = student(data)
    # Мы используем защитное заклинание `no_grad`, так как Магистр не должен
    # учиться.
    with torch.no_grad():
        # Мы получаем "мудрый шепот" от Магистра.
        teacher_logits = teacher(data)

    # --- Вычисление Составной Ошибки ---
    # Ошибка №1 (Distillation Loss): Насколько "мысли" ученика отличаются от
    # "шепота" учителя.
    distillation_loss = nn.KLDivLoss(reduction="batchmean")(
        # Мы смягчаем и логируем "мысли" ученика.
        F.log_softmax(student_logits / distillation_temp, dim=1),
        # Мы смягчаем "шепот" учителя.
        F.softmax(teacher_logits / distillation_temp, dim=1),
    )

    # Ошибка №2 (Student Loss): Насколько ответы ученика соответствуют
    # реальным "правильным ответам".
    student_loss = criterion(student_logits, target)

    # Мы вычисляем финальную, взвешенную Ошибку.
    loss = (
        # Мы умножаем "ошибку дистилляции" на ее вес и поправочный коэффициент.
        alpha * distillation_loss * (distillation_temp**2)
        # Мы добавляем "студенческую ошибку", умноженную на ее вес.
        + (1.0 - alpha) * student_loss
    )

    # Мы производим стандартный ритуал исправления ошибок.
    loss.backward()
    # Мы "подкручиваем" руны Ученика.
    optimizer_student.step()

# Мы оглашаем, что ритуал завершен, и сообщаем финальную ошибку.
print(
    f"\nРитуал завершен! Финальная Ошибка Подмастерья (Loss): {loss.item():.4f}"
)
# Мы оглашаем, что Подмастерье получил новые знания.
print("Подмастерье впитал мудрость Магистра.")
