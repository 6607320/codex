# **Архитектура Передачи Знаний: Исчерпывающий Анализ Методологий Дистилляции и Оптимизации Нейронных Сетей**

## **1\. Введение: Эпоха Вычислительного Гигантизма и Парадокс Эффективности**

В современной парадигме искусственного интеллекта наблюдается фундаментальная дихотомия. С одной стороны, эмпирический закон масштабирования (Scaling Laws) диктует, что увеличение количества параметров, объема обучающих данных и вычислительных ресурсов неизбежно приводит к снижению функции потерь и улучшению генерализующей способности моделей.1 Современные архитектуры, такие как большие языковые модели (LLM) и глубокие сверточные сети (CNN), достигли феноменальной производительности, часто превосходя человеческие возможности в специализированных задачах. Однако, с другой стороны, этот рост сложности создает колоссальный барьер для практического внедрения. Модели, требующие гигабайтов видеопамяти и триллионов операций с плавающей запятой (FLOPs) для одного вывода (инференса), оказываются непригодными для развертывания в средах с ограниченными ресурсами — от мобильных устройств и IoT-датчиков до систем реального времени, где задержка (latency) является критическим фактором.3  
Именно в этом контексте технология Дистилляции Знаний (Knowledge Distillation, KD), представленная в концептуальном коде «Квеста 13.2», трансформируется из академического любопытства в промышленный стандарт. Суть KD заключается не просто в сжатии модели, а в фундаментальном переосмыслении того, что есть «обучение». В классическом обучении с учителем (Supervised Learning) знание отождествляется с минимизацией ошибки относительно жесткой разметки (Ground Truth). Дистилляция же предлагает альтернативную гипотезу: истинное знание модели заключено не в финальном ответе, а в структуре распределения вероятностей по всем возможным классам, включая ошибочные.5  
Данный отчет представляет собой всестороннее исследование феномена Дистилляции Знаний. Мы деконструируем механизмы, позволяющие «маленькому» и быстрому «Голему-Подмастерью» (Student) абсорбировать интуицию «большого» и мудрого «Голема-Магистра» (Teacher). Анализ охватит теоретические основы термодинамики softmax-распределений, математическую неизбежность масштабирования градиентов, нюансы реализации в PyTorch, представленные в исходном коде, а также широчайший спектр промышленных применений — от квантования нейросетей для смартфонов до передачи цепочек рассуждений (Chain-of-Thought) в современных LLM.5

### **1.1 Концепция "Темного Знания" (Dark Knowledge)**

Центральным понятием в теории дистилляции является «темное знание». В стандартной задаче классификации, например, распознавании рукописных цифр MNIST (как в рассматриваемом коде), целевая метка представляет собой one-hot вектор: для изображения цифры «7» это вектор, где позиция 7 равна 1.0, а все остальные — 0.0. Такой подход, хотя и эффективен для обучения точности, безжалостно уничтожает информацию о структурном сходстве классов. Обучаясь на «жестких» метках, модель узнает, что это «семь», но не узнает, что эта «семь» визуально похожа на «единицу», но совсем не похожа на «восьмерку».  
Когда же мы обучаем большую модель-учителя, она, благодаря своей избыточной емкости, выучивает эти нюансы. На выходе softmax-слоя учителя для той же цифры «7» мы можем увидеть распределение: $P(7)=0.98$, $P(1)=0.015$, $P(9)=0.005$, а $P(8) \\approx 0$. Эти малые вероятности, или «мягкие метки» (soft targets), и есть «темное знание».1 Они кодируют геометрию пространства признаков, показывая ученику, какие классы семантически близки. Передача этого знания позволяет ученику построить более ровную и обобщающую разделяющую поверхность, избегая переобучения, свойственного обучению на разреженных жестких метках.8

## ---

**2\. Теоретический Фундамент и Математическая Формулировка**

Для глубокого понимания механизма, заложенного в предоставленном скрипте quest_13_2.py, необходимо разобрать математический аппарат, управляющий процессом передачи знаний. Ключевым элементом здесь является функция Softmax с температурным масштабированием.

### **2.1 Термодинамика Softmax-распределения**

Стандартная функция Softmax преобразует выходные логиты нейронной сети $z\_i$ в вероятности $q\_i$. В контексте дистилляции вводится гиперпараметр температуры $T$, заимствованный из статистической механики (распределение Больцмана). Формула имеет вид:

$$q\_i(T) \= \\frac{\\exp(z\_i / T)}{\\sum\_{j} \\exp(z\_j / T)}$$  
Где:

- $z\_i$ — логит (необработанный выход нейрона) для класса $i$.
- $T$ — температура дистилляции (в коде distillation_temp \= 5.0).
- $q\_i(T)$ — "смягченная" вероятность класса $i$.1

#### **Роль Температуры: Спектральный Анализ Уверенности**

Параметр $T$ действует как регулятор энтропии выходного распределения. Его влияние можно разделить на три режима:

1. **Низкая температура ($T \\to 0$):** Распределение стремится к вырожденному виду (argmax). Максимальный логит получает вероятность, близкую к 1, остальные — к 0\. В этом режиме информация о соотношении "неправильных" классов теряется. Это режим максимальной уверенности, который полезен для финального предсказания (инференса), но бесполезен для обучения нюансам.9
2. **Стандартная температура ($T \= 1$):** Режим обычного обучения. Распределение отражает стандартную уверенность модели. Для очень уверенных моделей (overconfident), характерных для глубоких сетей, распределение все еще может быть слишком пиковым, скрывая вторичную информацию.6
3. **Высокая температура ($T \> 1$):** Именно этот режим используется в дистилляции. Деление логитов на $T$ уменьшает их абсолютные значения, что "сплющивает" экспоненту. Различия между малыми логитами (например, между \-5 и \-7) становятся более значимыми в итоговом распределении вероятностей. Это позволяет "проявить" скрытую структуру данных — то самое "темное знание". Ученик начинает видеть, что "грузовик" больше похож на "автомобиль", чем на "кошку", даже если правильный ответ — "грузовик".1

В коде установлена температура distillation_temp \= 5.0. Это эмпирически обоснованное значение для задач уровня MNIST/CIFAR, позволяющее достаточно "размягчить" распределение учителя, чтобы ученик мог считать информацию о вторичных классах, но не настолько, чтобы превратить выход в равномерный шум (максимальную энтропию), где информация теряется полностью.11

### **2.2 Составная Функция Потерь: Баланс Авторитетов**

Успех дистилляции зависит от правильного взвешивания двух источников обучающего сигнала. Функция потерь $L\_{total}$ представляет собой линейную комбинацию:

$$L\_{total} \= \\alpha \\cdot T^2 \\cdot L\_{distill} \+ (1 \- \\alpha) \\cdot L\_{student}$$  
В коде это реализовано следующим образом:

Python

loss \= (  
 alpha \* distillation_loss \* (distillation_temp\*\*2)  
 \+ (1.0 \- alpha) \* student_loss  
)

Разберем компоненты:

- **$L\_{student}$ (Student Loss):** Это стандартная перекрестная энтропия (Cross-Entropy) между предсказаниями ученика (при $T=1$) и истинными метками. Она гарантирует, что ученик учит "факты" и имеет высокую точность на целевой задаче.
- **$L\_{distill}$ (Distillation Loss):** Это дивергенция Кульбака-Лейблера (KL Divergence) между мягкими предсказаниями ученика и учителя (оба при температуре $T$). Она заставляет ученика мимикрировать под "мнение" учителя.
- **$\\alpha$ (Alpha):** Коэффициент баланса. Значение 0.5 означает равноправие источников. В некоторых исследованиях предлагается динамически изменять $\\alpha$ или использовать более высокие значения (до 0.9), чтобы придать больший вес учителю, особенно если разметка данных зашумлена.10

### **2.3 Градиентное Масштабирование: Загадка $T^2$**

Наименее очевидный, но математически критичный аспект реализации — это умножение дистилляционной ошибки на $T^2$. Без этого множителя эффективность обучения катастрофически падает при высоких температурах. Почему это происходит?  
Рассмотрим градиент функции потерь дистилляции по отношению к логитам ученика $z\_i$. Как показано в фундаментальной работе Хинтона (2015), градиент пропорционален $1/T$:

$$\\frac{\\partial L\_{distill}}{\\partial z\_i} \\approx \\frac{1}{T} (q\_i \- v\_i)$$  
Где $q\_i$ и $v\_i$ — мягкие вероятности ученика и учителя. При $T \\gg 1$, значения $q\_i$ и $v\_i$ становятся близкими к равномерному распределению ($1/N$), и их разность стремится к нулю. Более того, сам множитель $1/T$ дополнительно уменьшает величину градиента.11  
Если использовать разложение Тейлора для экспоненты ($e^x \\approx 1+x$) при высоких температурах (когда $z\_i/T \\to 0$), можно показать, что градиент масштабируется как $1/T^2$:

$$\\frac{\\partial L}{\\partial z\_i} \\approx \\frac{1}{N T^2} (z\_i \- v\_i)$$  
Таким образом, величина градиента от дистилляционной части потери падает квадратично с ростом $T$. В то же время градиент от "студенческой" потери ($L\_{student}$), работающей при $T=1$, остается неизменным. Это создает дисбаланс: "жесткая" потеря начинает полностью доминировать, и ученик перестает "слышать" учителя. Умножение $L\_{distill}$ на $T^2$ в формуле потерь компенсирует это падение ($T^2 \\cdot \\frac{1}{T^2} \= 1$), восстанавливая баланс градиентов и позволяя эффективно учиться при любых $T$.12

## ---

**3\. Деконструкция Реализации: Анализ "Квеста 13.2"**

Рассмотрим представленный код как эталонную реализацию (Reference Implementation) и проанализируем архитектурные и инженерные решения.

### **3.1 Архитектурный Разрыв: Учитель против Ученика**

**Голем-Магистр (TeacherModel):**

- **Структура:** 2 сверточных слоя (Conv2d: 32 \-\> 64 фильтров) \+ 2 полносвязных слоя (Linear: 9216 \-\> 128 \-\> 10).
- **Характеристика:** Обладает достаточной глубиной и шириной для извлечения иерархических признаков MNIST. 64 фильтра во втором слое позволяют детектировать сложные паттерны (дуги, пересечения). Скрытый слой на 128 нейронов создает мощное векторное представление перед классификацией.15

**Голем-Подмастерье (StudentModel):**

- **Структура:** 1 сверточный слой (Conv2d: 16 фильтров) \+ 1 полносвязный слой (Linear: 2704 \-\> 10).
- **Характеристика:** Экстремально сжатая модель.
  - _Уменьшение глубины:_ Отсутствие второго сверточного слоя лишает модель возможности строить сложные иерархии признаков.
  - _Уменьшение ширины:_ 16 фильтров вместо 32/64 ограничивают разнообразие детектируемых примитивов.
  - _Отсутствие скрытого слоя:_ Прямая проекция с карт признаков на классы (Linear 2704-\>10) означает, что модель должна линейно разделить пространство признаков, что значительно сложнее.7
- **Роль Дистилляции:** Без учителя такая слабая модель, скорее всего, застряла бы в локальных минимумах или показала низкую обобщающую способность. Дистилляция направляет ее параметры в ту область пространства весов, которая, несмотря на простоту, аппроксимирует сложную решающую границу учителя.

### **3.2 Инженерия Данных: Магия Нормализации**

В коде используется специфическая нормализация:

Python

transforms.Normalize((0.1307,), (0.3081,))

Эти числа — не случайные константы. Это глобальное среднее (mean) и стандартное отклонение (std) пикселей всего обучающего набора данных MNIST.

- **Зачем это нужно?** Нейронные сети обучаются значительно быстрее и стабильнее, когда входные данные центрированы вокруг нуля и имеют единичную дисперсию. Это предотвращает насыщение функций активации и обеспечивает более симметричное обновление весов при обратном распространении ошибки.
- **Важность согласованности:** Критически важно, чтобы и Учитель (при его предварительном обучении), и Ученик (при дистилляции) видели данные в одном и том же диапазоне. Если подать нормализованные данные учителю, обученному на сырых пикселях , его логиты будут бессмысленным шумом, что разрушит процесс дистилляции.16

### **3.3 Тонкости PyTorch: KLDivLoss и log_softmax**

Реализация функции потерь в PyTorch требует особого внимания из\-за специфики API, что часто становится источником ошибок для новичков.

Python

distillation_loss \= nn.KLDivLoss(reduction="batchmean")(  
 F.log_softmax(student_logits / distillation_temp, dim=1),  
 F.softmax(teacher_logits / distillation_temp, dim=1),  
)

1. **Асимметрия входов:** Математически KL-дивергенция определяется как $\\sum P(x) \\log \\frac{P(x)}{Q(x)}$. Однако функция nn.KLDivLoss в PyTorch ожидает, что _первый_ аргумент (вход, prediction) уже логарифмирован (log*softmax), а *второй\_ (цель, target) — является обычными вероятностями (softmax).
   - Использование log_softmax вместо явного log(softmax(x)) необходимо для численной стабильности (Log-Sum-Exp trick), чтобы избежать переполнения или исчезновения порядков при вычислении экспонент.19
2. **Редукция batchmean:** Параметр reduction="batchmean" критически важен. В старых версиях PyTorch mean делила сумму потерь на общее количество элементов (batch_size \* num_classes), тогда как математически корректно делить только на размер батча. batchmean гарантирует соответствие математическому определению KL-дивергенции, делая значения потерь интерпретируемыми и независимыми от количества классов.19

## ---

**4\. Классификация Знаний: За Пределами Logits**

Хотя в "Квесте" используется **Response-Based** (основанная на откликах) дистилляция, современная наука выделяет три основных типа передачи знаний. Понимание этих типов необходимо для применения KD в более сложных задачах, чем MNIST.

| Тип Дистилляции    | Источник Знаний                    | Механизм Передачи                                                                                                                        | Применение                                                                                                                     |
| :----------------- | :--------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------- |
| **Response-Based** | Выходной слой (Logits/Softmax)     | Минимизация KL-дивергенции между распределениями вероятностей.                                                                           | Классификация, простые задачи, сжатие моделей. (Используется в Квесте).22                                                      |
| **Feature-Based**  | Промежуточные слои (Feature Maps)  | Минимизация L2-расстояния между картами признаков учителя и ученика. Требует регрессоров для выравнивания размерностей.                  | Компьютерное зрение (Object Detection), глубокие сети (ResNet), когда важно научить "как видеть", а не только "что отвечать".5 |
| **Relation-Based** | Связи между примерами (Structural) | Сравнение матриц сходства (Gram matrices) для батча данных. Если примеры А и Б похожи для учителя, они должны быть похожи и для ученика. | Задачи Metric Learning, Face Recognition, сложные семантические задачи.22                                                      |

**Feature-Based** подход решает проблему "черного ящика". В Response-Based методе ученик может угадать правильный ответ, используя совершенно неверные признаки (например, фон вместо объекта). Заставляя ученика копировать промежуточные активации учителя, мы гарантируем, что он "смотрит" на те же части изображения (например, уши кошки), что и учитель.5

## ---

**5\. Продвинутые Методологии и Вариации**

Исследования последних лет расширили границы применения KD, предложив методы, которые улучшают базовый алгоритм, представленный в коде.

### **5.1 Self-Distillation (Самодистилляция)**

Парадоксальный феномен: модель может улучшить сама себя.

- **Алгоритм:** Обучаем модель М1. Затем используем М1 как учителя для обучения модели М2 _той же самой архитектуры_. Повторяем процесс: М2 учит М3.
- **Результат:** Каждое поколение работает лучше предыдущего (до определенного предела).
- **Причина:** Мягкие метки учителя действуют как сглаживание меток (Label Smoothing) и регуляризация. Учитель "очищает" данные от шума разметки, предоставляя ученику более когерентный сигнал. Это доказывает, что KD работает не только за счет разницы в емкости моделей, но и за счет качества обучающего сигнала.25

### **5.2 Curriculum Temperature (CTKD)**

Использование фиксированной температуры (как T=5.0 в коде) может быть субоптимальным.

- **Идея:** В начале обучения задача имитации учителя может быть слишком сложной для "глупого" ученика.
- **Решение:** Динамическое изменение температуры. Начинаем с высокой температуры (фокус на общих соотношениях классов), затем постепенно понижаем её, заставляя ученика фокусироваться на более тонких различиях. Это реализует стратегию Curriculum Learning ("от простого к сложному").10

## ---

**6\. Прикладные Сценарии: Индустриальный Масштаб**

Переход от MNIST к реальным бизнес-задачам демонстрирует экономическую мощь дистилляции.

### **6.1 NLP: Кейс DistilBERT**

В обработке естественного языка (NLP) модели Transformer (BERT, GPT) стали стандартом, но их размеры (сотни миллионов параметров) затрудняют внедрение.

- **Проблема:** BERT-Base имеет 110 млн параметров. Инференс на CPU слишком медленный для веб\-приложений.
- **Решение:** Hugging Face создали **DistilBERT**.28
  - _Архитектура:_ Удалили эмбеддинги типов токенов, убрали пулер, сократили количество слоев в 2 раза (с 12 до 6).
  - _Инициализация:_ Использовали веса учителя для инициализации ученика (взяли каждый второй слой). Это важный хак: ученик начинает не с нуля, а уже имея структуру признаков.
  - _Результат:_ Сохранение 97% качества BERT при уменьшении размера на 40% и ускорении инференса на 60%. Это сделало возможным запуск BERT-подобных моделей на мобильных устройствах.2

### **6.2 Edge Computing и Интернет Вещей (IoT)**

Для запуска ИИ на камерах видеонаблюдения или дронах (Edge Devices) каждый милливатт энергии на счету.

- **Квантование \+ Дистилляция:** Часто KD комбинируют с квантованием (переводом весов из float32 в int8). Учитель (float32) обучает ученика, который сразу квантуется. Это позволяет запускать модели на DSP и нейропроцессорах смартфонов с минимальным потреблением энергии.4
- **Пример:** Распознавание лиц на смартфоне. Большая облачная модель (ResNet-101) дистиллируется в MobileNetV3, которая работает локально, обеспечивая приватность данных и работу без интернета.3

### **6.3 Большие Языковые Модели (LLM) и Chain-of-Thought**

В эпоху Generative AI дистилляция эволюционировала. Теперь мы дистиллируем не вероятности токенов, а **рассуждения**.

- **Проблема:** GPT-4 умная, но дорогая. Llama-3-8B быстрая, но плохо рассуждает.
- **Решение:** Используем GPT-4 для генерации ответов с цепочкой рассуждений (Chain-of-Thought, CoT). Например: "Сначала вычислим X, потом Y, следовательно ответ Z".
- **Fine-Tuning как Дистилляция:** Обучаем маленькую модель на этих данных. Она учится не просто давать ответ "Z", а воспроизводить логику "сначала X, потом Y". Это передача _процедурного_ знания. Модели вроде Alpaca и DeepSeek являются продуктами такой дистилляции.31
- **Синтетическая Дистилляция:** Использование "R1" (reasoning model) для генерации 800k примеров данных, на которых обучаются меньшие модели, фактически является формой дистилляции знаний через данные (Data Distillation).31

## ---

**7\. Заключение**

Квест 13.2 и методология Дистилляции Знаний в целом открывают фундаментальную истину машинного обучения: **информация и представление информации — это не одно и то же**. "Темное знание", скрытое в мягких вероятностях учителя, содержит богатейшую картину мира, которую невозможно передать через сухие факты жесткой разметки.  
Использование температуры $T$ как инструмента "фокусировки" внимания ученика, балансировка функций потерь с помощью $\\alpha$ и $T^2$, а также грамотный выбор архитектуры ученика — это те рычаги, которые позволяют инженерам ИИ преодолевать ограничения закона Мура. Мы переходим от парадигмы "чем больше, тем лучше" к парадигме "чем эффективнее, тем лучше".  
В будущем, по мере того как модели-учителя будут достигать триллионов параметров, роль дистилляции станет еще более критической. Она станет основным "интерфейсом" между сверхчеловеческим интеллектом облачных суперкомпьютеров и прикладным интеллектом устройств, окружающих нас в повседневной жизни. Дистилляция — это мост, по которому интеллект сходит с небес на землю.

---

**Таблица: Сравнительный анализ влияния температуры на дистилляцию**

| Температура (T)            | Характеристика распределения | Эффект обучения                                                                             | Риск                                                                                 |
| :------------------------- | :--------------------------- | :------------------------------------------------------------------------------------------ | :----------------------------------------------------------------------------------- |
| $T \\to 1$ (Низкая)        | Пиковое (Argmax-подобное)    | Ученик фокусируется только на правильном классе. Игнорирует "темное знание".                | Потеря семантических связей между классами. Эквивалентно обучению на hard labels.    |
| $T \\approx 2-5$ (Средняя) | Сбалансированное ("Мягкое")  | Оптимальный баланс. Видны связи (7 похожа на 1), но лидер очевиден.                         | Требует подбора под конкретный датасет.                                              |
| $T \\to \\infty$ (Высокая) | Равномерное (Uniform)        | Градиенты становятся линейными. Дистилляция превращается в регрессию логитов ($L\_2$ loss). | Исчезновение сигнала о правильном классе. Шум начинает доминировать над информацией. |

---

**Использованные источники:**.1

#### **Источники**

1. Understanding Knowledge Distillation: From Hinton's Paper to OpenAI's Mini Models | by Asim Adnan Eijaz | Medium, дата последнего обращения: декабря 20, 2025, [https://medium.com/@asimadnan/understanding-knowledge-distillation-from-hintons-paper-to-openai-s-mini-models-e5a761b0dc47](https://medium.com/@asimadnan/understanding-knowledge-distillation-from-hintons-paper-to-openai-s-mini-models-e5a761b0dc47)
2. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter \- arXiv, дата последнего обращения: декабря 20, 2025, [https://arxiv.org/pdf/1910.01108](https://arxiv.org/pdf/1910.01108)
3. How Knowledge Distillation Cuts AI Model Inference Costs | Galileo, дата последнего обращения: декабря 20, 2025, [https://galileo.ai/blog/knowledge-distillation-ai-models](https://galileo.ai/blog/knowledge-distillation-ai-models)
4. Deploying AI on Edge: Advancement and Challenges in Edge Intelligence \- MDPI, дата последнего обращения: декабря 20, 2025, [https://www.mdpi.com/2227-7390/13/11/1878](https://www.mdpi.com/2227-7390/13/11/1878)
5. What is Knowledge distillation? | IBM, дата последнего обращения: декабря 20, 2025, [https://www.ibm.com/think/topics/knowledge-distillation](https://www.ibm.com/think/topics/knowledge-distillation)
6. Knowledge distillation \- Wikipedia, дата последнего обращения: декабря 20, 2025, [https://en.wikipedia.org/wiki/Knowledge_distillation](https://en.wikipedia.org/wiki/Knowledge_distillation)
7. A Comprehensive Survey on Knowledge Distillation \- arXiv, дата последнего обращения: декабря 20, 2025, [https://arxiv.org/html/2503.12067v2](https://arxiv.org/html/2503.12067v2)
8. Optimal Knowledge Distillation through Non-Heuristic Control of Dark Knowledge \- MDPI, дата последнего обращения: декабря 20, 2025, [https://www.mdpi.com/2504-4990/6/3/94](https://www.mdpi.com/2504-4990/6/3/94)
9. Speculative Knowledge Distillation: Bridging the Teacher-Student Gap Through Interleaved Sampling \- arXiv, дата последнего обращения: декабря 20, 2025, [https://arxiv.org/html/2410.11325v3](https://arxiv.org/html/2410.11325v3)
10. Curriculum Temperature for Knowledge Distillation, дата последнего обращения: декабря 20, 2025, [https://ojs.aaai.org/index.php/AAAI/article/view/25236/25008](https://ojs.aaai.org/index.php/AAAI/article/view/25236/25008)
11. Distilling the Knowledge in a Neural Network \- arXiv, дата последнего обращения: декабря 20, 2025, [https://arxiv.org/pdf/1503.02531](https://arxiv.org/pdf/1503.02531)
12. Why the gradients produced by the soft targets scale as 1/T^2 in knowledge distillation?, дата последнего обращения: декабря 20, 2025, [https://ai.stackexchange.com/questions/41389/why-the-gradients-produced-by-the-soft-targets-scale-as-1-t2-in-knowledge-disti](https://ai.stackexchange.com/questions/41389/why-the-gradients-produced-by-the-soft-targets-scale-as-1-t2-in-knowledge-disti)
13. AI's Path to Efficiency: The Science of Knowledge Distillation | by Simon Palma | Medium, дата последнего обращения: декабря 20, 2025, [https://medium.com/@simon.palma/ais-path-to-efficiency-the-science-of-knowledge-distillation-3525e5e68dfc](https://medium.com/@simon.palma/ais-path-to-efficiency-the-science-of-knowledge-distillation-3525e5e68dfc)
14. Knowledge Distillation math proof \- Mathematics Stack Exchange, дата последнего обращения: декабря 20, 2025, [https://math.stackexchange.com/questions/3579546/knowledge-distillation-math-proof](https://math.stackexchange.com/questions/3579546/knowledge-distillation-math-proof)
15. examples/mnist/main.py at main · pytorch/examples \- GitHub, дата последнего обращения: декабря 20, 2025, [https://github.com/pytorch/examples/blob/master/mnist/main.py](https://github.com/pytorch/examples/blob/master/mnist/main.py)
16. Example: classifying the MNIST dataset \- Training in Research Computing, дата последнего обращения: декабря 20, 2025, [https://mint.westdri.ca/ai/pt/pt_mnist](https://mint.westdri.ca/ai/pt/pt_mnist)
17. MNIST normalizing and scaling the dataset at the same time \- vision \- PyTorch Forums, дата последнего обращения: декабря 20, 2025, [https://discuss.pytorch.org/t/mnist-normalizing-and-scaling-the-dataset-at-the-same-time/95218](https://discuss.pytorch.org/t/mnist-normalizing-and-scaling-the-dataset-at-the-same-time/95218)
18. Correct way of normalizing and scaling the MNIST dataset \- Stack Overflow, дата последнего обращения: декабря 20, 2025, [https://stackoverflow.com/questions/63746182/correct-way-of-normalizing-and-scaling-the-mnist-dataset](https://stackoverflow.com/questions/63746182/correct-way-of-normalizing-and-scaling-the-mnist-dataset)
19. A Friendly Guide to Knowledge Distillation (with PyTorch code you can paste today), дата последнего обращения: декабря 20, 2025, [https://mohamed-stifi.medium.com/a-friendly-guide-to-knowledge-distillation-with-pytorch-code-you-can-paste-today-5a764762e7c7](https://mohamed-stifi.medium.com/a-friendly-guide-to-knowledge-distillation-with-pytorch-code-you-can-paste-today-5a764762e7c7)
20. trying to perform knowledge distillation using KL divergence loss . the loss is too high, дата последнего обращения: декабря 20, 2025, [https://stackoverflow.com/questions/78452655/trying-to-perform-knowledge-distillation-using-kl-divergence-loss-the-loss-is](https://stackoverflow.com/questions/78452655/trying-to-perform-knowledge-distillation-using-kl-divergence-loss-the-loss-is)
21. The magnitude of KL divergence loss is too low compared to Cross-entropy loss, дата последнего обращения: декабря 20, 2025, [https://discuss.pytorch.org/t/the-magnitude-of-kl-divergence-loss-is-too-low-compared-to-cross-entropy-loss/95775](https://discuss.pytorch.org/t/the-magnitude-of-kl-divergence-loss-is-too-low-compared-to-cross-entropy-loss/95775)
22. Knowledge Distillation: Principles, Algorithms, Applications, дата последнего обращения: декабря 20, 2025, [https://neptune.ai/blog/knowledge-distillation](https://neptune.ai/blog/knowledge-distillation)
23. Understanding Knowledge Distillation: In Simple Terms | by Nikita Parate | Medium, дата последнего обращения: декабря 20, 2025, [https://medium.com/@nikitaparate9/understanding-knowledge-distillation-in-simple-terms-01c247ee8a72](https://medium.com/@nikitaparate9/understanding-knowledge-distillation-in-simple-terms-01c247ee8a72)
24. \[Quick Review\] Exploring Feature-based Knowledge Distillation for Recommender System: A Frequency Perspective \- Liner, дата последнего обращения: декабря 20, 2025, [https://liner.com/review/exploring-featurebased-knowledge-distillation-for-recommender-system-frequency-perspective](https://liner.com/review/exploring-featurebased-knowledge-distillation-for-recommender-system-frequency-perspective)
25. Understanding the Gains from Repeated Self-Distillation \- NIPS papers, дата последнего обращения: декабря 20, 2025, [https://papers.nips.cc/paper_files/paper/2024/file/0eb1ac7551ddbae575415aa5183a88be-Paper-Conference.pdf](https://papers.nips.cc/paper_files/paper/2024/file/0eb1ac7551ddbae575415aa5183a88be-Paper-Conference.pdf)
26. Three mysteries in deep learning: Ensemble, knowledge distillation, and self-distillation, дата последнего обращения: декабря 20, 2025, [https://www.microsoft.com/en-us/research/blog/three-mysteries-in-deep-learning-ensemble-knowledge-distillation-and-self-distillation/](https://www.microsoft.com/en-us/research/blog/three-mysteries-in-deep-learning-ensemble-knowledge-distillation-and-self-distillation/)
27. Understanding the Gains from Repeated Self-Distillation \- arXiv, дата последнего обращения: декабря 20, 2025, [https://arxiv.org/html/2407.04600v1](https://arxiv.org/html/2407.04600v1)
28. Distilbert: A Smaller, Faster, and Distilled BERT \- Zilliz Learn, дата последнего обращения: декабря 20, 2025, [https://zilliz.com/learn/distilbert-distilled-version-of-bert](https://zilliz.com/learn/distilbert-distilled-version-of-bert)
29. Smaller, faster, cheaper, lighter: Introducing DistilBERT, a distilled version of BERT | by Victor Sanh | HuggingFace | Medium, дата последнего обращения: декабря 20, 2025, [https://medium.com/huggingface/smaller-faster-cheaper-lighter-introducing-dilbert-a-distilled-version-of-bert-8cf3380435b5](https://medium.com/huggingface/smaller-faster-cheaper-lighter-introducing-dilbert-a-distilled-version-of-bert-8cf3380435b5)
30. DistilBERT \- Hugging Face, дата последнего обращения: декабря 20, 2025, [https://huggingface.co/docs/transformers/en/model_doc/distilbert](https://huggingface.co/docs/transformers/en/model_doc/distilbert)
31. Distilling vs Fine Tunning : r/LLMDevs \- Reddit, дата последнего обращения: декабря 20, 2025, [https://www.reddit.com/r/LLMDevs/comments/1irpz19/distilling_vs_fine_tunning/](https://www.reddit.com/r/LLMDevs/comments/1irpz19/distilling_vs_fine_tunning/)
32. LLM Inference Optimization: How to Speed Up, Cut Costs, and Scale AI Models, дата последнего обращения: декабря 20, 2025, [https://deepsense.ai/blog/llm-inference-optimization-how-to-speed-up-cut-costs-and-scale-ai-models/](https://deepsense.ai/blog/llm-inference-optimization-how-to-speed-up-cut-costs-and-scale-ai-models/)
33. KL divergence loss too high. Need some help : r/MLQuestions \- Reddit, дата последнего обращения: декабря 20, 2025, [https://www.reddit.com/r/MLQuestions/comments/1cnv4h3/kl_divergence_loss_too_high_need_some_help/](https://www.reddit.com/r/MLQuestions/comments/1cnv4h3/kl_divergence_loss_too_high_need_some_help/)
