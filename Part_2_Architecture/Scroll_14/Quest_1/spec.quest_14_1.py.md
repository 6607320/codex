# quest_14_1.py Specification

## 1. Meta Information

- Domain: Scripting
- Complexity: High
- Language: Python
- Frameworks: PyTorch, torchvision, tqdm
- Context: ../AGENTS.md

Здесь Скрипт — это ритуал, где подмогающиеся библиотеки выступают в роли заклинательных томов. Небольшой путь через мегалитическую тьму архитектуры, чтобы превратить тяжёлый FP32 в лёгкий INT8 и при этом сохранить мудрость модели.

## 2. Goal & Purpose (Цель и Назначение)

Легенда о артефакте Quest 14.1: модуль показывает пост-трансформационную квантизацию обученной нейронной сети на MNIST с целью уменьшения размера модели и ускорения деплоймента, при сохранении разумной точности. Этот файл демонстрирует алхимию квантизации: обучающий FP32 голем затем превращается в INT8 голема через конфигурацию квантизации, калибровку и конвертацию; финал — сравнение веса артефактов на примере двух разумов и доказательство того, что новый голем стал значительно легче.

Инструкция для Авиа: Этот раздел задаёт «почему» и «зачем» кода, чтобы понять контекст и мотивацию механизма квантизации, её ограничений и ожидаемого эффекта на размер модели.

## 3. Interface Contract (Интерфейсный Контракт)

### 3.1. Inputs (Входы)

Source: STDIN / CLI Args / API Request — не применяется в явной форме; скрипт автономно загружает данные MNIST через torchvision.
Format: JSON | Text | Binary — не предусмотрено явной структурой входов
Schema:
export interface InputData {
// В текущем артефакте входные данные не имеют внешнего CLI/API схемы.
}

### 3.2. Outputs (Выходы)

Destination: STDOUT
Format: Text
Success Criteria: Exit Code 0
Schema:
export interface OutputResult {
size_fp32_kb: number;
size_int8_kb: number;
compression_ratio: number;
notes?: string;
}

Артефакт выводит текстовые сообщения в консоль о ходе ритуала и печатает итоговое сравнение веса артефактов (FP32 против INT8). Опционально возвращает структурированные данные в виде OutputResult.

## 4. Implementation Details (The Source DNA / Исходный Код)

### 4.1. Algorithmic Logic (Для исполняемого кода)

Шаги ритуала описаны ниже в виде последовательности действий, не приводя клик-уроков кода:

- Акт 1: Подготовка Гримуара
  - Подгружаются сущности магии: PyTorch, nn, F, optim, torchvision и индикатор прогресса tqdm.
  - Создаётся чертёж миниатюрного голема MiniCNN: цепь из двух свёрточных этажей и линейного динамика на выходе из 10 классов.
  - Формируется конвейер подготовки данных: MNIST как учебный кодекс, конвейер трансформаций ToTensor и нормализация.
  - Задаётся поднос данных: DataLoader со размером пачки 64 и перемешиванием.
  - Сформирован FP32 голем: инициализация, вывод в режим обучения, выбор оптимизатора Adam и критерия CrossEntropyLoss.
  - Входной цикл обучения ограничен 100 батчами: на каждом батче производятся нули grad, форсируется предсказание, расчёт потери, обратное распространение и обновление параметров.

- Акт 2: Ритуал Трансмутации (Подготовка к квантованию)
  - Появляется новый экземпляр MiniCNN без обучения — модель-«чистый лист».
  - Переносятся веса обученного FP32 голема через state_dict.
  - Модель переводится в режим оценки (eval).
  - Задаётся конфигурация квантования via get_default_qconfig("fbgemm").
  - Внутренний механизм prepare расставляет наблюдателей внутри структуры.

- Акт 3: Калибровка
  - Сообщается начало калибровки.
  - На 10 пачках данных прогоняются данные через модель, чтобы собрать статистику необходимую для квантования.

- Акт 4: Трансмутация
  - Выполняется ключевое заклинание: torch.quantization.convert(model, inplace=True), превращающее FP32 в INT8.

- Акт 5: Сравнение Веса Артефактов
  - Сохранение весов FP32 и INT8 временно в файлы.
  - Измерение размера файлов в килобайтах.
  - Вывод итогов: веса FP32 и INT8 и отношение облегчения.
  - Очистка временных файлов и завершение ритуала.

- Визуальная часть: в финале скрипт печатает заголовок сравнения и итоговое соотношение, демонстрируя, что трансмутация действительно уменьшила вес артефакта.

### 4.2. Declarative Content (Для конфигураций и данных)

- Конфигурации квантования: стандартная конфигурация через get_default_qconfig("fbgemm").
- Данные для обучения: MNIST, трансформации ToTensor и нормализация (0.1307, 0.3081).
- Временные файлы: temp_fp32_model.pth и temp_int8_model.pth.

## 5. Structural Decomposition (Декомпозиция структуры)

- MiniCNN (класс)
  - **init**: конструктор архитектуры (conv1, conv2, fc1)
  - forward: поток данных через слои и лог-софтмакс
- transform: пайплайн преобразований данных
- train_loader: загрузчик MNIST
- model_fp32: обученный FP32 голем
- optimizer, criterion: инструменты обучения
- Цикл обучения: проход через данные, вычисление потерь, обновления
- model_to_quantize: копия модели для квантования
- qconfig: конфигурация квантования
- prepare / calibrate / convert: шаги квантования
- Файловые операции: сохранение весов, измерение размеров и удаление временных файлов

## 6. System Context & Constraints (Системный контекст и Ограничения)

### 6.1. Technical Constraints

- Performance: стандартный CPU (не используется CUDA); сцена сосредоточена на небольшом наборе MNIST, поэтому память умеренная.
- Concurrency: последовательная обработка; нет параллельных процессов в основном цикле.
- Dependencies: PyTorch и torchvision для моделирования; tqdm для индикатора прогресса.
- Данные: MNIST доступен через torchvision.datasets; загрузка происходит на этапе исполнения.
- Эфемерные артефакты: используются временные файлы для вычисления размера модели.

### 6.2. Prohibited Actions (Negative Constraints)

- НЕ хранить секреты в открытом виде; не использовать явные токены в коде.
- НЕ печатать сырые данные в продакшене; логи должны быть безопасны.
- НЕ делать синхронные сетевые вызовы в критичных местах основного цикла.
- НЕ оборачивать конфигурационные файлы (.yaml, .json) в скрипты как часть исполняемого кода.
- НЕ менять версии библиотек или путей во время реконструкции артефакта без явной необходимости.

## 7. Verification & Testing (Верификация)

Геркин-сценарии

Feature: Script quest_14_1 quantization ritual
Scenario: Successful quantization and weight comparison
Given окружение содержит PyTorch, torchvision и доступ к MNIST
When выполняется ритуал квантования: обучение FP32, калибровка и конвертация
Then выводится итоговое сравнение веса и отношение облегчения, Exit Code 0

Feature: Calibration failure scenario
Scenario: Insufficient data for калибровки
Given данные MNIST недоступны или загрузкаDataLoader терпит сбой
When попытка калибровки начинается
Then процесс завершается с ошибкой и выходной код не равен 0, с соответствующим сообщением лога

Итого, артефакт quest_14_1.py — это целостный ритуал техномагии, где FP32 голем обезличивает себя через квантование, чтобы стать легче и пригоднее для deployment, сохранив при этом мудрость в разумной степени точности.
