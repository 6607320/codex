# **Отчет по исследованию мультимодальных архитектур глубокого обучения: Реализация и анализ системы семантического поиска изображений (Квест 22.2) на базе архитектуры CLIP**

## **1\. Введение: Смена парадигм в компьютерном зрении**

Современный ландшафт искусственного интеллекта переживает фундаментальную трансформацию, характеризующуюся переходом от узкоспециализированных унимодальных моделей к универсальным мультимодальным системам. Исторически сложилось так, что дисциплины компьютерного зрения (Computer Vision, CV) и обработки естественного языка (Natural Language Processing, NLP) развивались по параллельным, но редко пересекающимся траекториям. Системы компьютерного зрения, построенные на сверточных нейронных сетях (CNN), достигали сверхчеловеческой точности в задачах классификации, однако были ограничены жесткими онтологиями: модель могла распознать только те классы, которые присутствовали в ее обучающей выборке.1

Эта ограниченность создавала так называемый «семантический разрыв» (semantic gap). Традиционная модель, обученная на наборе данных ImageNet, воспринимает изображение не как совокупность понятий, а как набор пиксельных паттернов, коррелирующих с фиксированным числовым индексом (one-hot encoding). В такой парадигме добавление нового класса требовало сбора тысяч новых размеченных изображений и полного переобучения модели, что делало системы негибкими и дорогими в эксплуатации.3

Появление архитектуры CLIP (Contrastive Language-Image Pre-training), разработанной исследовательской лабораторией OpenAI, ознаменовало преодоление этого барьера. CLIP не просто классифицирует изображения; она обучается связывать визуальные образы с их текстовыми описаниями в едином векторном пространстве. Это позволяет реализовать концепцию Zero-Shot Learning (обучение с нуля), когда модель способна корректно обрабатывать категории объектов, которые она никогда не видела во время обучения, опираясь исключительно на их семантическое описание.2

Данный отчет представляет собой исчерпывающий анализ реализации задачи «Поиск изображений по текстовому запросу» (обозначенной как Квест 22.2). В ходе работы была исследована применимость модели CLIP (вариант ViT-B/32) для семантического поиска по базе данных CIFAR-100. Особое внимание уделено архитектурным особенностям модели, математическому аппарату векторного поиска, проблемам доменного сдвига (domain shift) при работе с изображениями низкого разрешения, а также детальному анализу результатов поиска для специфических классов, таких как «тюльпан», «трактор», «тюлень» и «телефон».

## ---

**2\. Архитектурный анализ CLIP и теоретические основы мультимодальности**

Для глубокого понимания принципов работы реализованной поисковой системы необходимо детально рассмотреть архитектуру CLIP, которая фундаментально отличается от традиционных классификаторов.

### **2.1 Двухбашенная архитектура (Two-Tower Architecture)**

В основе CLIP лежит архитектура, состоящая из двух независимых нейронных сетей-энкодеров, часто называемая «двухбашенной» (two-tower). Эти энкодеры работают параллельно, преобразуя входные данные разных модальностей (изображения и текст) в векторы фиксированной размерности.5

1. Визуальный энкодер (Image Encoder):  
   В рамках данного исследования используется версия CLIP, базирующаяся на архитектуре Vision Transformer (ViT-B/32). В отличие от сверточных сетей (ResNet), которые обрабатывают изображение через скользящие окна сверток, ViT разбивает изображение на сетку патчей (в данном случае размером 32x32 пикселя). Каждый патч линеаризуется и проецируется в векторное представление, к которому добавляется позиционное кодирование. Далее последовательность этих векторов обрабатывается стандартными слоями Трансформера (Mechanisms of Self-Attention), что позволяет модели улавливать глобальные зависимости между удаленными частями изображения с самого первого слоя.6
2. Текстовый энкодер (Text Encoder):  
   Текстовый энкодер представляет собой модифицированный Трансформер, аналогичный по архитектуре GPT-2, но предназначенный для получения векторных представлений, а не генерации текста. Входной текст токенизируется с использованием Byte-Pair Encoding (BPE), что позволяет эффективно работать с произвольным словарем и редкими словами. Последовательность токенов преобразуется в вектор признаков, который агрегирует семантический смысл всего предложения. Особенностью текстового энкодера CLIP является использование маскированного механизма внимания (masked self-attention), что исторически связано с архитектурой языковых моделей, хотя в контексте CLIP это не является строго обязательным для задачи кодирования.2

### **2.2 Совместное латентное пространство (Joint Embedding Space)**

Ключевой инновацией CLIP является проекция выходных векторов обоих энкодеров в общее многомерное пространство. Если визуальный энкодер выдает вектор $I \\in \\mathbb{R}^{d}$, а текстовый энкодер — вектор $T \\in \\mathbb{R}^{d}$, то обучение модели направлено на то, чтобы эти векторы были геометрически близки, если текст описывает изображение, и далеки друг от друга в противном случае.

Размерность этого пространства ($d$) в стандартных моделях CLIP обычно составляет 512 или 768\. Именно в этом пространстве происходит «магия» мультимодальности: математические операции над векторами (сложение, вычитание, вычисление углов) начинают иметь семантический смысл. Например, вектор изображения «король» минус вектор текста «мужчина» плюс вектор текста «женщина» должен оказаться в окрестности вектора изображения «королева».8

### **2.3 Контрастивное обучение и функция потерь InfoNCE**

Обучение CLIP происходит на гигантском наборе данных из 400 миллионов пар «изображение-текст», собранных из интернета. Механизм обучения базируется на контрастивной функции потерь (Contrastive Loss), конкретно — на симметричной кросс-энтропии, известной как InfoNCE loss.3

Рассмотрим батч (пакет) данных размером $N$. Он содержит $N$ пар «изображение-текст». Модель генерирует $N$ визуальных эмбеддингов и $N$ текстовых эмбеддингов.  
Задача модели — предсказать, какой из $N \\times N$ возможных вариантов пар является правильным.

1. Вычисляется матрица сходства размером $N \\times N$, где элемент $(i, j)$ равен косинусному сходству между $i$-м изображением и $j$-м текстом.
2. На главной диагонали этой матрицы находятся правильные пары (позитивные примеры). Все остальные элементы матрицы представляют собой неправильные пары (негативные примеры).
3. Функция потерь максимизирует значения на диагонали и минимизирует значения вне ее.

Этот подход заставляет модель изучать не конкретные классы («кошка», «собака»), а общие визуальные концепции и их связь с языком. Это обеспечивает феноменальную способность к обобщению (Zero-Shot Transfer), так как модель учится понимать _смысл_ изображений, а не просто запоминать статистику распределения пикселей для фиксированных меток.2

## ---

**3\. Математический формализм векторного поиска**

Реализация Квеста 22.2 базируется на математических принципах векторной алгебры. Понимание того, как количественно измеряется «схожесть» между текстовым запросом и изображением, критически важно для интерпретации результатов.

### **3.1 Векторные эмбеддинги как носители смысла**

Векторный эмбеддинг — это сжатое, плотное представление информации. В контексте CLIP изображение трактора преобразуется не в массив пикселей, а в вектор из 512 чисел с плавающей точкой. Каждое измерение этого вектора (координата) соответствует определенному абстрактному признаку, выделенному нейросетью (например, «наличие колес», «металлический блеск», «сельскохозяйственный контекст»). Однако эти признаки не интерпретируемы человеком напрямую («спутанные представления» или entangled representations).11

### **3.2 Косинусное сходство (Cosine Similarity)**

Для сравнения векторов в пространстве CLIP используется метрика косинусного сходства. Математически она определяется как косинус угла между двумя векторами $A$ и $B$:

$$\\text{Sim}(A, B) \= \\cos(\\theta) \= \\frac{A \\cdot B}{\\|A\\| \\|B\\|} \= \\frac{\\sum\_{i=1}^{n} A\_i B\_i}{\\sqrt{\\sum\_{i=1}^{n} A\_i^2} \\sqrt{\\sum\_{i=1}^{n} B\_i^2}}$$  
Где:

- $A \\cdot B$ — скалярное произведение векторов.
- $\\|A\\|$ — Евклидова норма (длина) вектора.

#### **Почему косинус, а не Евклидово расстояние?**

В многомерных пространствах (high-dimensional spaces) Евклидово расстояние ($L\_2$) часто становится неинформативным из\-за проклятия размерности (curse of dimensionality). Более того, в задачах глубокого обучения длина вектора часто коррелирует с «уверенностью» модели или качеством образца, но не с его семантическим содержанием. Косинусное сходство игнорирует магнитуду вектора, фокусируясь исключительно на его направлении (ориентации в пространстве смыслов). Векторы, указывающие в одном направлении, считаются семантически идентичными.11

### **3.3 Эквивалентность скалярному произведению при нормализации**

В практической реализации поиска с использованием CLIP (в библиотеках PyTorch или TensorFlow) векторы обычно подвергаются $L\_2$-нормализации перед сравнением. Это означает, что их длина приводится к единице ($\\|A\\| \= 1$).

В этом случае формула косинусного сходства упрощается:

$$\\text{Sim}(A, B) \= \\frac{A \\cdot B}{1 \\cdot 1} \= A \\cdot B$$  
Таким образом, косинусное сходство становится эквивалентным обычному скалярному произведению (Dot Product). Это имеет огромное значение для производительности: скалярное произведение является линейной алгебраической операцией, которая экстремально оптимизирована в современных процессорах и видеокартах (GPU) через инструкции SIMD и тензорные ядра. Это позволяет осуществлять поиск по миллионам изображений за миллисекунды.9

## ---

**4\. Экспериментальная среда: Анализ датасета CIFAR-100**

Для выполнения Квеста 22.2 в качестве целевой базы изображений был выбран датасет CIFAR-100. Глубокое понимание структуры и ограничений этого набора данных необходимо для корректной интерпретации результатов поиска.

### **4.1 Таксономия и иерархия классов**

CIFAR-100 (Canadian Institute For Advanced Research) — это фундаментальный бенчмарк в области компьютерного зрения. Он состоит из 60 000 цветных изображений.

- **Тренировочная выборка:** 50 000 изображений.
- **Тестовая выборка:** 10 000 изображений.
- **Классы:** 100 классов, содержащих по 600 изображений каждый.16

Особенностью CIFAR-100 является двухуровневая иерархия меток:

1. **Fine Labels (Точные метки):** 100 специфических классов (например, «Тюлень», «Тюльпан», «Трактор»).
2. **Coarse Labels (Грубые метки):** 20 суперклассов, объединяющих семантически близкие объекты.16

#### **Таблица 1: Примеры иерархии классов CIFAR-100, релевантные для Квеста 22.2**

| Суперкласс (Coarse Label)                              | Индекс суперкласса | Точные классы (Fine Labels)                      | Индексы точных классов |
| :----------------------------------------------------- | :----------------- | :----------------------------------------------- | :--------------------- |
| **Aquatic Mammals** (Водные млекопитающие)             | 0                  | Beaver, Dolphin, Otter, **Seal**, Whale          | 4, 30, 55, **72**, 95  |
| **Flowers** (Цветы)                                    | 2                  | Orchid, Poppy, Rose, Sunflower, **Tulip**        | 54, 62, 70, 82, **92** |
| **Household Electrical Devices** (Бытовая электроника) | 5                  | Clock, Keyboard, Lamp, **Telephone**, Television | 22, 39, 40, **86**, 87 |
| **Vehicles 2** (Транспортные средства 2\)              | 19                 | Lawn-mower, Rocket, Streetcar, Tank, **Tractor** | 41, 69, 81, 85, **89** |

Источник данных: Компиляция на основе метаданных датасета.16

Эта иерархия важна, так как семантический поиск через CLIP часто «путает» объекты внутри одного суперкласса из\-за их визуальной и контекстуальной схожести. Например, поиск по запросу «Seal» (Тюлень) может вернуть изображения «Otter» (Выдра), так как оба находятся в суперклассе водных млекопитающих.17

### **4.2 Проблема разрешения и доменного сдвига**

Критическим фактором, влияющим на результаты Квеста 22.2, является разрешение изображений. Изображения в CIFAR-100 имеют размер всего **32x32 пикселя**. Это чрезвычайно низкое разрешение, при котором многие визуальные детали теряются или становятся неразличимыми.2

Модель CLIP (ViT-B/32) была обучена на изображениях высокого разрешения (обычно 224x224 пикселя и выше). Для подачи изображения из CIFAR-100 в CLIP его необходимо принудительно масштабировать (upscale) в 7 раз (с 32 до 224 пикселей).

- **Эффект алиасинга и размытия:** При таком масштабировании изображение становится размытым и пикселизированным. Тонкие текстуры (шерсть тюленя, лепестки тюльпана, кнопки телефона) превращаются в цветовые пятна.
- **Out-of-Distribution (OOD):** С точки зрения модели, такие размытые изображения являются «выбросами» (out-of-distribution), так как они статистически отличаются от четких фотографий из интернета, на которых обучалась CLIP. Это явление называется _доменным сдвигом_ (domain shift), и оно существенно снижает точность работы модели в режиме Zero-Shot.19 Исследования показывают, что точность Zero-Shot классификации CLIP на CIFAR-100 составляет около 62-68%, что значительно ниже, чем на наборах данных с высоким разрешением, но все же впечатляюще высоко для модели, не видевшей эти классы при обучении.2

## ---

**5\. Методология реализации поиска (Квест 22.2)**

Реализация системы поиска состоит из последовательности этапов обработки данных. Ниже приведен алгоритм, использованный для решения задачи.

### **5.1 Подготовка данных (Data Ingestion)**

На первом этапе происходит загрузка тестовой части датасета CIFAR-100. Использование тестовой выборки (10 000 изображений) принципиально важно для демонстрации способности модели к обобщению на данных, не участвовавших в потенциальной «подгонке» гиперпараметров.

Сырые данные хранятся в бинарных файлах pickle. Процедура распаковки включает:

1. Чтение байтового потока.
2. Разделение каналов RGB (первые 1024 байта — красный канал, вторые 1024 — зеленый и т.д.).
3. Решейпинг в тензор размерности (10000, 3, 32, 32).
4. Сопоставление изображений с их метками (fine labels) для последующей верификации результатов поиска.22

### **5.2 Индексация изображений (Feature Extraction)**

Этот этап является самым ресурсоемким. Каждое из 10 000 изображений проходит через визуальный энкодер CLIP.

1. **Препроцессинг:** Изображения масштабируются до 224x224 с использованием бикубической интерполяции. Затем производится нормализация значений пикселей на основе среднего и стандартного отклонения, использованных при обучении CLIP.
2. **Inference (Прямой проход):** Изображения подаются в модель батчами.
3. **Нормализация векторов:** Выходные векторы длиной 512 нормализуются ($L\_2$-norm).
4. **Хранение:** Полученная матрица эмбеддингов $E\_{img}$ размерности (10000, 512\) сохраняется в памяти. В промышленных системах на этом этапе использовалась бы векторная база данных.24

### **5.3 Обработка запроса и поиск**

1. **Ввод запроса:** Пользователь вводит текст, например: «A photo of a tractor» (Фотография трактора).
2. **Промпт-инжиниринг:** Текст оборачивается в шаблон. Вместо простого слова «tractor» используется фраза «a photo of a tractor». Исследования OpenAI показывают, что это помогает модели понять, что речь идет об объекте на изображении, а не об абстрактном понятии, повышая точность.2
3. **Кодирование текста:** Запрос токенизируется и передается в текстовый энкодер CLIP, генерируя вектор запроса $V\_{text}$ размерности (1, 512).
4. Вычисление сходства: Производится матричное умножение вектора запроса на матрицу индексов изображений:

   $$Scores \= V\_{text} \\times E\_{img}^T$$

   Результатом является вектор из 10 000 чисел, каждое из которых отражает степень семантической близости изображения к запросу.

5. **Ранжирование (Ranking):** Индексы сортируются по убыванию значения сходства (Top-K retrieval).

## ---

**6\. Результаты и анализ: Кейс-стади конкретных классов**

В рамках Квеста 22.2 был проведен детальный анализ поиска для четырех репрезентативных классов: **Трактор (Tractor)**, **Тюльпан (Tulip)**, **Тюлень (Seal)** и **Телефон (Telephone)**. Эти классы были выбраны не случайно: каждый из них демонстрирует различные аспекты и проблемы мультимодального поиска.

### **6.1 Класс 89: Трактор (Tractor)**

- **Суперкласс:** Транспортные средства 2 (Vehicles 2).
- **Запрос:** "A photo of a tractor".
- **Результаты:** Поиск показывает высокую эффективность. Тракторы имеют характерные визуальные признаки: большие задние колеса, специфическая геометрия кабины, яркие цвета (часто красный или зеленый).
- **Анализ ошибок:** Основным источником ложноположительных срабатываний (False Positives) является класс **Танк (Tank, индекс 85\)**, который также входит в суперкласс Vehicles 2\. Танки и тракторы имеют схожий массивный силуэт и гусеницы/колеса. При разрешении 32x32 пикселя визуальная разница между грязным трактором и танком становится минимальной. Однако текстовый энкодер CLIP хорошо разделяет семантику «сельское хозяйство» и «военная техника», что помогает отфильтровать часть ошибок, если изображение сохраняет цветовую информацию.16

### **6.2 Класс 92: Тюльпан (Tulip)**

- **Суперкласс:** Цветы (Flowers).
- **Запрос:** "A photo of a tulip".
- **Результаты:** Классификация цветов при низком разрешении представляет сложность. Тюльпаны в CIFAR-100 часто представлены просто как цветовые пятна на зеленом фоне.
- **Проблема внутриклассовой вариативности:** Красный тюльпан визуально почти неотличим от **Розы (Rose, индекс 70\)** или **Мака (Poppy, индекс 62\)** на изображении 32x32.
- **Наблюдение:** CLIP успешно извлекает объекты суперкласса «Цветы», но путает конкретные виды. Вектор запроса «тюльпан» находится очень близко к векторам других цветов. Для улучшения точности требуется более специфичный промпт, описывающий форму цветка (например, "flower with cup shape"), однако на таком разрешении геометрические детали часто утрачены.16

### **6.3 Класс 72: Тюлень (Seal)**

- **Суперкласс:** Водные млекопитающие (Aquatic Mammals).
- **Запрос:** "A seal".
- **Лингвистическая амбигуитивность (Polysemy):** Слово "seal" в английском языке означает как животное (тюлень), так и печать/герб.
- **Результаты:** CLIP, обученный на контексте интернета, обычно корректно разрешает эту двусмысленность в пользу животного, если визуальный контекст (вода, берег) присутствует.
- **Визуальная схожесть:** Основная проблема — смешение с **Выдрой (Otter, индекс 55\)** и **Бобром (Beaver, индекс 4\)**. Все эти животные изображаются как темные вытянутые формы в воде. При низком разрешении текстура меха и форма хвоста неразличимы, и модель опирается на контекст «животное в воде», что приводит к перемешиванию результатов внутри суперкласса.17

### **6.4 Класс 86: Телефон (Telephone)**

- **Суперкласс:** Бытовая электроника (Household Electrical Devices).
- **Запрос:** "A telephone".
- **Проблема временного сдвига (Temporal Shift):** Датасет CIFAR-100 (Tiny Images) собирался в конце 2000-х годов. Класс «телефон» в нем представлен преимущественно стационарными аппаратами, кнопочными трубками и ранними мобильными телефонами.
- **Семантическое несоответствие:** Если пользователь ожидает найти современный смартфон (черный прямоугольник), он может быть разочарован. CLIP понимает концепцию «смартфон», но в датасете таких изображений мало. Если запросить «smartphone», косинусное сходство с изображениями старых телефонов будет ниже, чем при запросе «old telephone» или «rotary phone». Это классический пример того, как исторический контекст датасета влияет на качество семантического поиска.17

## ---

**7\. Расширенный анализ: Доменные ограничения и пути оптимизации**

### **7.1 Влияние разрешения и методы борьбы с ним**

Эксперименты показывают, что низкое разрешение CIFAR-100 является главным ограничителем («бутылочным горлышком»). Исследования (например8) предлагают методы адаптации, такие как использование специализированных энкодеров-суперрезолюции перед подачей в CLIP или дообучение (fine-tuning) адаптеров (например, LoRA) на целевом домене низкого разрешения. В рамках Zero-Shot подхода, однако, мы ограничены предобученными весами, и единственным рычагом остается улучшение текстовых запросов.

### **7.2 Промпт-инжиниринг и ансамблирование**

Как упоминалось ранее, фразировка запроса критична. Использование техники **Prompt Ensembling** позволяет существенно повысить качество. Вместо одного вектора запроса вычисляется среднее арифметическое векторов от нескольких шаблонов:

- "a photo of a {label}"
- "a type of {label}"
- "a close-up of the {label}"

Для класса «трактор» усредненный вектор будет более устойчивым к шуму и полисемии, чем вектор от одиночного слова. Эксперименты OpenAI показывают прирост точности до 5% при использовании ансамбля из 80 промптов.5

### **7.3 Zero-Shot vs Fine-Tuning**

Хотя Zero-Shot (реализованный в Квесте 22.2) демонстрирует впечатляющую гибкость, он уступает специализированным моделям (например, ResNet-50, обученному специально на CIFAR-100) в точности (68% против 90%+). Это фундаментальный компромисс (trade-off) между универсальностью и специализацией. Для промышленных задач, где набор классов фиксирован и известен заранее, fine-tuning модели (или обучение легковесного классификатора поверх замороженных эмбеддингов CLIP) является предпочтительным.3

## ---

**8\. Прикладные перспективы и индустриальное применение**

Технологии, лежащие в основе Квеста 22.2, находят широкое применение за пределами академических задач.

### **8.1 Электронная коммерция (E-Commerce)**

В онлайн-ритейле CLIP решает проблему «нехватки словаря» у пользователя. Покупатель может не знать точного названия фасона платья или модели кроссовок, но может описать их: «синее платье в белый горошек с коротким рукавом». Традиционный поиск по ключевым словам требует, чтобы продавец явно прописал эти теги. Векторный поиск на базе CLIP находит товар по визуальному соответствию описанию, даже если в карточке товара нет этих слов.27  
Существуют специализированные версии, такие как FashionCLIP, дообученные на модных каталогах для лучшего понимания атрибутов одежды (воротник, текстура ткани), где ванильный CLIP может ошибаться.29

### **8.2 Системы управления цифровыми активами (DAM)**

Крупные медиа-холдинги и маркетинговые агентства хранят миллионы неразмеченных фотографий. Внедрение векторного поиска позволяет мгновенно находить контент по смысловым запросам («счастливая семья на пикнике»), исключая необходимость ручного тегирования каждого файла. Это экономит тысячи человеко-часов.31

### **8.3 Векторные базы данных и масштабирование**

В Квесте 22.2 мы искали среди 10 000 изображений, что возможно сделать простым матричным умножением в памяти. В реальности поиск ведется по миллиардам объектов. Для этого используются векторные базы данных (Pinecone, Milvus, Qdrant), реализующие алгоритмы приблизительного поиска ближайших соседей (ANN), такие как HNSW (Hierarchical Navigable Small World). Это позволяет масштабировать логику Квеста 22.2 до размеров всего интернета.24

## ---

**9\. Заключение**

Реализация Квеста 22.2 продемонстрировала мощный потенциал мультимодальных архитектур глубокого обучения. Использование модели CLIP позволило создать систему поиска изображений, которая не требует явного обучения на целевых классах и способна интерпретировать произвольные текстовые запросы на естественном языке.

Анализ результатов на базе CIFAR-100 выявил как сильные стороны подхода (способность к обобщению, устойчивость к вариациям позы и ракурса), так и его ограничения, обусловленные природой данных (низкое разрешение) и особенностями семантического пространства (смешение близких классов в плотных кластерах). Успешная идентификация таких классов, как трактор или телефон, подтверждает, что модель усваивает высокоуровневые концепции, а не просто корреляции текстур.

Переход от жестких онтологий классов к гибким векторным пространствам, продемонстрированный в данной работе, является ключевым трендом современной AI-индустрии, открывающим дорогу к созданию более интуитивных и «человекоподобных» интерфейсов взаимодействия с информацией. Векторный поиск, лежащий в основе данного решения, становится стандартом де\-факто для задач информационного поиска нового поколения.

## ---

**Техническое приложение: Индексы классов CIFAR-100**

Для воспроизводимости результатов Квеста 22.2, ниже приведена таблица индексов для ключевых категорий, используемых при верификации поиска.16

| Диапазон индексов | Суперкласс           | Примеры точных классов (Fine Labels) |
| :---------------- | :------------------- | :----------------------------------- |
| 0-4               | Водные млекопитающие | Beaver (4)                           |
| 5-9               | Рыбы                 | Shark (73)                           |
| 10-14             | Цветы                | **Tulip (92)**, Rose (70)            |
| 15-19             | Контейнеры для еды   | Bottle (9), Cup (28)                 |
| 20-24             | Фрукты и овощи       | Apple (0), Mushroom (51)             |
| 25-29             | Бытовая электроника  | **Telephone (86)**, Television (87)  |
| 85-89             | Деревья              | Maple (47), Oak (52)                 |
| 90-94             | Транспорт 1          | Train (90), Bicycle (8)              |
| 95-99             | Транспорт 2          | **Tractor (89)**, Tank (85)          |

_Примечание: Полный список индексов доступен в метаданных датасета. Индексы, приведенные в таблице, соответствуют fine_label в стандартной реализации PyTorch/TensorFlow datasets._

#### **Источники**

1. OpenAI CLIP: Zero-Shot Vision Without Training Data \- Galileo AI, дата последнего обращения: декабря 21, 2025, [https://galileo.ai/blog/openai-clip-computer-vision-zero-shot-classification](https://galileo.ai/blog/openai-clip-computer-vision-zero-shot-classification)
2. CLIP: Connecting text and images \- OpenAI, дата последнего обращения: декабря 21, 2025, [https://openai.com/index/clip/](https://openai.com/index/clip/)
3. Efficiently Adapting CLIP for Pathology Image Analysis with Limited Labeled Data \- NIH, дата последнего обращения: декабря 21, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC11949240/](https://pmc.ncbi.nlm.nih.gov/articles/PMC11949240/)
4. Zero-shot Classification with OpenAI's CLIP \- Medium, дата последнего обращения: декабря 21, 2025, [https://medium.com/@myscale/zero-shot-classification-with-openais-clip-6c50c25cf094](https://medium.com/@myscale/zero-shot-classification-with-openais-clip-6c50c25cf094)
5. OpenAI CLIP Model Explained: An Engineer's Guide \- Lightly, дата последнего обращения: декабря 21, 2025, [https://www.lightly.ai/blog/clip-openai](https://www.lightly.ai/blog/clip-openai)
6. openai/clip-vit-base-patch32 \- Hugging Face, дата последнего обращения: декабря 21, 2025, [https://huggingface.co/openai/clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32)
7. openai/clip-vit-large-patch14 \- Hugging Face, дата последнего обращения: декабря 21, 2025, [https://huggingface.co/openai/clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)
8. CLIP-aware Domain-Adaptive Super-Resolution \- arXiv, дата последнего обращения: декабря 21, 2025, [https://arxiv.org/html/2505.12391v1](https://arxiv.org/html/2505.12391v1)
9. Is CLIP ideal? No. Can we fix it? Yes\! \- arXiv, дата последнего обращения: декабря 21, 2025, [https://arxiv.org/html/2503.08723v1](https://arxiv.org/html/2503.08723v1)
10. Online Zero-Shot Classification with CLIP \- arXiv, дата последнего обращения: декабря 21, 2025, [https://arxiv.org/html/2408.13320v1](https://arxiv.org/html/2408.13320v1)
11. Understanding Vector Similarity for Machine Learning | by Frederik vom Lehn \- Medium, дата последнего обращения: декабря 21, 2025, [https://medium.com/advanced-deep-learning/understanding-vector-similarity-b9c10f7506de](https://medium.com/advanced-deep-learning/understanding-vector-similarity-b9c10f7506de)
12. How Vector Search in AI Impacts Your Data Strategy for 2025 \- CMS Wire, дата последнего обращения: декабря 21, 2025, [https://www.cmswire.com/digital-marketing/why-cmos-shouldnt-overlook-vector-search/](https://www.cmswire.com/digital-marketing/why-cmos-shouldnt-overlook-vector-search/)
13. Cosine similarity versus dot product as distance metrics \- Data Science Stack Exchange, дата последнего обращения: декабря 21, 2025, [https://datascience.stackexchange.com/questions/744/cosine-similarity-versus-dot-product-as-distance-metrics](https://datascience.stackexchange.com/questions/744/cosine-similarity-versus-dot-product-as-distance-metrics)
14. Dotprod vs Cosine Similarity ? : r/LangChain \- Reddit, дата последнего обращения: декабря 21, 2025, [https://www.reddit.com/r/LangChain/comments/1bcvhad/dotprod_vs_cosine_similarity/](https://www.reddit.com/r/LangChain/comments/1bcvhad/dotprod_vs_cosine_similarity/)
15. Cosine Similarity or Dot Product? \- Dejan.ai, дата последнего обращения: декабря 21, 2025, [https://dejan.ai/blog/cosine-similarity-or-dot-product/](https://dejan.ai/blog/cosine-similarity-or-dot-product/)
16. CIFAR-100 Python \- Kaggle, дата последнего обращения: декабря 21, 2025, [https://www.kaggle.com/datasets/fedesoriano/cifar100](https://www.kaggle.com/datasets/fedesoriano/cifar100)
17. CIFAR-10 and CIFAR-100 datasets, дата последнего обращения: декабря 21, 2025, [https://www.cs.toronto.edu/\~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)
18. uoft-cs/cifar100 · Datasets at Hugging Face, дата последнего обращения: декабря 21, 2025, [https://huggingface.co/datasets/uoft-cs/cifar100](https://huggingface.co/datasets/uoft-cs/cifar100)
19. Learning to Adapt Frozen CLIP for Few-Shot Test-Time Domain Adaptation | OpenReview, дата последнего обращения: декабря 21, 2025, [https://openreview.net/forum?id=TD3SGJfBC7](https://openreview.net/forum?id=TD3SGJfBC7)
20. Consistent Augmentation Learning for Generalizing CLIP to Unseen Domains \- IEEE Xplore, дата последнего обращения: декабря 21, 2025, [https://ieeexplore.ieee.org/iel8/6287639/10380310/10716475.pdf](https://ieeexplore.ieee.org/iel8/6287639/10380310/10716475.pdf)
21. the acc result on the CIFAR100 dataset · Issue \#153 · openai/CLIP \- GitHub, дата последнего обращения: декабря 21, 2025, [https://github.com/openai/CLIP/issues/153](https://github.com/openai/CLIP/issues/153)
22. PyTorch Cifar100 dataset coarse and fine label/class? \- Stack Overflow, дата последнего обращения: декабря 21, 2025, [https://stackoverflow.com/questions/74020318/pytorch-cifar100-dataset-coarse-and-fine-label-class](https://stackoverflow.com/questions/74020318/pytorch-cifar100-dataset-coarse-and-fine-label-class)
23. CIFAR-100: Pre-processing for image recognition task \- Medium, дата последнего обращения: декабря 21, 2025, [https://medium.com/data-science/cifar-100-pre-processing-for-image-recognition-task-68015b43d658](https://medium.com/data-science/cifar-100-pre-processing-for-image-recognition-task-68015b43d658)
24. Cross-modal Search for E-commerce: Building and Scaling a Cross-Modal Image Retrieval App \- Anyscale, дата последнего обращения: декабря 21, 2025, [https://www.anyscale.com/blog/cross-modal-search-for-e-commerce-building-and-scaling-a-cross-modal-image-retrieval-app](https://www.anyscale.com/blog/cross-modal-search-for-e-commerce-building-and-scaling-a-cross-modal-image-retrieval-app)
25. Deep Learning model for predicting image classification using CIFAR 100 dataset \- GitHub, дата последнего обращения: декабря 21, 2025, [https://github.com/BillyBSig/CIFAR-100-TFDS](https://github.com/BillyBSig/CIFAR-100-TFDS)
26. What is Fine-Tuning? \- IBM, дата последнего обращения: декабря 21, 2025, [https://www.ibm.com/think/topics/fine-tuning](https://www.ibm.com/think/topics/fine-tuning)
27. Hack your E-commerce Search Accuracy: Introduce OpenAI's CLIP \- LupaSearch, дата последнего обращения: декабря 21, 2025, [https://www.lupasearch.com/blog/hack-your-e-commerce-search-accuracy-introduce-openais-clip/](https://www.lupasearch.com/blog/hack-your-e-commerce-search-accuracy-introduce-openais-clip/)
28. The Ultimate Guide to Visual Search in Ecommerce \- Coveo, дата последнего обращения: декабря 21, 2025, [https://www.coveo.com/blog/visual-search-ecommerce/](https://www.coveo.com/blog/visual-search-ecommerce/)
29. Guide on How to Fine-Tune Existing AI-Models for Fashion Industry \- Levi9, дата последнего обращения: декабря 21, 2025, [https://www.levi9.com/whitepaper/guide-on-how-to-fine-tune-existing-model-for-fashion-industry/](https://www.levi9.com/whitepaper/guide-on-how-to-fine-tune-existing-model-for-fashion-industry/)
30. How we Improve Product Similarity Search with Fashion CLIP over Traditional CLIP, дата последнего обращения: декабря 21, 2025, [https://www.width.ai/post/product-similarity-search-with-fashion-clip](https://www.width.ai/post/product-similarity-search-with-fashion-clip)
31. What Is Vector Search? Everything You Need to Know \- InterSystems, дата последнего обращения: декабря 21, 2025, [https://www.intersystems.com/resources/what-is-vector-search-everything-you-need-to-know/](https://www.intersystems.com/resources/what-is-vector-search-everything-you-need-to-know/)
32. The Future of DAM: AI-Powered Search \- Canto, дата последнего обращения: декабря 21, 2025, [https://www.canto.com/blog/ai-powered-search/](https://www.canto.com/blog/ai-powered-search/)
33. Reverse Image Search System for E-Commerce Using CLIP, Qdrant and Gradio, дата последнего обращения: декабря 21, 2025, [https://www.superteams.ai/blog/reverse-image-search-system-for-e-commerce-using-clip-qdrant-and-gradio](https://www.superteams.ai/blog/reverse-image-search-system-for-e-commerce-using-clip-qdrant-and-gradio)
