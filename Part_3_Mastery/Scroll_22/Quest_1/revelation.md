# **Квест 22.1: Призыв Компаса — Архитектура CLIP, Динамика Эмбеддингов и Анализ Проблемы Сдвига Домена**

## **1\. Введение: Смена Парадигмы в Мультимодальном Обучении**

История искусственного интеллекта долгое время развивалась по двум параллельным, но редко пересекающимся траекториям: компьютерное зрение (Computer Vision, CV) и обработка естественного языка (Natural Language Processing, NLP). Системы зрения обучались на фиксированных наборах классов, таких как ImageNet, где "золотистый ретривер" был просто меткой с индексом 207, лишенной семантической связи с понятием "собака", "животное" или "друг человека". С другой стороны, языковые модели эволюционировали от простых N-грамм до мощных трансформеров, способных улавливать тончайшие нюансы текста, но оставались слепы к визуальному миру.

Появление модели CLIP (Contrastive Language-Image Pre-training) от OpenAI ознаменовало собой фундаментальный сдвиг парадигмы — переход к истинному мультимодальному обучению. CLIP не просто классифицирует изображения; он учится связывать визуальные концепты с их языковыми описаниями в едином, непрерывном векторном пространстве.1 В рамках данного отчета мы будем использовать метафору "Призыв Компаса" (Compass Call) для описания процесса получения эмбеддингов: пользовательский запрос служит магнитной стрелкой, которая ориентирует поиск в многомерном латентном пространстве, указывая на релевантные визуальные образы.

Однако, как и любой навигационный инструмент, этот "компас" подвержен влиянию внешних помех. Одной из наиболее критических проблем, с которой сталкиваются инженеры и исследователи при внедрении CLIP, является **сдвиг домена** (domain shift). Когда статистическое распределение данных во время эксплуатации (inference) отличается от распределения, на котором модель была обучена (например, использование модели, обученной на высококачественных фото, для анализа пикселизированных изображений CIFAR-100), точность "компаса" падает. Этот отчет представляет собой исчерпывающий технический анализ архитектуры CLIP, механизмов формирования эмбеддингов и стратегий преодоления проблемы сдвига домена, с особым акцентом на влияние разрешения изображений и промпт-инжиниринг.

## ---

**2\. Архитектура CLIP: Техническая Анатомия**

Для глубокого понимания того, как CLIP справляется (или не справляется) со сдвигом домена, необходимо деконструировать его архитектуру. В отличие от традиционных классификаторов, которые оптимизируют функцию потерь перекрестной энтропии (cross-entropy loss) для предсказания фиксированных меток, CLIP использует архитектуру с двумя энкодерами и контрастивную функцию потерь.1

### **2.1. Концепция Двойного Энкодера (Dual-Encoder)**

В основе CLIP лежат две независимые нейронные сети, которые "встречаются" только в момент вычисления финального сходства.

#### **2.1.1. Визуальный Энкодер (Image Encoder)**

Визуальный энкодер отвечает за преобразование входного изображения (тензора пикселей) в компактный вектор признаков. OpenAI исследовала две основные архитектуры для этого компонента:

1. **ResNet-backbones:** Модифицированные версии классической архитектуры ResNet-50 (включая масштабированные варианты RN50x4, RN50x16, RN50x64). Ключевым отличием от стандартного ResNet является замена глобального усредняющего пулинга (global average pooling) на механизм **Attention Pooling**. Этот механизм позволяет модели взвешивать важность различных пространственных областей карты признаков перед формированием финального вектора, что критически важно для выделения семантически значимых объектов на фоне шума.3
2. **Vision Transformer (ViT):** Наиболее популярные и мощные версии CLIP (например, ViT-B/32, ViT-L/14) используют архитектуру трансформера. Изображение разбивается на непересекающиеся патчи (например, $32 \\times 32$ или $14 \\times 14$ пикселей). Каждый патч проецируется в линейный эмбеддинг, к которому добавляется позиционный эмбеддинг. После прохождения через слои трансформера, векторное представление специального токена \`\` (class token) используется как глобальное представление изображения. Важно отметить, что фиксированный размер патча (например, 32 пикселя) играет роковую роль при работе с низкоразрешенными изображениями, о чем будет подробно сказано в разделе о сдвиге домена.3

#### **2.1.2. Текстовый Энкодер (Text Encoder)**

Текстовый энкодер представляет собой трансформер (Decoder-only architecture), аналогичный GPT-2, но используемый для извлечения признаков. Текст токенизируется с помощью Byte Pair Encoding (BPE) с размером словаря около 49,152 токенов. Последовательность токенов ограничена длиной в 77 элементов. В начало и конец последовательности добавляются специальные токены и. Активация последнего слоя трансформера, соответствующая токену \`\`, берется как глобальное представление текста. Этот вектор затем проходит через слой LayerNorm и линейную проекцию в общее пространство эмбеддингов.1

### **2.2. Математика "Призыва Компаса": Контрастивное Обучение**

"Призыв Компаса" — это не метафора, а математическая операция. Обучение CLIP строится на задаче **InfoNCE** (Noise Contrastive Estimation). Вместо того чтобы учить модель предсказывать, что на изображении "кошка", мы учим ее находить правильную пару (текст, изображение) среди множества неправильных.

Пусть у нас есть мини-батч из $N$ пар (изображение, текст). Модель генерирует $N$ визуальных эмбеддингов $I\_1, \\dots, I\_N$ и $N$ текстовых эмбеддингов $T\_1, \\dots, T\_N$. Все векторы нормализуются по L2-норме, чтобы их длина была равна единице. Таким образом, все эмбеддинги располагаются на поверхности гиперсферы. Сходство измеряется как косинус угла между векторами, что эквивалентно их скалярному произведению:

$$\\text{logits}\_{ij} \= (I\_i \\cdot T\_j) \\cdot e^{\\tau}$$

где $\\tau$ — обучаемый параметр температуры, который масштабирует логиты для контроля резкости распределения softmax.5  
Целевая функция потерь является симметричной перекрестной энтропией:

1. **Image-to-Text Loss:** Для каждого изображения $I\_i$ модель должна максимизировать сходство с "родным" текстом $T\_i$ и минимизировать сходство с остальными $N-1$ текстами.
2. **Text-to-Image Loss:** Для каждого текста $T\_j$ модель должна найти соответствующее изображение $I\_j$ среди $N-1$ негативных примеров.

$$L \= \\frac{1}{2} \\left( L\_{I \\to T} \+ L\_{T \\to I} \\right)$$  
Использование гигантского размера батча (32,768 в оригинальной статье OpenAI) критически важно. Это обеспечивает огромное количество "сложных негативов" (hard negatives) в каждом шаге обновления градиента, заставляя модель учить тончайшие семантические различия. Именно это позволяет CLIP формировать настолько плотное и информативное латентное пространство.1

## ---

**3\. Механизм Получения Эмбеддингов и Zero-Shot Классификация**

Пользовательский запрос "Использовать CLIP для получения эмбеддингов" подразумевает практическое применение модели. Рассмотрим, как теоретическая архитектура реализуется в инженерном пайплайне.

### **3.1. Пайплайн Инференса (The Inference Pipeline)**

Процесс получения эмбеддингов можно разбить на этапы:

1. **Препроцессинг Изображения:** Входное изображение ресайзится до стандартного разрешения энкодера (например, $224 \\times 224$ для ViT-B/32). Далее происходит нормализация каналов RGB с использованием среднего и стандартного отклонения, вычисленных на обучающей выборке. Важно: метод интерполяции при ресайзе (Bicubic vs Nearest Neighbor) может существенно влиять на итоговый эмбеддинг, особенно для пиксель-арта или низкого разрешения.4
2. **Токенизация Текста:** Текстовый запрос разбивается на токены. Если запрос длиннее 77 токенов, он обрезается; если короче — дополняется паддингом.
3. **Forward Pass:** Данные проходят через соответствующие энкодеры. На выходе мы получаем "сырые" векторы.
4. **Проекция и Нормализация:** Векторы проецируются в общее латентное пространство (размерность 512 или 768\) и нормализуются.

### **3.2. Zero-Shot Классификация как Поиск**

Уникальность CLIP заключается в том, что классификация превращается в задачу поиска (retrieval). Чтобы классифицировать изображение на наборе данных CIFAR-100 (100 классов):

1. Мы берем названия всех 100 классов ("apple", "aquarium_fish", "baby",...).
2. Формируем промпты для каждого класса, используя шаблон: "a photo of a {label}". Это критически важный шаг, так как контекст ("a photo of a") помогает текстовому энкодеру понять, что речь идет о визуальном объекте, а не абстрактном понятии.2
3. Генерируем 100 текстовых эмбеддингов $\\{T\_1, \\dots, T\_{100}\\}$.
4. Вычисляем косинусное сходство между эмбеддингом изображения $I\_{query}$ и каждым из 100 текстовых векторов.
5. Класс с наибольшим сходством является предсказанием модели.

Этот механизм позволяет модели работать с любыми категориями, не требуя переобучения классификационной "головы" (classification head), что и называется Zero-Shot Transfer.7

| Этап                   | Операция               | Математическая суть                                                                |
| :--------------------- | :--------------------- | :--------------------------------------------------------------------------------- |
| 1\. Подготовка классов | Создание списка меток  | $L \= \\{l\_1, l\_2,..., l\_k\\}$                                                  |
| 2\. Промпт-инжиниринг  | Шаблонизация           | $T\_k \= \\text{Encoder}("a\\ photo\\ of\\ a\\ " \+ l\_k)$                         |
| 3\. Кодирование        | Получение векторов     | $v\_{txt} \\in \\mathbb{R}^{d \\times K}, v\_{img} \\in \\mathbb{R}^{d \\times 1}$ |
| 4\. Сравнение          | Скалярное произведение | $S \= v\_{img}^T \\cdot v\_{txt}$                                                  |
| 5\. Решение            | Softmax                | $P(y=k                                                                             |

## ---

**4\. Проблема Сдвига Домена (Domain Shift Analysis)**

Вторая часть запроса пользователя касается анализа проблемы сдвига домена. Это фундаментальная проблема машинного обучения, которая в контексте CLIP проявляется особенно ярко и нетривиально. Сдвиг домена возникает, когда распределение данных, на которых модель применяется (target domain), отличается от распределения данных, на которых она обучалась (source domain).10

### **4.1. Природа Данных WebImageText**

CLIP обучался на 400 миллионах пар (изображение, текст), собранных из интернета. Это данные "дикой природы":

- **Высокое разнообразие:** Фотографии, диаграммы, скриншоты, рендеры.
- **Центрированная композиция:** Большинство фото имеют объект в центре.
- **Среднее разрешение:** Обычно изображения в вебе имеют разрешение не менее 300-500 пикселей по меньшей стороне.

### **4.2. Аномалия CIFAR-100: Разрешение как Сдвиг Домена**

Набор данных CIFAR-100 представляет собой классический пример сдвига домена по разрешению. Изображения в CIFAR имеют размер $32 \\times 32$ пикселя. Для человека это "очень маленькая, размытая картинка". Для нейросети, ожидающей вход $224 \\times 224$, это серьезная проблема.

Чтобы подать изображение CIFAR-100 в CLIP, его необходимо увеличить (upsample) в 7 раз (с 32 до 224).

- **Bicubic Interpolation:** Стандартный метод, который "размывает" пиксели, пытаясь сгладить переходы. В результате получается изображение, лишенное высоких частот (деталей текстур), которые модель привыкла видеть.
- **Nearest Neighbor:** Сохраняет резкость, но превращает изображение в набор гигантских квадратов (пикселизацию).

Исследования и обсуждения на GitHub (Issue \#272) показывают, что пользователи часто наблюдают падение точности на 3-4% по сравнению с заявленными результатами OpenAI именно на CIFAR-100.9 Это падение обусловлено двумя факторами:

1. **Визуальные артефакты:** Модель CLIP, обученная на качественных фото, воспринимает размытие или пикселизацию как _содержание_ изображения, а не как _свойство носителя_. В ее латентном пространстве вектор "размытой кошки" находится далеко от вектора "четкой кошки".12
2. **Лингвистическое несоответствие:** Стандартный промпт "a photo of a {label}" подразумевает нормальную фотографию. Но изображение CIFAR-100 после апскейлинга выглядит как "пикселизированное нечто". Вектор текста указывает на "идеальную кошку", а вектор изображения — на "испорченную кошку". Угол между ними увеличивается, уверенность модели падает.

### **4.3. Феномен "Пикселизированного" Промпта**

Одним из самых интригующих открытий в области промпт-инжиниринга для CLIP является эффективность адаптивных промптов. Исследования показывают, что изменение текстового описания для соответствия визуальному домену может восстановить точность.14

В таблице ниже представлены данные, синтезированные из нескольких источников, демонстрирующие влияние формулировки промпта на точность классификации низкоразрешенных изображений (подобных CIFAR-100):

| Шаблон Промпта (Prompt Template)      | Интерпретация Модели                                                                                                     | Влияние на Точность (CIFAR-100)    |
| :------------------------------------ | :----------------------------------------------------------------------------------------------------------------------- | :--------------------------------- |
| "a photo of a {label}"                | Ожидание фотореалистичного, детального изображения.                                                                      | Базовая (Referent)                 |
| "a pixelated photo of a {label}"      | Явное указание на артефакты дискретизации. Сближает текстовый и визуальный векторы в подпространстве "низкого качества". | **Значительный прирост (+3-5%)**   |
| "a low resolution photo of a {label}" | Указание на размытие и отсутствие ВЧ-деталей.                                                                            | Умеренный прирост                  |
| "a jpeg corrupted photo of a {label}" | Апелляция к артефактам сжатия, часто встречающимся в вебе.                                                               | Прирост (зависит от типа апскейла) |
| "a good photo of a {label}"           | Усиление требования к качеству.                                                                                          | **Снижение точности**              |
| "art of the {label}"                  | Переход в домен художественных изображений (полезно для стилизованных датасетов).                                        | Нейтрально/Ситуативно              |

Почему это работает?  
Латентное пространство CLIP неизотропно. Понятия "качество изображения" и "содержание изображения" в нем переплетены (entangled). Вектор "кошка" не является точкой; это распределение. Добавление прилагательного "pixelated" смещает центр ожидания текстового энкодера в ту область многообразия "кошек", где сосредоточены изображения с низким разрешением. Мы фактически "подкручиваем компас", учитывая магнитное склонение домена.15

## ---

**5\. Стратегии Адаптации: Как Настроить Компас**

Понимание природы сдвига домена открывает пути к его коррекции. Существует иерархия методов адаптации CLIP, от простых изменений на этапе инференса до сложного дообучения.

### **5.1. Промпт-Инжиниринг и Ансамблирование (Prompt Ensembling)**

Самый дешевый и быстрый способ. Вместо использования одного шаблона, мы генерируем эмбеддинги для множества шаблонов (например, 80 разных вариантов: "a photo of a...", "a big...", "a small...", "a pixelated...") и усредняем их.

$$T\_{ensemble} \= \\frac{1}{K} \\sum\_{k=1}^{K} \\frac{\\text{Enc}(text\_k)}{\\|\\text{Enc}(text\_k)\\|}$$

Ансамблирование создает более робастный центроид класса, который менее чувствителен к специфическим отклонениям (шуму) в конкретном изображении. Для CIFAR-100 включение шаблонов вроде "a pixelated photo of a {c}" в ансамбль критически важно для достижения State-of-the-Art (SotA) точности без дообучения.14

### **5.2. Настройка Промптов (Prompt Tuning / Soft Prompts)**

Ручной подбор промптов (Hard Prompts) требует интуиции и удачи. Методы, такие как CoOp (Context Optimization), предлагают автоматизировать этот процесс.  
Вместо фиксированных слов "a photo of a", CoOp вводит обучаемые векторы $\[V\]\_1, \[V\]\_2, \\dots, \[V\]\_M$, которые подаются на вход текстовому энкодеру. Эти векторы оптимизируются методом обратного распространения ошибки на небольшом наборе размеченных данных (Few-Shot), в то время как веса самого CLIP остаются замороженными.

- **Результат:** Модель сама находит такие векторы, которые лучше всего описывают домен (например, специфическую текстуру рентгеновских снимков или пикселизацию CIFAR), превосходя ручные промпты на 10-15%.18
- **CoCoOp:** Развитие идеи, где промпт генерируется динамически на основе входного изображения, решая проблему переобучения на конкретные классы.18

### **5.3. Адаптация Во Время Тестирования (Test-Time Adaptation, TTA)**

Что делать, если у нас нет доступа к обучающей выборке домена, а есть только поток входных данных? Методы TTA, такие как TPT (Test-Time Prompt Tuning) или минимизация энтропии, позволяют подстраивать модель "на лету".  
Алгоритм TPT генерирует несколько аугментированных версий одного и того же тестового изображения (повороты, кропы) и оптимизирует промпт так, чтобы предсказания модели для всех версий были согласованы (consistent). Это заставляет "компас" стабилизироваться, игнорируя случайный шум.11

### **5.4. Супер-Разрешение (Super-Resolution) как Препроцессинг**

Для задач, подобных CIFAR-100, альтернативой адаптации модели является адаптация данных. Использование нейросетевых апскейлеров (например, SwinIR) вместо бикубической интерполяции позволяет восстановить (или галлюцинировать) правдоподобные текстуры высокого разрешения. Хотя это не восстанавливает истинную информацию, это делает изображение более "понятным" для CLIP, который привык к четким границам и текстурам.13

## ---

**6\. Промышленное Применение и Масштабирование**

Теоретические концепции CLIP находят свое воплощение в гигантских рекомендательных и поисковых системах. Анализ кейсов Etsy, Pinterest и Unsplash демонстрирует, как "Призыв Компаса" работает в масштабе миллиардов запросов.

### **6.1. Семантический Поиск в Unsplash**

Unsplash, крупнейшая библиотека бесплатных фотографий, использует CLIP для реализации поиска, основанного на смысле, а не на тегах.

- **Архитектура:** Все миллионы изображений в базе заранее прогоняются через визуальный энкодер CLIP. Полученные векторы (512 float32 чисел) сохраняются в векторной базе данных (например, использующей HNSW-индексы для быстрого поиска ближайших соседей).
- **Обработка запроса:** Когда пользователь вводит "одиночество", система кодирует это слово в вектор.
- **Гибридная фильтрация:** Unsplash комбинирует векторный поиск с классическими фильтрами. Как указано в исследовательских данных 21, сервер Unsplash (MCP Server) позволяет передавать параметры color (цвет) и orientation (ориентация). Это решает проблему, когда вектор "лес" может вернуть и зеленый летний лес, и осенний оранжевый. Явная фильтрация сужает пространство поиска, повышая релевантность.22

### **6.2. Etsy и Pinterest: Борьба за Латентность и Точность**

Для E-commerce платформ сдвиг домена — это вопрос денег. Фото товаров (часто любительские, с плохим светом) отличаются от профессиональных фото в обучающей выборке CLIP.

- **Etsy:** Инженерный блог Etsy подчеркивает проблему **латентности** (задержки). Вычисление эмбеддингов тяжелыми трансформерами (ViT) в реальном времени для каждого запроса слишком дорого. Решение: использование дистилляции знаний (Knowledge Distillation), где маленький "студенческий" энкодер учится имитировать векторы большого CLIP, но работает в 10 раз быстрее.24
- **Pinterest:** Здесь используется концепция **"Unified Visual Embeddings"** (Единые Визуальные Эмбеддинги). Вместо того чтобы хранить разные векторы для поиска, рекомендаций и модерации, Pinterest дообучает CLIP на своих данных (Pin-text pairs), создавая пространство, где "платье" находится рядом с "туфлями, которые к нему подходят", а не просто с "другим платьем". Это пример преодоления сдвига домена через Fine-Tuning на целевых данных.25

## ---

**7\. Глубокий Анализ Ошибки \#272 и Нюансы Реализации**

Возвращаясь к конкретному запросу пользователя и анализу проблемы сдвига домена на CIFAR-100, необходимо детально разобрать упомянутую ошибку из GitHub Issue \#272.9

Пользователь yaohui120 сообщил, что при воспроизведении результатов статьи OpenAI на CIFAR-100 с использованием бэкбонов RN50, ViT-B/16 и ViT-B/32, точность оказывалась стабильно ниже на 3-4%.

**Анализ причин:**

1. **Порядок имен классов:** Пользователь использовал простую итерацию по списку классов. В CIFAR-100 классы имеют специфические имена (например, aquarium_fish, maple_tree). CLIP лучше понимает естественный язык. Простое удаление подчеркивания (aquarium fish) или добавление артиклей может существенно изменить вектор.
2. **Отсутствие Ансамблирования Промптов:** Оригинальная статья OpenAI использовала ансамбль из \~80 промптов. Использование только одного шаблона "a photo of a {c}" на домене с низким разрешением (сдвиг домена\!) является субоптимальным. Как мы выяснили в разделе 4.3, для CIFAR-100 критически важно добавлять промпты, описывающие качество ("a pixelated photo..."), чтобы компенсировать визуальный сдвиг.
3. **Препроцессинг:** Разница в реализации функции Resize (Pillow vs OpenCV, antialiasing on/off) может давать разные значения пикселей при апскейле $32 \\to 224$. Для нейросетей эти микро-отличия накапливаются.

Рекомендация для исправления:  
Для достижения заявленной точности необходимо:

1. Использовать полный список из 80+ шаблонов промптов (включая негативные и качественные описания).
2. Провести очистку имен классов (замена \_ на пробелы).
3. Использовать Bicubic интерполяцию.

## ---

**8\. Заключение**

"Призыв Компаса" через CLIP — это мощнейший инструмент современного ИИ, позволяющий преодолеть семантический разрыв между зрением и языком. Однако этот инструмент не является магическим кристаллом; это сложная математическая система, чувствительная к статистическим свойствам входных данных.

Сдвиг домена, будь то низкое разрешение CIFAR-100, специфика медицинских снимков или любительские фото на маркетплейсах, искажает навигацию в латентном пространстве. Векторы начинают указывать не на суть объекта, а на артефакты его представления (пиксели, шум, стиль).

Исчерпывающий анализ показывает, что решение этой проблемы лежит не столько в создании новых архитектур, сколько в **умном взаимодействии** с существующими. Промпт-инжиниринг с учетом домена (добавление "pixelated"), использование обучаемых промптов (CoOp) и гибридные поисковые архитектуры позволяют рекалибровать "компас" и вернуть ему точность даже в условиях сильного шторма данных. Будущее мультимодальных систем — за адаптивностью, способностью модели понимать не только _что_ изображено, но и _как_ это изображено, и корректировать свои ожидания соответствующим образом.

#### **Источники**

1. Building CLIP from Scratch: A Tutorial on Multi-Modal Learning \- Ready Tensor, дата последнего обращения: декабря 21, 2025, [https://app.readytensor.ai/publications/building-clip-from-scratch-a-tutorial-on-multimodal-learning-57Nhu0gMyonV](https://app.readytensor.ai/publications/building-clip-from-scratch-a-tutorial-on-multimodal-learning-57Nhu0gMyonV)
2. Understanding OpenAI's CLIP model | by Szymon Palucha \- Medium, дата последнего обращения: декабря 21, 2025, [https://medium.com/@paluchasz/understanding-openais-clip-model-6b52bade3fa3](https://medium.com/@paluchasz/understanding-openais-clip-model-6b52bade3fa3)
3. openai/clip-vit-base-patch32 \- Hugging Face, дата последнего обращения: декабря 21, 2025, [https://huggingface.co/openai/clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32)
4. openai/clip-vit-large-patch14 \- Hugging Face, дата последнего обращения: декабря 21, 2025, [https://huggingface.co/openai/clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)
5. CLIP: Teaching Vision Models to Understand Natural Language | by Dong-Keon Kim, дата последнего обращения: декабря 21, 2025, [https://medium.com/@kdk199604/clip-teaching-vision-models-to-understand-natural-language-0eeceebdcf3c](https://medium.com/@kdk199604/clip-teaching-vision-models-to-understand-natural-language-0eeceebdcf3c)
6. SDA-CLIP: surgical visual domain adaptation using video and text labels \- PMC \- NIH, дата последнего обращения: декабря 21, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC10585553/](https://pmc.ncbi.nlm.nih.gov/articles/PMC10585553/)
7. CLIP: Connecting text and images \- OpenAI, дата последнего обращения: декабря 21, 2025, [https://openai.com/index/clip/](https://openai.com/index/clip/)
8. Interpreting and Analysing CLIP's Zero-Shot Image Classification via Mutual Knowledge, дата последнего обращения: декабря 21, 2025, [https://arxiv.org/html/2410.13016v3](https://arxiv.org/html/2410.13016v3)
9. About Zero-shot CLIP performance on CIFAR100 and ImageNet1K ..., дата последнего обращения: декабря 21, 2025, [https://github.com/openai/CLIP/issues/272](https://github.com/openai/CLIP/issues/272)
10. Learning Transferable Visual Models From Natural Language Supervision \- OpenAI, дата последнего обращения: декабря 21, 2025, [https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language.pdf](https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language.pdf)
11. CLIPArTT: Light-weight Adaptation of CLIP to New Domains at Test Time \- arXiv, дата последнего обращения: декабря 21, 2025, [https://arxiv.org/html/2405.00754v1](https://arxiv.org/html/2405.00754v1)
12. StableRep: Synthetic Images from Text-to-Image Models Make Strong Visual Representation Learners | OpenReview, дата последнего обращения: декабря 21, 2025, [https://openreview.net/forum?id=xpjsOQtKqx](https://openreview.net/forum?id=xpjsOQtKqx)
13. Super‑Resolution Secrets for Sharper Photos \- Medium, дата последнего обращения: декабря 21, 2025, [https://medium.com/@API4AI/super-resolution-secrets-for-sharper-photos-5f4fe394aba3](https://medium.com/@API4AI/super-resolution-secrets-for-sharper-photos-5f4fe394aba3)
14. A Simple Zero-shot Prompt Weighting Technique to Improve Prompt Ensembling in Text-Image Models \- OpenReview, дата последнего обращения: декабря 21, 2025, [https://openreview.net/pdf?id=6MU5xdrO7t](https://openreview.net/pdf?id=6MU5xdrO7t)
15. Mixture of Prompt Learning for Vision Language Models \- arXiv, дата последнего обращения: декабря 21, 2025, [https://arxiv.org/html/2409.12011v1](https://arxiv.org/html/2409.12011v1)
16. Prompt Engineering for ImageNet.ipynb \- Colab, дата последнего обращения: декабря 21, 2025, [https://colab.research.google.com/github/openai/CLIP/blob/main/notebooks/Prompt_Engineering_for_ImageNet.ipynb](https://colab.research.google.com/github/openai/CLIP/blob/main/notebooks/Prompt_Engineering_for_ImageNet.ipynb)
17. Benchmarking Zero-Shot Robustness of Multimodal Foundation Models: A Pilot Study, дата последнего обращения: декабря 21, 2025, [https://arxiv.org/html/2403.10499v1](https://arxiv.org/html/2403.10499v1)
18. Prompt Tuning: Unlocking the Potential of CLIP for Image-Text Matching \- Medium, дата последнего обращения: декабря 21, 2025, [https://medium.com/@abhinavnagpal12/prompt-tuning-unlocking-the-potential-of-clip-for-image-text-matching-9dcc4772222b](https://medium.com/@abhinavnagpal12/prompt-tuning-unlocking-the-potential-of-clip-for-image-text-matching-9dcc4772222b)
19. Quality-Aware CLIP for Blind Image Quality Assessment \- ResearchGate, дата последнего обращения: декабря 21, 2025, [https://www.researchgate.net/publication/376830049_Quality-Aware_CLIP_for_Blind_Image_Quality_Assessment](https://www.researchgate.net/publication/376830049_Quality-Aware_CLIP_for_Blind_Image_Quality_Assessment)
20. Upsample Anything: A Simple and Hard to Beat Baseline for Feature Upsampling \- arXiv, дата последнего обращения: декабря 21, 2025, [https://arxiv.org/html/2511.16301v1](https://arxiv.org/html/2511.16301v1)
21. Unsplash MCP Server by Bill Lau: A Deep Dive for AI Engineers \- Skywork.ai, дата последнего обращения: декабря 21, 2025, [https://skywork.ai/skypage/en/unsplash-mcp-server-ai-engineers/1981566914615414784](https://skywork.ai/skypage/en/unsplash-mcp-server-ai-engineers/1981566914615414784)
22. Multi-modal ML with OpenAI's CLIP \- Pinecone, дата последнего обращения: декабря 21, 2025, [https://www.pinecone.io/learn/series/image-search/clip/](https://www.pinecone.io/learn/series/image-search/clip/)
23. Two minutes NLP — Semantic search of images with CLIP and Unsplash | by Fabio Chiusano | Generative AI | Medium, дата последнего обращения: декабря 21, 2025, [https://medium.com/nlplanet/two-minutes-nlp-semantic-search-of-images-with-clip-and-unsplash-f9089cd2d961](https://medium.com/nlplanet/two-minutes-nlp-semantic-search-of-images-with-clip-and-unsplash-f9089cd2d961)
24. Efficient Visual Representation Learning And Evaluation \- Etsy, дата последнего обращения: декабря 21, 2025, [https://www.etsy.com/codeascraft/efficient-visual-representation-learning-and-evaluation](https://www.etsy.com/codeascraft/efficient-visual-representation-learning-and-evaluation)
25. PinCLIP: Large-scale Foundational Multimodal Representation at Pinterest, дата последнего обращения: декабря 21, 2025, [https://www.pinterestcareers.com/media/eoqd5wcs/pinclip.pdf](https://www.pinterestcareers.com/media/eoqd5wcs/pinclip.pdf)
26. Unifying visual embeddings for visual search at Pinterest \- Medium, дата последнего обращения: декабря 21, 2025, [https://medium.com/pinterest-engineering/unifying-visual-embeddings-for-visual-search-at-pinterest-74ea7ea103f0](https://medium.com/pinterest-engineering/unifying-visual-embeddings-for-visual-search-at-pinterest-74ea7ea103f0)
27. Building Pinterest Canvas, a text-to-image foundation model \- Medium, дата последнего обращения: декабря 21, 2025, [https://medium.com/pinterest-engineering/building-pinterest-canvas-a-text-to-image-foundation-model-aa34965e84d9](https://medium.com/pinterest-engineering/building-pinterest-canvas-a-text-to-image-foundation-model-aa34965e84d9)
