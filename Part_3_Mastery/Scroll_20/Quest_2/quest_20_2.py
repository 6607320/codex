# Великий Кодекс Техномагии
# Свиток 20: Предсказатель Будущего
# Квест 20.2: Создание Оракула

# --- Часть I: Импорт Магических Гримуаров ---

# Призываем 'matplotlib.pyplot' (plt) - нашего верного "магического
# художника" для визуализации данных.
import matplotlib.pyplot as plt

# Призываем гримуар 'numpy' (np) - наш магический калькулятор для работы с
# числовыми массивами.
import numpy as np

# Призываем гримуар 'pandas' (pd) - для работы с "летописями времени" в табличной форме.
import pandas as pd

# Призываем 'torch' - основу всей нашей техномагии, наш Источник Маны.
import torch

# Призываем 'torch.nn' (nn) - главу из гримуара 'torch', хранящую чертежи
# всех строительных блоков для големов.
import torch.nn as nn

# Призываем 'MinMaxScaler' - духа-нормализатора, который "сжимает" числа в
# заданный диапазон.
from sklearn.preprocessing import MinMaxScaler

# --- Часть II: Подготовка к Ритуалу ---

# Определяем устройство ('cuda' - Кристалл Маны GPU, 'cpu' - Разум CPU),
# где будет вершиться магия.
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
# Оповещаем о выбранном устройстве.
print(f"Магия будет вершиться на устройстве: {DEVICE}")


# --- Функция 1: Создание "Учебных Карточек" для Оракула ---
# Эта функция-ритуал превращает плоскую летопись в набор "карточек" для обучения.
def create_sequences(data, seq_length):
    # Создаем пустой "мешок" (список) для хранения последовательностей прошлого.
    sequences = []
    # Создаем пустой "мешок" (список) для хранения соответствующих значений будущего.
    labels = []
    # Запускаем цикл, который будет двигать "магическое окно" по нашей летописи.
    for i in range(len(data) - seq_length):
        # Вырезаем из летописи кусок прошлого (нашу последовательность).
        seq = data[i: i + seq_length]
        # Берем ровно одно значение, следующее сразу за этим куском.
        label = data[i + seq_length]
        # Кладем последовательность прошлого в один "мешок".
        sequences.append(seq)
        # Кладем значение будущего в другой "мешок".
        labels.append(label)
    # Превращаем наши "мешки" в удобные для вычислений массивы numpy и возвращаем их.
    return np.array(sequences), np.array(labels)


# --- Часть III: Чертеж Голема-Оракула ---
# Мы начинаем чертеж нашего Голема-Оракула, наследуя его от базового
# чертежа всех големов 'nn.Module'.
class OracleLSTM(nn.Module):
    # В ритуале сотворения (`__init__`) мы описываем слои, из которых будет
    # состоять наш голем.
    def __init__(self, input_size=1, hidden_layer_size=100, output_size=1):
        # 'super().__init__()' - обязательное заклинание, призывающее дух предка для корректной инициализации.
        super().__init__()
        # 'self.hidden_layer_size' - задаем размер "разума" голема, сколько информации он хранит в своей памяти.
        self.hidden_layer_size = hidden_layer_size

        # 'self.lstm' - создаем главный компонент, LSTM-слой, который будет обрабатывать последовательности.
        self.lstm = nn.LSTM(input_size, hidden_layer_size)

        # 'self.linear' - создаем простой линейный слой, который превратит "мысли" LSTM в финальное предсказание.
        self.linear = nn.Linear(hidden_layer_size, output_size)

        # Создаем пустой "сосуд" для Краткосрочной Памяти (hidden state).
        h0 = torch.zeros(1, 1, self.hidden_layer_size)
        # Создаем пустой "сосуд" для Долгосрочной Памяти (cell state).
        c0 = torch.zeros(1, 1, self.hidden_layer_size)
        # 'self.hidden_cell' - создаем атрибут-тайник, хранящий пару "сосудов памяти" в виде "магической связки" (tuple).
        self.hidden_cell = (h0, c0)

    # В ритуале "прямого прохода" (`forward`) мы описываем, как данные
    # проходят через слои голема.
    def forward(self, input_seq):
        # Извлекаем сосуд с Краткосрочной Памятью и отправляем на магический алтарь.
        h0 = self.hidden_cell[0].to(input_seq.device)
        # Извлекаем сосуд с Долгосрочной Памятью и отправляем на магический алтарь.
        c0 = self.hidden_cell[1].to(input_seq.device)
        # Собираем их обратно в "магическую связку" уже на нужном устройстве.
        current_hidden_cell = (h0, c0)

        # Пропускаем последовательность через LSTM. 'view' меняет форму тензора для совместимости.
        # LSTM возвращает нам обработанную последовательность и свое новое
        # состояние памяти.
        lstm_out, self.hidden_cell = self.lstm(
            input_seq.view(len(input_seq), 1, -1), current_hidden_cell
        )

        # Берем вывод только от *последнего* элемента последовательности и
        # отправляем в линейный слой.
        predictions = self.linear(lstm_out.view(len(input_seq), -1)[-1])
        # Возвращаем финальное предсказание.
        return predictions


# --- Часть IV: Главный Ритуал Наставления и Экзамена ---
# Эта конструкция ('if __name__ == "__main__":') - священное начало любого пергамента.
if __name__ == "__main__":
    # --- Акт 1: Загрузка и подготовка Летописи ---
    # Оповещение о начале.
    print("Загружаем 'Летопись Времени' из прошлого квеста...")
    # Приказываем Хранителю Летописей 'pandas' прочитать наш CSV-свиток и
    # создать таблицу.
    df = pd.read_csv("time_series_data.csv")
    # Извлекаем только колонку 'value', превращая ее в массив numpy для вычислений.
    all_data = df["value"].values.astype(float)

    # Разделяем Летопись на "учебник" (train_data) и "экзамен" (test_data).
    train_data = all_data[:-65]
    # Последние 65 дней - для проверки.
    test_data = all_data[-65:]
    # Оповещаем о разделении.
    print("Летопись разделена на учебную и экзаменационную части.")

    # Призываем духа-нормализатора. 'feature_range' говорит "сжимай все числа
    # в диапазон от -1 до 1".
    scaler = MinMaxScaler(feature_range=(-1, 1))
    # Приказываем духу "изучить и преобразовать" наши учебные данные.
    # '.reshape(-1, 1)' - техническая руна для совместимости.
    train_data_normalized = scaler.fit_transform(train_data.reshape(-1, 1))

    # --- Акт 2: Создание Учебных Пособий ---
    # Длина "взгляда в прошлое" - сколько дней мы показываем Оракулу за раз.
    seq_length = 10

    # Вызываем наш ритуал для создания "учебных карточек" из нормализованных данных.
    X_train, y_train = create_sequences(train_data_normalized, seq_length)

    # Превращаем наши numpy массивы в тензоры PyTorch, понятные Источнику
    # Маны. '.float()' задает тип данных.
    X_train = torch.from_numpy(X_train).float()
    # То же самое делаем для меток (правильных ответов).
    y_train = torch.from_numpy(y_train).float()
    # Сообщаем о создании "карточек".
    print("Созданы 'учебные карточки' для Оракула.")

    # --- Акт 3: Призыв и Наставление Оракула ---
    # Призываем нашего Голема-Оракула по чертежу и отправляем на магический
    # алтарь (`.to(DEVICE)`).
    model = OracleLSTM().to(DEVICE)
    # Выбираем "меру ошибки" (Среднеквадратичная ошибка), чтобы оценивать
    # промахи голема.
    loss_function = nn.MSELoss()
    # Выбираем "наставника" (алгоритм Adam), который будет корректировать
    # "разум" Оракула.
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    # Задаем количество эпох - сколько раз мы покажем Оракулу весь "учебник".
    epochs = 150
    # Оповещаем о начале ритуала наставления.
    print(f"\nНачинаем ритуал наставления Оракула на {epochs} эпох...")

    # Начинаем Великий Цикл Наставления, который повторится 'epochs' раз.
    for i in range(epochs):
        # Внутренний цикл: проходим по каждой "учебной карточке".
        # 'zip' - дух, который позволяет идти по двум "мешкам" (X_train, y_train) одновременно.
        for seq, labels in zip(X_train, y_train):
            # Отправляем текущую "карточку" (прошлое и будущее) на магический алтарь.
            seq, labels = seq.to(DEVICE), labels.to(DEVICE)

            # 'optimizer.zero_grad()' - важнейшее заклинание: "Очисти память наставника от прошлых ошибок".
            optimizer.zero_grad()
            # Создаем пустой "сосуд" для Краткосрочной Памяти и отправляем на алтарь.
            h0 = torch.zeros(1, 1, model.hidden_layer_size).to(DEVICE)
            # Создаем пустой "сосуд" для Долгосрочной Памяти и отправляем на алтарь.
            c0 = torch.zeros(1, 1, model.hidden_layer_size).to(DEVICE)
            # Обнуляем память самого Голема перед тем, как показать ему новую
            # "карточку".
            model.hidden_cell = (h0, c0)

            # 'y_pred = model(seq)' - Оракул смотрит на прошлое ('seq') и делает предсказание.
            y_pred = model(seq)

            # 'single_loss = ...' - Сравниваем предсказание ('y_pred') с истинным будущим ('labels').
            single_loss = loss_function(y_pred, labels)
            # 'single_loss.backward()' - Вычисляем, в какую сторону нужно исправить "разум" голема.
            single_loss.backward()
            # 'optimizer.step()' - Наставник делает шаг, исправляя "разум" (веса) голема.
            optimizer.step()

        # Каждые 25 эпох мы выводим отчет о прогрессе.
        if (i + 1) % 25 == 0:
            # Выводим номер эпохи и текущее значение ошибки. ':.8f' - форматирование
            # до 8 знаков после запятой.
            print(f"Эпоха {i+1:3}/{epochs} | Ошибка: {single_loss.item():.8f}")

    # Сообщаем о завершении наставления.
    print("...Наставление завершено.")

    # --- Акт 4: Экзамен для Оракула ---
    # Оповещаем о начале экзамена.
    print("\nНачинаем экзамен...")
    # Берем последние 'seq_length' дней из УЧЕБНЫХ данных как "затравку" для
    # первого предсказания.
    test_inputs = train_data_normalized[-seq_length:].tolist()

    # 'model.eval()' - переводим модель в режим "экзамена" (отключаем случайности вроде dropout).
    model.eval()

    # Запускаем цикл предсказаний на столько шагов, сколько у нас дней в
    # "экзаменационной" части.
    for i in range(len(test_data)):
        # Берем последнюю известную нам последовательность и превращаем ее в тензор.
        seq = torch.FloatTensor(test_inputs[-seq_length:]).to(DEVICE)

        # 'with torch.no_grad():' - создаем "поле тишины", где мы не считаем ошибки, только предсказываем.
        with torch.no_grad():
            # Создаем пустой "сосуд" для Краткосрочной Памяти (h_n) на нужном
            # устройстве.
            h_n = torch.zeros(1, 1, model.hidden_layer_size).to(DEVICE)
            # Создаем пустой "сосуд" для Долгосрочной Памяти (c_n) на нужном устройстве.
            c_n = torch.zeros(1, 1, model.hidden_layer_size).to(DEVICE)
            # **ИСПРАВЛЕНИЕ ОШИБКИ**: Присваиваем "чистую" память голему перед каждым предсказанием.
            model.hidden_cell = (h_n, c_n)
            # Оракул делает предсказание следующего дня. '.item()' извлекает число из
            # тензора.
            next_pred = model(seq).item()
            # Добавляем предсказание в конец нашей истории, чтобы использовать его для
            # следующего шага.
            test_inputs.append([next_pred])

    # --- Акт 5: Оценка и Визуализация ---
    # Приказываем духу-нормализатору 'разжать' наши предсказанные данные
    # обратно в понятный человеку масштаб.
    actual_predictions = scaler.inverse_transform(
        np.array(test_inputs[seq_length:]).reshape(-1, 1)
    )

    # Берем ось времени из нашей изначальной таблицы.
    x = df["date"]

    # 'plt.figure(...)' - создаем "холст" для нашего художника.
    plt.figure(figsize=(14, 7))
    # 'plt.title(...)' - добавляем заголовок нашему рисунку.
    plt.title("Проверка Оракула: Предсказание vs Реальность")
    # 'plt.ylabel(...)' - подписываем ось Y.
    plt.ylabel("Значение")
    # 'plt.xlabel(...)' - подписываем ось X.
    plt.xlabel("Дата")
    # 'plt.grid(True)' - добавляем сетку на фон для лучшего восприятия.
    plt.grid(True)
    # 'plt.plot(...)' - приказываем художнику нарисовать всю историю синей линией.
    plt.plot(x, all_data, label="Полная летопись")
    # 'plt.plot(...)' - приказываем нарисовать предсказания оранжевой пунктирной линией.
    plt.plot(
        x[-len(actual_predictions):],
        actual_predictions,
        label="Предсказания Оракула",
        linestyle="--",
    )
    # 'plt.axvline(...)' - приказываем нарисовать вертикальную красную линию в точке начала экзамена.
    plt.axvline(x=x.iloc[-65], c="r", linestyle="--", label="Начало Экзамена")
    # 'plt.legend()' - приказываем отобразить "легенду" - подписи для каждой линии.
    plt.legend()

    # Задаем имя для нашего финального артефакта.
    plot_filename = "oracle_prediction.png"
    # Приказываем художнику 'plt' сохранить получившийся рисунок в файл.
    plt.savefig(plot_filename)
    # Сообщаем об успешном сохранении графика.
    print(f"\nРезультаты экзамена сохранены в артефакт: {plot_filename}")
