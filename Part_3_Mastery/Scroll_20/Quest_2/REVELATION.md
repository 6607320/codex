# **Проект «Оракул»: Архитектурные парадигмы и ограничения рекуррентных нейронных сетей в задачах экстраполяции временных рядов**

## **1\. Введение: Эволюция прогностических моделей и концепция «Голема»**

В современной науке о данных задача прогнозирования временных рядов занимает центральное место, находя применение в самых разных областях — от алгоритмической торговли на фондовых рынках до предиктивного обслуживания сложного промышленного оборудования. Цель данного исследования, инициированного в рамках квеста 20.2 «Создание Оракула», заключается не просто в реализации алгоритма регрессии, а в создании и глубоком анализе архитектуры, которую метафорически можно назвать «Големом». Как и мифическое существо из глины, обладающее огромной силой, но лишенное собственной воли и творческого начала, нейронная сеть класса LSTM (Long Short-Term Memory) представляет собой мощный инструмент для запоминания сложных паттернов, однако она фундаментально ограничена математическими рамками своих функций активации, что делает ее неспособной к истинной экстраполяции трендов за пределы обучающей выборки.

Исторически методы прогнозирования развивались от простых линейных моделей, таких как скользящие средние и экспоненциальное сглаживание, к более сложным статистическим методам, таким как ARIMA (Auto-Regressive Integrated Moving Average). Статистические методы, полагающиеся на строгие предположения о стационарности данных, долгое время оставались золотым стандартом. Однако с ростом объемов данных и сложности выявляемых зависимостей возникла потребность в алгоритмах, способных моделировать нелинейные динамические системы.1 Появление рекуррентных нейронных сетей (RNN) обещало прорыв, предложив архитектуру, сохраняющую «скрытое состояние» или память о предыдущих шагах. Тем не менее, классические RNN столкнулись с проблемой затухающего градиента, что ограничивало их способность улавливать долгосрочные зависимости.1

Архитектура LSTM, разработанная Хохрайтером и Шмидхубером, стала ответом на этот вызов. Внедрение механизма гейтинга (gating mechanism) позволило создать своего рода «информационную супермагистраль» для градиентов, обеспечив возможность обучения на последовательностях длиной в тысячи временных шагов.2 В данном отчете мы детально рассмотрим процесс создания такого «Оракула» с использованием библиотеки PyTorch, подготовку данных с помощью scikit-learn и, что наиболее важно, проведем критический анализ его «слепого пятна» — неспособности предсказывать неуграниченный рост или падение (тренд), несмотря на идеальное воспроизведение циклических колебаний (сезонности). Мы также предложим и обоснуем гибридный подход, объединяющий мощь глубокого обучения с надежностью классических линейных моделей для решения этой фундаментальной проблемы.

## **2\. Теоретические основы архитектуры LSTM: Анатомия памяти**

Понимание ограничений нашего «Голема» невозможно без детального разбора его внутреннего устройства. В отличие от стандартных полносвязных сетей (feed-forward), где информация течет только в одном направлении, LSTM обладает рекуррентными связями, позволяющими сохранять информацию во времени.

### **2.1. Проблема долгосрочных зависимостей и решение LSTM**

Классические RNN обучаются методом обратного распространения ошибки во времени (Backpropagation Through Time, BPTT). При этом градиент функции потерь умножается на матрицу весов на каждом временном шаге. Если спектральный радиус этой матрицы меньше единицы, градиент экспоненциально стремится к нулю по мере движения назад во времени. Это явление, известное как проблема затухающего градиента, делает невозможным обучение сети зависимостям, разнесенным далеко во времени.1

LSTM решает эту проблему за счет введения концепции «ячейки памяти» (cell state, $C\_t$) и трех регулирующих ворот (gates). Состояние ячейки проходит сквозь всю цепочку с минимальными линейными взаимодействиями, что позволяет информации сохраняться практически неизменной.

### **2.2. Математическая модель гейтинга**

Каждый блок LSTM управляет потоком информации с помощью функций активации. Это критически важный момент для нашего исследования, так как именно выбор функций активации определяет границы возможностей модели в экстраполяции.

**Внутренняя структура ячейки LSTM:**

| Компонент                                 | Уравнение                                                         | Функция активации                  | Диапазон значений     | Роль в системе                                                                |
| :---------------------------------------- | :---------------------------------------------------------------- | :--------------------------------- | :-------------------- | :---------------------------------------------------------------------------- |
| **Forget Gate (Ворота забывания)**        | $f\_t \= \\sigma(W\_f \\cdot \[h\_{t-1}, x\_t\] \+ b\_f)$         | Сигмоида ($\\sigma$)               | $$                    | Решает, какую часть прошлой памяти удалить. 0 — забыть всё, 1 — помнить всё.2 |
| **Input Gate (Входные ворота)**           | $i\_t \= \\sigma(W\_i \\cdot \[h\_{t-1}, x\_t\] \+ b\_i)$         | Сигмоида ($\\sigma$)               | $$                    | Определяет, какие значения будут обновлены.                                   |
| **Candidate Memory (Кандидат памяти)**    | $\\tilde{C}\_t \= \\tanh(W\_C \\cdot \[h\_{t-1}, x\_t\] \+ b\_C)$ | Гиперболический тангенс ($\\tanh$) | $\[-1, 1\]$           | Создает вектор новых значений-кандидатов для добавления в состояние.          |
| **Cell State Update (Обновление ячейки)** | $C\_t \= f\_t \\cdot C\_{t-1} \+ i\_t \\cdot \\tilde{C}\_t$       | Линейная комбинация                | $(-\\infty, \\infty)$ | Физическое обновление памяти.                                                 |
| **Output Gate (Выходные ворота)**         | $o\_t \= \\sigma(W\_o \\cdot \[h\_{t-1}, x\_t\] \+ b\_o)$         | Сигмоида ($\\sigma$)               | $$                    | Контролирует, какая часть памяти попадет в скрытое состояние.                 |
| **Hidden State (Скрытое состояние)**      | $h\_t \= o\_t \\cdot \\tanh(C\_t)$                                | Гиперболический тангенс ($\\tanh$) | $\[-1, 1\]$           | Финальный выход ячейки на данном шаге.                                        |

Ключевым для понимания задачи квеста является последнее уравнение: $h\_t \= o\_t \\cdot \\tanh(C\_t)$. Выход скрытого состояния $h\_t$, который передается на следующий шаг и используется для финального предсказания, _всегда_ ограничен диапазоном $\[-1, 1\]$ из\-за применения функции $\\tanh$.4 Даже если внутреннее состояние ячейки $C\_t$ растет неограниченно (накапливая тренд), операция $\\tanh(C\_t)$ «сплющит» это значение до 1\. Это математическое ограничение является фундаментом «неспособности экстраполировать тренд», которую мы будем наблюдать в эксперименте.

## **3\. Инженерия данных с Scikit-Learn: Подготовка фундамента для Оракула**

Прежде чем приступить к обучению модели на PyTorch, необходимо тщательно подготовить данные. Сырые временные ряды редко пригодны для прямого скармливания нейронным сетям. Эффективность обучения напрямую зависит от качества предобработки, осуществляемой с помощью инструментов библиотеки scikit-learn.

### **3.1. Генерация синтетических данных: Волна и Тренд**

Для чистоты эксперимента мы создаем синтетический набор данных, который явно содержит два компонента, интересующих нас в рамках квеста: сезонность (волну) и тренд.  
Модель данных может быть описана уравнением:

$$y(t) \= A \\cdot \\sin(\\omega t \+ \\phi) \+ \\alpha t \+ \\epsilon$$

где $A \\cdot \\sin(\\dots)$ — периодическая составляющая, $\\alpha t$ — линейный тренд, а $\\epsilon$ — случайный шум. Именно наличие линейного слагаемого $\\alpha t$ станет камнем преткновения для стандартной LSTM модели.

### **3.2. Нормализация данных (MinMaxScaler)**

Нейронные сети, особенно использующие функции активации с насыщением (сигмоида, тангенс), чрезвычайно чувствительны к масштабу входных данных. Если подать на вход значения порядка сотен или тысяч, нейроны перейдут в зону насыщения, где градиенты близки к нулю, что остановит обучение. Кроме того, большие веса могут привести к нестабильности численных вычислений.6

Инструмент MinMaxScaler из библиотеки scikit-learn является стандартом де\-факто для этой задачи. Он трансформирует данные в заданный диапазон, обычно $$или $\[-1, 1\]$.

$$X\_{scaled} \= \\frac{X \- X\_{min}}{X\_{max} \- X\_{min}} \\cdot (max \- min) \+ min$$

Критическое замечание по утечке данных: При работе с временными рядами категорически запрещено вычислять $X\_{min}$ и $X\_{max}$ на всем наборе данных. Это приведет к «заглядыванию в будущее» (look-ahead bias). Скалер должен быть обучен (fit) только на обучающей выборке (training set), и затем применен (transform) к тестовой выборке.7 Игнорирование этого правила превращает Оракула в шарлатана, который знает будущее не потому, что предсказал его, а потому, что видел его во время нормализации.

### **3.3. Формирование последовательностей (Sliding Window)**

LSTM не воспринимает время как абстрактную концепцию; ей нужны конкретные последовательности векторов признаков. Для задачи прогнозирования «многие-к-одному» (Many-to-One) мы используем метод скользящего окна. Исходный временной ряд преобразуется в набор пар (Вход, Выход):

- **Вход ($X$):** Последовательность значений длиной $L$ (look-back window), например, $\[x\_{t-L}, \\dots, x\_{t-1}\]$.
- **Выход ($Y$):** Целевое значение на следующем шаге, $x\_t$.

В терминах тензоров PyTorch мы должны получить структуру размерности (Batch_Size, Sequence_Length, Input_Size). Если мы используем окно размером 50, каждый образец обучения будет представлять собой матрицу $50 \\times 1$ (для одномерного ряда). Функция подготовки данных итеративно проходит по нормализованному массиву, «вырезая» окна. Этот процесс переводит задачу временного ряда в формат обучения с учителем (supervised learning).8

## **4\. Архитектура Голема на PyTorch: Проектирование модели**

Переходя к реализации на PyTorch, мы конструируем класс модели, наследуемый от nn.Module. Архитектура должна быть достаточно простой, чтобы продемонстрировать базовые принципы, но достаточно мощной для улавливания сезонности.

### **4.1. Слой LSTM**

Центральным элементом является слой nn.LSTM. Его конструктор принимает ключевые параметры:

- input_size: Количество входных признаков. Для нашего одномерного ряда это 1\.
- hidden_size: Размерность скрытого вектора $h$. Это емкость краткосрочной памяти модели. Значение 64 или 128 обычно достаточно для простых задач.
- num_layers: Количество слоев LSTM, уложенных друг на друга. Для задачи квеста достаточно одного или двух слоев.
- batch_first=True: Этот флаг критически важен для удобства. По умолчанию PyTorch ожидает тензоры в формате (Seq, Batch, Feature), но человеку привычнее мыслить в формате (Batch, Seq, Feature). Установка этого флага переключает ожидаемый формат ввода.10

### **4.2. Декодер (Полносвязный слой)**

LSTM возвращает последовательность скрытых состояний. Для получения конкретного числового прогноза нам необходимо преобразовать скрытое состояние последнего временного шага обратно в пространство значений временного ряда. Для этого используется линейный слой nn.Linear, который отображает вектор размерности hidden_size в скаляр (размерность 1).11

Код метода forward должен явно извлекать выход последнего шага:

Python

\# out shape: (batch, seq_len, hidden_size)  
out, \_ \= self.lstm(x)  
\# Берем только последний временной шаг  
last_step_out \= out\[:, \-1, :\]  
prediction \= self.linear(last_step_out)

Именно здесь происходит магия — и именно здесь проявляется ограничение. Линейный слой, по сути, выполняет аффинное преобразование $y \= Wx \+ b$. Если $x$ (выход LSTM) ограничен диапазоном $\[-1, 1\]$, то и $y$ будет ограничен диапазоном, заданным весами $W$ и смещением $b$, выученными во время обучения. Модель не может выдать значение, выходящее за пределы линейной комбинации своих весов и максимально возможной активации.11

### **4.3. Функция потерь и Оптимизатор**

Для задачи регрессии (предсказание непрерывной величины) стандартной функцией потерь является среднеквадратичная ошибка (MSELoss). Она сильно штрафует большие отклонения, что полезно для точного следования волне. В качестве оптимизатора предпочтителен Adam (Adaptive Moment Estimation), который сочетает в себе идеи накопления импульса и адаптации шага обучения для каждого параметра. Это позволяет быстрее сходиться на сложных ландшафтах функций потерь рекуррентных сетей.12

## **5\. Эксперимент: Феномен «Плоского прогноза» (Flatline)**

После обучения модели на тренировочном наборе данных (например, первые 80% временного ряда) мы переходим к тестированию на отложенной выборке, где данные продолжают расти по тренду. Здесь мы сталкиваемся с тем, что в литературе называют «Extrapolation Paradox» или неспособностью к экстраполяции.

### **5.1. Визуализация провала**

При визуализации результатов эксперимента мы увидим следующую картину:

1. **Зона обучения (Train):** Предсказания модели (красная линия) идеально накладываются на реальные данные (синяя линия), повторяя и микро-колебания синусоиды, и общий наклон тренда.
2. **Зона тестирования (Test/Future):** Реальные данные продолжают уходить вверх (или вниз) по линейной траектории, сохраняя колебания. Предсказания модели, однако, моментально прекращают рост. Они продолжают осциллировать (повторять волну), но ось этих колебаний становится горизонтальной. Линия предсказаний выпрямляется, превращаясь в «плоскую» волну, которая навсегда остается на уровне последнего значения из обучающей выборки.13

### **5.2. Анализ причинно-следственных связей**

Почему это происходит? Это не ошибка обучения (underfitting), как можно было бы подумать. Это фундаментальное свойство распределения данных.

- **Нарушение гипотезы i.i.d.:** Большинство алгоритмов машинного обучения предполагают, что данные обучения и тестирования являются независимыми и одинаково распределенными (independent and identically distributed). Наличие тренда делает временной ряд нестационарным — его среднее значение и дисперсия меняются со временем. Тестовая выборка лежит в области значений (например, $y \> 100$), которых модель никогда не видела во время обучения (где $y \\in $).15
- **Ограниченность области значений Tanh:** Как обсуждалось в разделе 2, нейроны LSTM выдают значения строго в интервале $\[-1, 1\]$. Линейный слой на выходе учится масштабировать этот интервал в диапазон значений обучающей выборки. Когда на вход поступают данные, требующие выхода за пределы этого диапазона, внутренняя активация LSTM насыщается (достигает 1.0 или \-1.0). Математически невозможно получить на выходе значение больше, чем $W \\cdot 1.0 \+ b$. Сеть физически не может сгенерировать сигнал, превышающий её структурный предел.4

Этот результат подтверждает, что «Голем» — это превосходный интерполятор (он отлично заполняет пробелы внутри известного диапазона), но плохой экстраполятор. Он не «понял» концепцию линейного роста; он просто запомнил, что «после значения X обычно идет значение Y», но у него нет правил для значений, которые он никогда не встречал.

## **6\. Гибридный подход: Симбиоз линейности и нелинейности**

Для решения проблемы тренда необходимо изменить архитектурный подход. Мы не можем заставить LSTM делать то, что противоречит её математической природе. Вместо этого мы должны использовать композицию моделей, где каждый компонент отвечает за свою часть сигнала. Этот подход известен как «Decomposition» или остаточное обучение (Residual Learning).

### **6.1. Методология декомпозиции**

Идея заключается в разделении временного ряда на компоненты: тренд, сезонность и остаток.

$$Y(t) \= Trend(t) \+ Seasonality(t) \+ Noise$$

Для нашего «Гибридного Оракула» мы будем использовать следующую стратегию 18:

1. **Моделирование тренда (Linear Stream):** Мы используем простую модель (например, линейную регрессию из scikit-learn или полиномиальную регрессию), чтобы аппроксимировать общий тренд на обучающей выборке. Линейная регрессия обладает свойством бесконечной экстраполяции: уравнение $y \= ax \+ b$ работает для любого $t$, даже в далеком будущем.
2. Детрендинг (Detrending): Мы вычитаем предсказанный тренд из исходного ряда.

   $$Y\_{residuals} \= Y\_{actual} \- Y\_{trend\\\_pred}$$

   Полученный ряд остатков ($Y\_{residuals}$) становится стационарным (или близким к этому). Он содержит только сезонные колебания и шум, а его среднее значение стабильно. Это идеальная среда для работы LSTM.20

3. **Обучение Голема на остатках:** Мы обучаем LSTM предсказывать именно $Y\_{residuals}$. Поскольку эти значения лежат в ограниченном диапазоне, LSTM легко справляется с их запоминанием и воспроизведением без риска насыщения от тренда.
4. Реконструкция прогноза: Для получения финального предсказания на тестовой выборке мы суммируем выходы двух моделей:

   $$Y\_{final\\\_pred} \= LinearModel(t\_{future}) \+ LSTM(t\_{future})$$

### **6.2. Преимущества гибридной архитектуры**

Этот подход, часто называемый «Boosting» в контексте ансамблей или «Residual Modeling», позволяет каждому алгоритму играть на своем поле. Линейная модель берет на себя «тяжелую атлетику» по предсказанию общего уровня значений, а LSTM фокусируется на тонкой работе по моделированию сложных нелинейных зависимостей и цикличности. Результатом становится прогноз, который не только сохраняет форму волны, но и корректно следует восходящему или нисходящему тренду, что подтверждается метриками (значительное снижение RMSE) и визуальным анализом.19

## **7\. Промышленный контекст и анализ кейсов**

Разработанные в рамках квеста принципы не являются абстрактными упражнениями. Они находят прямое отражение в критически важных промышленных системах, где цена ошибки прогноза исчисляется миллионами долларов.

### **7.1. Предиктивное обслуживание в тяжелой промышленности (Predictive Maintenance)**

В исследовании, посвященном промышленным прессам для бумаги, LSTM использовалась для прогнозирования показаний датчиков (крутящий момент, давление, температура) на 30 дней вперед. Цель — обнаружить аномалии, предвещающие поломку, до того, как она произойдет.23

- **Проблема:** Износ оборудования часто проявляется как медленный линейный дрейф показателей (тренд) на фоне рабочих циклов (сезонность). Если использовать «чистую» LSTM, модель может успешно предсказывать циклы работы пресса, но проигнорировать медленный рост температуры или давления, вызванный износом подшипника, так как он выходит за рамки обучающей выборки. Это приведет к тому, что система мониторинга не выдаст сигнал тревоги вовремя.
- **Решение:** Применение гибридных подходов или тщательная нормализация с ресемплингом данных позволили исследователям достичь высокой точности (RMSE около 1.97 для нормализованных данных) и внедрить стратегию обслуживания по состоянию, а не по расписанию.23

### **7.2. Прогнозирование энергопотребления (Energy Demand Forecasting)**

Сталелитейные заводы потребляют колоссальное количество энергии. Точное прогнозирование нагрузки необходимо для оптимизации закупок электричества и планирования производства.

- **Кейс:** Исследование на базе данных южнокорейской сталелитейной компании DAEWOO показало, что двуслойная LSTM превосходит традиционные методы (ARIMA) в предсказании энергопотребления.24
- **Роль гибридизации:** Однако потребление энергии зависит не только от производственных циклов, но и от макроэкономических факторов и долгосрочных климатических изменений (трендов). Для долгосрочного планирования (на годы вперед) использование чистых LSTM рискованно. Гибридные модели, объединяющие LSTM с методами интерпретируемого ИИ (XAI) или статистическими моделями, позволяют учитывать как суточные пики потребления, так и годовой рост производства.22

### **7.3. Ритейл и управление цепочками поставок (Retail Demand Forecasting)**

Компании масштаба Walmart и Amazon оперируют миллионами товарных позиций (SKU).

- **Walmart:** Использует платформу машинного обучения Element для прогнозирования спроса на уровне каждого магазина. Здесь сезонность (праздники, выходные) играет огромную роль, с чем отлично справляется LSTM. Однако вывод нового популярного товара (вирусный тренд) создает экспоненциальный рост продаж. Чистая LSTM, обученная на спокойных продажах прошлого года, «сплющит» прогноз, что приведет к дефициту товара (stock-out) и упущенной прибыли.26
- **Amazon:** Сервис Amazon Forecast использует алгоритмы DeepAR, которые являются развитием идеи RNN, но включают вероятностные компоненты для обработки неопределенности и трендов. Они также активно используют гибридизацию, подавая в модель не только исторические продажи, но и внешние факторы (промо-акции, цены), которые помогают модели понять причину сдвига тренда.28

## **8\. Детальная реализация: Код и Логика**

Ниже приведена концептуальная структура кода для реализации «Гибридного Оракула», объединяющая все рассмотренные выше принципы.

### **8.1. Подготовка данных с декомпозицией**

Python

import numpy as np  
from sklearn.linear_model import LinearRegression  
from sklearn.preprocessing import MinMaxScaler

\# 1\. Генерация данных (Волна \+ Тренд)  
t \= np.arange(1000).reshape(-1, 1)  
y_trend \= 0.05 \* t  
y_season \= 10 \* np.sin(0.1 \* t)  
y_raw \= y_trend \+ y_season \+ np.random.normal(0, 1, (1000, 1))

\# 2\. Обучение линейной модели на тренировочной части (первые 800 точек)  
split \= 800  
lin_reg \= LinearRegression()  
lin_reg.fit(t\[:split\], y_raw\[:split\])

\# 3\. Детрендинг: Получение остатков  
trend_pred \= lin_reg.predict(t) \# Предсказываем тренд для ВСЕГО ряда  
y_detrended \= y_raw \- trend_pred \# Остатки содержат только волну

\# 4\. Масштабирование остатков (критично для LSTM)  
scaler \= MinMaxScaler(feature_range=(-1, 1))  
\# Fit только на train\!  
scaler.fit(y_detrended\[:split\])  
y_scaled \= scaler.transform(y_detrended)

### **8.2. Создание датасета (Sliding Window)**

Python

def create_sequences(data, seq_length):  
 xs, ys \=,  
 for i in range(len(data) \- seq_length):  
 x \= data\[i:(i \+ seq_length)\]  
 y \= data\[i \+ seq_length\]  
 xs.append(x)  
 ys.append(y)  
 return np.array(xs), np.array(ys)

\# Преобразование в тензоры PyTorch  
X_train, y_train \= create_sequences(y_scaled\[:split\], seq_length=50)  
X_train \= torch.from_numpy(X_train).float()  
y_train \= torch.from_numpy(y_train).float()

### **8.3. Модель LSTM (Голем)**

Python

import torch  
import torch.nn as nn

class GolemOracle(nn.Module):  
 def \_\_init\_\_(self, input_size=1, hidden_size=64):  
 super(GolemOracle, self).\_\_init\_\_()  
 \# batch_first=True для удобства работы с размерностями (N, L, H)  
 self.lstm \= nn.LSTM(input_size, hidden_size, batch_first=True)  
 self.linear \= nn.Linear(hidden_size, 1)

    def forward(self, x):
        \# x shape: (batch, seq\_len, 1\)
        lstm\_out, \_ \= self.lstm(x)
        \# Нас интересует только последний выход последовательности
        last\_time\_step \= lstm\_out\[:, \-1, :\]
        prediction \= self.linear(last\_time\_step)
        return prediction

### **8.4. Цикл обучения и Гибридный прогноз**

После стандартного цикла обучения (forward pass \-\> loss \-\> backward \-\> optimizer step), мы выполняем прогноз:

1. LSTM предсказывает масштабированный остаток (волну) для тестового периода.
2. Применяем scaler.inverse_transform для возврата к реальному масштабу остатков.
3. Линейная регрессия предсказывает тренд для тестового периода (экстраполяция).
4. Суммируем: Final_Prediction \= Inverse_LSTM_Pred \+ Linear_Trend_Pred.

Этот алгоритм гарантирует, что итоговый график будет не «плоской линией», а корректным продолжением синусоиды вдоль восходящей линии тренда.

## **9\. Заключение: Границы искусственного предвидения**

Эксперимент по созданию «Оракула» приводит нас к важному эпистемологическому выводу относительно природы современных нейронных сетей. LSTM, несмотря на свою сложность и способность хранить долгосрочные зависимости, остается по своей сути интерполятором. Она виртуозно моделирует структуру данных внутри того многообразия (manifold), на котором обучалась. Однако, когда задача требует выхода за пределы этого многообразия — как в случае с экстраполяцией линейного тренда в бесконечность — архитектурные ограничения функций активации ($\\tanh$) становятся непреодолимым барьером.

Феномен «плоского прогноза» — это не ошибка, а свойство системы, стремящейся к стабильности в пределах известных ей границ. Решение этой проблемы лежит не в увеличении количества слоев или эпох обучения, а в грамотном архитектурном дизайне. Гибридный подход, сочетающий детерминированную жесткость линейных моделей (для трендов) и адаптивную гибкость нейросетей (для сезонности и нелинейных паттернов), представляет собой наиболее надежную парадигму для построения промышленных систем прогнозирования.

В мире, где данные редко бывают стационарными, истинный «Оракул» — это не монолитный «Голем», а сложный ансамбль, способный различать вечное повторение циклов и неумолимое движение времени вперед. Будущее развитие таких систем лежит в области архитектур Трансформеров и механизмов внимания, однако понимание фундаментальных ограничений рекуррентности, продемонстрированных в этом отчете, остается обязательным условием для любого специалиста в области анализа временных рядов.

#### **Источники**

1. What is LSTM \- Long Short Term Memory? \- GeeksforGeeks, дата последнего обращения: декабря 21, 2025, [https://www.geeksforgeeks.org/deep-learning/deep-learning-introduction-to-long-short-term-memory/](https://www.geeksforgeeks.org/deep-learning/deep-learning-introduction-to-long-short-term-memory/)
2. Long short-term memory \- Wikipedia, дата последнего обращения: декабря 21, 2025, [https://en.wikipedia.org/wiki/Long_short-term_memory](https://en.wikipedia.org/wiki/Long_short-term_memory)
3. Harnessing the Power of LSTM Networks for Accurate Time Series Forecasting \- Medium, дата последнего обращения: декабря 21, 2025, [https://medium.com/@silva.f.francis/harnessing-the-power-of-lstm-networks-for-accurate-time-series-forecasting-c3589f9e0494](https://medium.com/@silva.f.francis/harnessing-the-power-of-lstm-networks-for-accurate-time-series-forecasting-c3589f9e0494)
4. LSTM RNN: An In-depth Look at its Architecture | by Rezowanur Rahman Robin | Medium, дата последнего обращения: декабря 21, 2025, [https://medium.com/@robin5002234/lstm-rnn-an-in-depth-look-at-its-architecture-749c1d26abcf](https://medium.com/@robin5002234/lstm-rnn-an-in-depth-look-at-its-architecture-749c1d26abcf)
5. What is the intuition of using tanh in LSTM? \[closed\] \- Stack Overflow, дата последнего обращения: декабря 21, 2025, [https://stackoverflow.com/questions/40761185/what-is-the-intuition-of-using-tanh-in-lstm](https://stackoverflow.com/questions/40761185/what-is-the-intuition-of-using-tanh-in-lstm)
6. Time Series Prediction with LSTM Recurrent Neural Networks in Python with Keras \- MachineLearningMastery.com, дата последнего обращения: декабря 21, 2025, [https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/](https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/)
7. PyTorch: LSTM Networks for Time-Series Data (Regression Tasks) \- CoderzColumn, дата последнего обращения: декабря 21, 2025, [https://coderzcolumn.com/tutorials/artificial-intelligence/pytorch-lstm-networks-for-time-series-regression-tasks](https://coderzcolumn.com/tutorials/artificial-intelligence/pytorch-lstm-networks-for-time-series-regression-tasks)
8. Using PyTorch to Train an LSTM Forecasting Model | by Michael Rowe | Medium, дата последнего обращения: декабря 21, 2025, [https://medium.com/@mike.roweprediger/using-pytorch-to-train-an-lstm-forecasting-model-e5a04b6e0e67](https://medium.com/@mike.roweprediger/using-pytorch-to-train-an-lstm-forecasting-model-e5a04b6e0e67)
9. Hybrid Machine Learning Model with Python \- Aman Kharwal, дата последнего обращения: декабря 21, 2025, [https://amanxai.com/2024/11/04/hybrid-machine-learning-model-with-python/](https://amanxai.com/2024/11/04/hybrid-machine-learning-model-with-python/)
10. How to use PyTorch LSTMs for time series regression \- Brian Patrick Kent, дата последнего обращения: декабря 21, 2025, [https://www.crosstab.io/articles/time-series-pytorch-lstm/](https://www.crosstab.io/articles/time-series-pytorch-lstm/)
11. LSTM for Time Series Prediction in PyTorch \- MachineLearningMastery.com, дата последнего обращения: декабря 21, 2025, [https://machinelearningmastery.com/lstm-for-time-series-prediction-in-pytorch/](https://machinelearningmastery.com/lstm-for-time-series-prediction-in-pytorch/)
12. Why are predictions from my LSTM Neural Network lagging behind true values?, дата последнего обращения: декабря 21, 2025, [https://datascience.stackexchange.com/questions/76826/why-are-predictions-from-my-lstm-neural-network-lagging-behind-true-values](https://datascience.stackexchange.com/questions/76826/why-are-predictions-from-my-lstm-neural-network-lagging-behind-true-values)
13. LSTM in Python generating flat forecasts \- Stack Overflow, дата последнего обращения: декабря 21, 2025, [https://stackoverflow.com/questions/56022318/lstm-in-python-generating-flat-forecasts](https://stackoverflow.com/questions/56022318/lstm-in-python-generating-flat-forecasts)
14. Time series prediction, LSTM outputs a flat line \- PyTorch Forums, дата последнего обращения: декабря 21, 2025, [https://discuss.pytorch.org/t/time-series-prediction-lstm-outputs-a-flat-line/193938](https://discuss.pytorch.org/t/time-series-prediction-lstm-outputs-a-flat-line/193938)
15. machine learning \- With an LSTM, with training samples on 0-\>250 ..., дата последнего обращения: декабря 21, 2025, [https://stats.stackexchange.com/questions/268027/with-an-lstm-with-training-samples-on-0-250-should-it-be-able-to-extrapolate](https://stats.stackexchange.com/questions/268027/with-an-lstm-with-training-samples-on-0-250-should-it-be-able-to-extrapolate)
16. I knew NNs don't extrapolate well, but I'm kinda stunned it's that bad : r/learnmachinelearning \- Reddit, дата последнего обращения: декабря 21, 2025, [https://www.reddit.com/r/learnmachinelearning/comments/ozajiw/i_knew_nns_dont_extrapolate_well_but_im_kinda/](https://www.reddit.com/r/learnmachinelearning/comments/ozajiw/i_knew_nns_dont_extrapolate_well_but_im_kinda/)
17. Time Series: Tips & Tricks for Training LSTMs \- Kaggle, дата последнего обращения: декабря 21, 2025, [https://www.kaggle.com/code/iamleonie/time-series-tips-tricks-for-training-lstms](https://www.kaggle.com/code/iamleonie/time-series-tips-tricks-for-training-lstms)
18. Hybrid Models \- Kaggle, дата последнего обращения: декабря 21, 2025, [https://www.kaggle.com/code/ryanholbrook/hybrid-models](https://www.kaggle.com/code/ryanholbrook/hybrid-models)
19. A Hybrid ARIMA-LSTM-XGBoost Model with Linear Regression Stacking for Transformer Oil Temperature Prediction \- MDPI, дата последнего обращения: декабря 21, 2025, [https://www.mdpi.com/1996-1073/18/6/1432](https://www.mdpi.com/1996-1073/18/6/1432)
20. Preparing and Shaping Timeseries Data for Keras LSTM Input: Part One \- Medium, дата последнего обращения: декабря 21, 2025, [https://medium.com/@john.kosinski/preparing-and-shaping-timeseries-data-for-keras-lstm-input-part-one-5bb882bc2143](https://medium.com/@john.kosinski/preparing-and-shaping-timeseries-data-for-keras-lstm-input-part-one-5bb882bc2143)
21. What is detrending of time series and how we get that detrending data using python?, дата последнего обращения: декабря 21, 2025, [https://www.researchgate.net/post/What_is_detrending_of_time_series_and_how_we_get_that_detrending_data_using_python](https://www.researchgate.net/post/What_is_detrending_of_time_series_and_how_we_get_that_detrending_data_using_python)
22. A Hybrid Model for Data-Driven Energy Consumption Forecasting in the Steel Industry: An Approach Based on LSTM and Hyperparameter Optimization with the Optuna Algorithm, дата последнего обращения: декабря 21, 2025, [https://abmir.yazd.ac.ir/article_3955.html?lang=en](https://abmir.yazd.ac.ir/article_3955.html?lang=en)
23. Anticipating Future Behavior of an Industrial Press Using LSTM ..., дата последнего обращения: декабря 21, 2025, [https://www.mdpi.com/2076-3417/11/13/6101](https://www.mdpi.com/2076-3417/11/13/6101)
24. LSTM-based estimation of energy consumption in energy-intensive facilities \- ResearchGate, дата последнего обращения: декабря 21, 2025, [https://www.researchgate.net/publication/388766371_LSTM-based_estimation_of_energy_consumption_in_energy-intensive_facilities](https://www.researchgate.net/publication/388766371_LSTM-based_estimation_of_energy_consumption_in_energy-intensive_facilities)
25. Energy Usage Forecasting Model Based on Long Short-Term ..., дата последнего обращения: декабря 21, 2025, [https://www.mdpi.com/2078-2489/14/5/265](https://www.mdpi.com/2078-2489/14/5/265)
26. Machine Learning in Retail: Walmart & Target Case Studies 2024-25 \- Articsledge, дата последнего обращения: декабря 21, 2025, [https://www.articsledge.com/post/machine-learning-retail-case-studies](https://www.articsledge.com/post/machine-learning-retail-case-studies)
27. Decking the aisles with data: How Walmart's AI-powered inventory system brightens the holidays, дата последнего обращения: декабря 21, 2025, [https://tech.walmart.com/content/walmart-global-tech/en_us/blog/post/walmarts-ai-powered-inventory-system-brightens-the-holidays.html](https://tech.walmart.com/content/walmart-global-tech/en_us/blog/post/walmarts-ai-powered-inventory-system-brightens-the-holidays.html)
28. Amazon Forecast Documentation, дата последнего обращения: декабря 21, 2025, [https://docs.aws.amazon.com/forecast/](https://docs.aws.amazon.com/forecast/)
29. Addressing the challenges with demand forecasting solutions on AWS, дата последнего обращения: декабря 21, 2025, [https://docs.aws.amazon.com/whitepapers/latest/demand-forecasting/addressing-the-challenges-with-demand-forecasting-solutions-on-aws.html](https://docs.aws.amazon.com/whitepapers/latest/demand-forecasting/addressing-the-challenges-with-demand-forecasting-solutions-on-aws.html)
