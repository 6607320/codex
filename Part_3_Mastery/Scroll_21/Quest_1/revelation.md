# **Глубокое обучение представлений в рекомендательных системах: Архитектурный анализ матричной факторизации и модуля nn.Embedding**

## **Аннотация**

В современном ландшафте машинного обучения рекомендательные системы представляют собой один из наиболее критически важных классов алгоритмов, обеспечивающих персонализацию контента для миллиардов пользователей на платформах от Netflix и Spotify до Alibaba и Uber. Центральным элементом этих систем является концепция **эмбеддинга** (embedding) — плотного векторного представления, которое трансформирует разреженные, высокоразмерные данные о взаимодействиях пользователей и предметов в компактное латентное пространство. В данном отчете проводится исчерпывающий технический анализ методологии создания и обучения эмбеддингов с использованием фреймворка PyTorch, в частности модуля nn.Embedding, и архитектур матричной факторизации.

Мы детально рассматриваем эволюцию методов коллаборативной фильтрации, переходя от эвристических подходов ближайших соседей к оптимизационным задачам факторизации матриц и нейросетевым архитектурам. Особое внимание уделяется инженерным аспектам реализации: эффективности операций поиска (lookup) по сравнению с матричным умножением, стратегиям инициализации весов, нюансам выбора функций потерь (MSE, RMSE, BPR) и оптимизаторов (SGD, Adam) в условиях разреженности данных. Кроме того, отчет исследует теоретические границы размерности эмбеддингов, методы визуализации латентных пространств (t-SNE, PCA) и практические стратегии решения проблемы «холодного старта», применяемые ведущими технологическими гигантами. Работа синтезирует теоретические основы с прикладными инсайтами, предоставляя дорожную карту для построения промышленных рекомендательных систем.

## ---

**1\. Теоретические основы латентных факторных моделей**

### **1.1 Парадигма коллаборативной фильтрации и проблема разреженности**

В основе современных рекомендательных систем лежит гипотеза коллаборативной фильтрации (Collaborative Filtering, CF), которая постулирует, что паттерны предпочтений пользователей в прошлом являются лучшими предсказателями их поведения в будущем. В отличие от контентной фильтрации, которая опирается на явные метаданные (жанр фильма, характеристики товара), CF абстрагируется от природы объектов, работая исключительно с матрицей взаимодействий «пользователь-предмет» (User-Item Interaction Matrix).

Пусть $U$ — множество пользователей, а $I$ — множество предметов. Матрица взаимодействий $R \\in \\mathbb{R}^{|U| \\times |I|}$ содержит известные оценки или факты взаимодействия $r\_{ui}$. Фундаментальная проблема заключается в том, что эта матрица катастрофически разрежена. В сервисах масштаба YouTube или Amazon, где количество пользователей и товаров исчисляется миллионами, один пользователь взаимодействует лишь с ничтожно малой долей каталога. Плотность данных (sparsity) часто превышает 99,9%.1 Задача рекомендательной системы сводится к **заполнению матрицы** (Matrix Completion) — предсказанию отсутствующих значений $r\_{ui}$.

Традиционные методы, такие как k-ближайших соседей (k-NN), сталкиваются с проблемами масштабируемости при росте $|U|$ и $|I|$. Вычисление попарных сходств становится вычислительно неподъемным ($O(|U|^2)$ или $O(|I|^2)$). Именно здесь на сцену выходят методы снижения размерности, или модели латентных факторов.

### **1.2 Математическая формулировка матричной факторизации**

Матричная факторизация (Matrix Factorization, MF) решает проблему разреженности, предполагая, что взаимодействия между пользователями и предметами определяются небольшим числом скрытых (латентных) факторов $K$, где $K \\ll \\min(|U|, |I|)$. Это позволяет аппроксимировать исходную матрицу $R$ произведением двух плотных матриц низкого ранга:

1. **Матрица эмбеддингов пользователей ($P$):** Размерность $|U| \\times K$. Каждая строка $p\_u$ представляет собой вектор интересов пользователя в $K$-мерном пространстве.
2. **Матрица эмбеддингов предметов ($Q$):** Размерность $|I| \\times K$. Каждая строка $q\_i$ описывает характеристики предмета в том же $K$-мерном пространстве.

Аппроксимация выражается формулой:

$$R \\approx P \\times Q^T$$  
Предсказанный рейтинг $\\hat{r}\_{ui}$ вычисляется как скалярное произведение (dot product) векторов пользователя и предмета:

$$\\hat{r}\_{ui} \= p\_u \\cdot q\_i \= \\sum\_{k=1}^{K} p\_{uk} q\_{ik}$$  
Эта операция лежит в основе архитектуры. Скалярное произведение геометрически интерпретируется как мера сходства: чем меньше угол между векторами $p\_u$ и $q\_i$ (и чем больше их модули), тем выше предсказанная оценка.2

#### **Связь с SVD (Singular Value Decomposition)**

Исторически метод восходит к сингулярному разложению (SVD), которое факторизует матрицу $A$ как $U \\Sigma V^T$. Однако классическое SVD не определено для матриц с пропущенными значениями, что делает его неприменимым к реальным данным рекомендательных систем без предварительной (и часто ошибочной) импутации нулями или средними значениями.3 Метод, популяризированный Саймоном Фанком (Simon Funk) во время конкурса Netflix Prize, часто называемый **FunkSVD**, является аппроксимацией, которая обучается только на _наблюдаемых_ парах $(u, i)$, минимизируя ошибку реконструкции с помощью градиентного спуска.3

### **1.3 Семантика латентного пространства**

Размерность $K$ (размер эмбеддинга) является ключевым гиперпараметром, определяющим емкость модели. Если мы выберем $K=50$ для рекомендаций фильмов, модель попытается сжать все многообразие кинематографа и человеческих вкусов в 50 ортогональных осей.

Важно понимать, что эти оси не задаются вручную. В процессе обучения на огромных массивах данных модель самостоятельно «открывает» такие концепции, как «Комедийность», «Напряженность сюжета», «Присутствие известных актеров» или даже более абстрактные понятия, не имеющие четких названий в естественном языке.5

- **Интерпретация значений:** Положительное значение в измерении $k$ может означать наличие характеристики, отрицательное — её отсутствие или противоположность.
- **Взаимодействие:** Если у пользователя $u$ высокое значение в факторе «Sci-Fi» ($+2.5$), а у фильма $i$ также высокое значение в этом факторе ($+3.0$), их произведение ($7.5$) внесет значительный вклад в итоговую оценку, сигнализируя о совпадении вкусов.5

Это свойство латентных факторов позволяет системе выявлять неявные связи. Например, если пользователи, любящие "Звездные войны", часто смотрят фильмы с Харрисоном Фордом, латентный фактор может начать кодировать «присутствие Харрисона Форда», даже если в базе данных нет явной информации об актерском составе.8

## ---

**2\. Архитектура и реализация эмбеддингов в PyTorch**

### **2.1 Инженерная анатомия модуля nn.Embedding**

В экосистеме PyTorch матрицы $P$ и $Q$ реализуются через класс torch.nn.Embedding. С точки зрения нейронной архитектуры, этот слой часто вызывает вопросы: является ли он полносвязным слоем или чем-то иным?

#### **Механизм Lookup Table vs. Матричное умножение**

Математически выбор эмбеддинга для пользователя с индексом $j$ эквивалентен умножению матрицы эмбеддингов $W$ на вектор one-hot кодирования $v$, где $v\_j \= 1$, а остальные элементы равны 0:

$$E(j) \= W^T \\cdot v\_{\\text{one-hot}}$$  
Однако в производственных системах, где количество пользователей ($|U|$) может достигать сотен миллионов (как у Alibaba или YouTube), создание one-hot вектора размерности $10^8$ невозможно из\-за ограничений памяти. Более того, операция матричного умножения была бы вычислительно избыточной, так как она сводилась бы к миллионам умножений на ноль.9

PyTorch реализует nn.Embedding как эффективную **таблицу поиска (lookup table)**.

1. **Вход:** Тензор индексов (целых чисел), например \`\`.
2. **Операция:** Слой напрямую обращается к строкам внутренней матрицы весов weight по указанным индексам, используя их как адреса памяти.
3. **Сложность:** Операция выполняется за $O(1)$ для каждого индекса в батче, что на порядки быстрее плотного матричного умножения.11

#### **Инициализация и параметры**

При создании nn.Embedding(num_embeddings, embedding_dim) создается обучаемый тензор весов. По умолчанию он инициализируется из нормального распределения $\\mathcal{N}(0, 1)$. Однако для задач матричной факторизации такая инициализация может быть слишком агрессивной, приводя к большим начальным значениям скалярного произведения. В практике часто используют равномерную инициализацию с малой амплитудой, например uniform\_(-0.05, 0.05), или инициализацию Ксавье (Xavier), чтобы сбалансировать дисперсию градиентов в начале обучения.13

### **2.2 Эмбеддинги как обучаемые параметры**

Важно подчеркнуть, что эмбеддинги не являются статичными дескрипторами. Это **параметры модели** (requires_grad=True), которые обновляются методом обратного распространения ошибки (backpropagation) точно так же, как веса в сверточных или рекуррентных слоях.

Когда модель делает ошибку в предсказании (например, предсказывает низкий рейтинг фильму, который пользователю понравился), градиент ошибки протекает обратно к скалярному произведению, а затем к конкретным векторам $p\_u$ и $q\_i$, участвовавшим в операции. Градиент «подталкивает» вектор пользователя и вектор предмета ближе друг к другу в $K$-мерном пространстве, корректируя их координаты.10

### **2.3 Работа с гигантскими словарями**

В промышленных масштабах (Alibaba, Google) таблицы эмбеддингов могут не помещаться в память одного GPU.

- **Sparse Gradients (Разреженные градиенты):** PyTorch позволяет включить опцию sparse=True в nn.Embedding. В этом режиме градиенты хранятся не как плотные тензоры (нули для всех пользователей, не попавших в батч), а как разреженные структуры, содержащие обновления только для затронутых индексов. Это критически важно для использования оптимизаторов типа SparseAdam, которые специально разработаны для эффективного обновления гигантских таблиц.11
- **Шардинг (Sharding):** Для моделей с миллиардами параметров (как в системах Alibaba или Facebook DLRM) таблицы эмбеддингов разбиваются на части и хранятся на разных устройствах или серверах параметров. Прямой проход собирает нужные векторы с разных шардов, а обратный проход рассылает градиенты.15

## ---

**3\. Построение архитектуры матричной факторизации**

### **3.1 Базовая модель Dot-Product**

Простейшая реализация рекомендательной системы на PyTorch представляет собой класс, наследующий nn.Module.

Python

class MatrixFactorization(nn.Module):  
 def \_\_init\_\_(self, num_users, num_items, emb_size=100):  
 super(MatrixFactorization, self).\_\_init\_\_()  
 self.user_emb \= nn.Embedding(num_users, emb_size)  
 self.item_emb \= nn.Embedding(num_items, emb_size)  
 \# Инициализация весов для стабильности  
 self.user_emb.weight.data.uniform\_(0, 0.05)  
 self.item_emb.weight.data.uniform\_(0, 0.05)

    def forward(self, u, v):
        u \= self.user\_emb(u) \# (batch\_size, emb\_size)
        v \= self.item\_emb(v) \# (batch\_size, emb\_size)
        \# Скалярное произведение
        return (u \* v).sum(1)

В этой архитектуре векторы перемножаются поэлементно (Hadamard product), а затем суммируются по оси факторов, что эквивалентно скалярному произведению.13

### **3.2 Критическая роль смещений (Bias Terms)**

Базовая модель скалярного произведения страдает от неспособности учитывать систематические отклонения. В реальных данных рейтинги зависят не только от взаимодействия, но и от индивидуальных свойств сущностей.

- **Смещение пользователя ($b\_u$):** Некоторые пользователи склонны ставить высокие оценки всем подряд (оптимисты), другие — занижать (критики). Это свойство не зависит от предмета.
- **Смещение предмета ($b\_i$):** Некоторые фильмы являются общепризнанными шедеврами (например, «Крестный отец») и получают высокие оценки от самых разных людей. Другие фильмы объективно плохи.
- **Глобальное смещение ($\\mu$):** Средний рейтинг по всей базе данных.

Для учета этих факторов модель расширяется:

$$\\hat{r}\_{ui} \= \\mu \+ b\_u \+ b\_i \+ p\_u \\cdot q\_i$$  
В PyTorch смещения реализуются как эмбеддинги размерности 1 (nn.Embedding(num, 1)). Результат squeeze()-ится для приведения размерности. Введение смещений позволяет латентным факторам $p\_u$ и $q\_i$ моделировать именно _специфические_ предпочтения (остаточную дисперсию), а не тратить свою емкость на объяснение того, почему популярный фильм имеет высокий рейтинг.16 Исследования показывают, что добавление смещений значительно снижает ошибку RMSE.3

### **3.3 Нейронная коллаборативная фильтрация (NCF)**

Традиционная матричная факторизация полагается на линейное взаимодействие факторов (скалярное произведение). Некоторые исследователи утверждают, что это ограничивает выразительность модели, не позволяя улавливать сложные нелинейные зависимости. Это привело к разработке архитектуры **Neural Collaborative Filtering (NCF)**.

#### **Архитектура NeuMF (Neural Matrix Factorization)**

NCF предлагает заменить (или дополнить) скалярное произведение многослойным перцептроном (MLP). Наиболее известная архитектура NeuMF объединяет два пути:

1. **GMF (Generalized Matrix Factorization):** Путь, имитирующий классическую факторизацию. Векторы пользователя и предмета перемножаются поэлементно.
2. **MLP (Multi-Layer Perceptron):** Векторы пользователя и предмета конкатенируются и пропускаются через серию полносвязных слоев с функциями активации (обычно ReLU). Это позволяет выучить произвольную нелинейную функцию взаимодействия $f(p\_u, q\_i)$.

Выходы GMF и MLP конкатенируются и подаются на финальный слой (обычно Sigmoid для предсказания вероятности взаимодействия в неявных данных).17

#### **Дискуссия: NCF против MF**

Несмотря на популярность NCF, недавние исследования (в частности, Rendle et al., 2020\) поставили под сомнение безусловное превосходство MLP над скалярным произведением. Было показано, что простая матричная факторизация (dot product), если её правильно настроить и обучить, часто превосходит сложные нейронные архитектуры. Скалярное произведение обладает уникальным свойством сохранения геометрической структуры (метрическое пространство), которое MLP может с трудом аппроксимировать.20 Тем не менее, NCF остается важным инструментом, особенно когда необходимо интегрировать дополнительные признаки (side features) в нелинейном режиме.

## ---

**4\. Динамика обучения и стратегии оптимизации**

### **4.1 Функции потерь: Явный и неявный отклик**

Выбор функции потерь (Loss Function) фундаментально меняет задачу, которую решает модель.

#### **Явный отклик (Explicit Feedback)**

Когда доступны рейтинги (1–5 звезд), задача формулируется как регрессия.

- **MSE (Mean Squared Error):** Стандартный выбор. Сильно штрафует большие отклонения. $\\mathcal{L} \= \\frac{1}{N} \\sum (r\_{ui} \- \\hat{r}\_{ui})^2$.
- **RMSE (Root Mean Squared Error):** Метрика, использовавшаяся в Netflix Prize. Оптимизация MSE эквивалентна оптимизации RMSE, так как корень — монотонная функция.22
- **Huber Loss (Smooth L1):** Компромисс между MSE и MAE. Ведет себя квадратично для малых ошибок и линейно для больших, что делает обучение более устойчивым к выбросам (outliers) в данных.23

#### **Неявный отклик (Implicit Feedback)**

В большинстве современных систем (клики, просмотры, лайки) мы знаем только о факте взаимодействия (1), но не знаем, является ли отсутствие взаимодействия (0) результатом неприязни или неосведомленности.

- **BCE (Binary Cross Entropy):** Рассматривает задачу как бинарную классификацию. Неизвестные взаимодействия часто сэмплируются как негативные примеры (Negative Sampling).
- BPR (Bayesian Personalized Ranking): Функция потерь, специально разработанная для рекомендаций. Вместо того чтобы предсказывать абсолютное число, BPR оптимизирует порядок: она стремится сделать разницу между предсказанием для позитивного примера ($i$) и негативного примера ($j$) максимально большой.

  $$\\mathcal{L}\_{\\text{BPR}} \= \- \\sum\_{(u,i,j) \\in D\_S} \\ln \\sigma(\\hat{r}\_{ui} \- \\hat{r}\_{uj})$$

  Это часто дает лучшие результаты для задач ранжирования (Top-N рекомендаций).24

### **4.2 Оптимизаторы: SGD против Adam**

Оптимизация эмбеддингов имеет специфику: параметры обновляются крайне неравномерно. Популярные предметы встречаются в батчах часто и получают много обновлений градиента; редкие предметы обновляются редко.

#### **Стохастический градиентный спуск (SGD)**

Классический SGD прост и эффективен по памяти, но имеет единый learning rate для всех параметров. Это может привести к тому, что популярные предметы будут «осциллировать» (overshooting), а редкие — обучаться слишком медленно. Тем не менее, в масштабных системах (Yahoo, Netflix) SGD часто использовался из\-за простоты реализации и хорошей обобщающей способности.25

#### **Адаптивные методы (Adam, Adagrad)**

Adam (Adaptive Moment Estimation) вычисляет индивидуальный learning rate для каждого параметра на основе истории градиентов.

- **Преимущество:** Идеально подходит для разреженных данных. Редкие признаки получают большие шаги обновления, когда они наконец встречаются. Сходимость происходит значительно быстрее.
- **Цена:** Требует хранения двух дополнительных векторов моментов ($m\_t, v\_t$) для каждого веса. Для модели с 100 млн параметров это утраивает потребление памяти. Кроме того, существуют свидетельства, что Adam может сходиться к острым минимумам, что ухудшает генерализацию по сравнению с хорошо настроенным SGD.26

**Таблица 1: Сравнительный анализ оптимизаторов для рекомендательных систем**

| Характеристика              | SGD                                            | Adam                               | SparseAdam                              |
| :-------------------------- | :--------------------------------------------- | :--------------------------------- | :-------------------------------------- |
| **Потребление памяти**      | Низкое (только веса)                           | Высокое (веса \+ моменты)          | Оптимизировано для разреженных тензоров |
| **Скорость сходимости**     | Медленная, требует настройки LR                | Быстрая, адаптивная                | Быстрая                                 |
| **Работа с разреженностью** | Слабая (без спец. трюков)                      | Отличная                           | Отличная                                |
| **Риск переобучения**       | Низкий (более плоские минимумы)                | Средний/Высокий                    | Средний                                 |
| **Применение**              | Масштабные системы (с кастомным планировщиком) | Исследования, сложные модели (NCF) | PyTorch nn.Embedding(sparse=True)       |

### **4.3 Регуляризация**

Модели матричной факторизации склонны к переобучению, особенно на разреженных данных. Если у пользователя всего 5 оценок, модель с $K=100$ может идеально запомнить эти 5 точек, но не выучить обобщающие паттерны.

- **L2-регуляризация (Weight Decay):** Добавление штрафа $\\lambda (||p\_u||^2 \+ ||q\_i||^2)$ в функцию потерь предотвращает рост весов до больших значений. Это заставляет модель распределять информацию по многим факторам, а не полагаться на один "сильный" признак.28
- **Dropout:** В NCF применяется dropout (случайное выключение нейронов) в слоях MLP для предотвращения ко-адаптации признаков.29

## ---

**5\. Настройка гиперпараметров и выбор размерности**

### **5.1 Размерность эмбеддинга ($K$)**

Выбор $K$ — это баланс между выразительностью (expressiveness) и вычислительной эффективностью/переобучением.

- **Слишком мало ($K \< 10$):** Модель недообучается (underfitting). Она не может разделить нюансы (например, отличает боевики от комедий, но не различает «интеллектуальные комедии» и «слэпстик»).30
- **Слишком много ($K \> 200$):** Модель переобучается, запоминая шум. Кроме того, растет стоимость хранения и инференса (время поиска ближайших соседей растет линейно с ростом размерности).

#### **Эмпирические правила (Rules of Thumb)**

В индустрии и академии выработались эвристики для выбора начального $K$:

1. **Правило FastAI:** $E \= \\min(600, \\text{round}(1.6 \\times N\_{cat}^{0.56}))$. Эта формула, выведенная Джереми Говардом экспериментально, предлагает нелинейную зависимость от размера словаря $N\_{cat}$.31
2. **Правило корня четвертой степени:** $E \= \\sqrt{N\_{cat}}$. Популярно в уроках по TensorFlow. Например, для 1 млн пользователей это даст $K \\approx 32$.33
3. **Индустриальные стандарты:** Часто используются степени двойки для выравнивания памяти (64, 128, 256). В системе рекомендаций Alibaba используются векторы размерности 64–1024 в зависимости от задачи.34

### **5.2 Автоматическая настройка (AutoML)**

Учитывая чувствительность моделей к $K$ и $\\lambda$, ручной подбор (Grid Search) часто неэффективен. Современный подход — использование байесовской оптимизации (например, библиотека Optuna), которая строит вероятностную модель функции качества и интеллектуально выбирает следующую точку для проверки гиперпараметров, значительно сокращая время поиска.35

## ---

**6\. Визуализация и интерпретируемость латентных пространств**

Одной из самых мощных возможностей эмбеддингов является их способность захватывать семантику. Близкие векторы означают семантическую близость.

### **6.1 Методы снижения размерности**

Поскольку мы не можем видеть в 64-мерном пространстве, используются алгоритмы проекции в 2D или 3D.

#### **PCA (Метод главных компонент)**

Линейный метод, который поворачивает оси так, чтобы максимизировать дисперсию данных.

- **Применение:** Полезен для выявления глобальной структуры. Например, первая главная компонента (PC1) в фильмах часто коррелирует с осью «Популярное — Артхаус» или «Для всей семьи — Для взрослых».38
- **Ограничения:** Плохо сохраняет локальные структуры и кластеры в сложных многообразиях.

#### **t-SNE (t-Distributed Stochastic Neighbor Embedding)**

Нелинейный метод, сохраняющий локальное соседство. Он пытается отобразить точки так, чтобы соседи в исходном пространстве остались соседями в проекции.

- **Применение:** Идеален для выявления кластеров. На карте t-SNE можно увидеть четкие острова: «Фильмы ужасов 80-х», «Диснеевские мультфильмы», «Научная фантастика».41
- **Интерпретация:** Важно помнить, что расстояние _между_ кластерами в t-SNE не всегда имеет смысл, но группировка _внутри_ кластера высоко информативна.

### **6.2 Арифметика латентного пространства**

Эмбеддинги предметов часто обладают композиционными свойствами, аналогичными Word2Vec ($\\text{Король} \- \\text{Мужчина} \+ \\text{Женщина} \\approx \\text{Королева}$).  
В рекомендациях это может выглядеть так:  
$\\text{Embedding(Shrek)} \- \\text{Embedding(Animation)} \+ \\text{Embedding(Human)} \\approx \\text{Embedding(Adventure Comedy)}$  
Пользователь также представляется точкой в этом пространстве. Рекомендация — это поиск k-ближайших предметов (Items) к вектору пользователя (User).43

## ---

**7\. Промышленные кейсы и масштабирование**

Внедрение систем матричной факторизации в продакшн требует решения проблем, выходящих за рамки чистого ML.

### **7.1 Netflix: От конкурса к продакшену**

Конкурс Netflix Prize (2006-2009) стал катализатором развития MF. Команда-победитель BellKor использовала ансамбль из более чем 100 моделей, но основу составляли именно вариации матричной факторизации (SVD++).

- **Временная динамика (TimeSVD++):** Одним из ключевых инсайтов стало то, что вкусы пользователей дрейфуют со временем, а популярность фильмов меняется. Включение времени ($t$) в качестве параметра смещения ($b\_i(t)$) дало критический прирост точности.1
- **Бизнес-ценность:** Несмотря на победу сложного ансамбля, в продакшн пошли более простые и интерпретируемые модели факторизации из\-за ограничений на время инференса.6

### **7.2 Spotify: Item2Vec и музыкальная семантика**

Spotify использует подход, аналогичный Word2Vec, названный **Item2Vec**. Плейлисты пользователей рассматриваются как «предложения», а треки — как «слова». Обучая модель Skip-gram на этих последовательностях, Spotify получает эмбеддинги треков, где близость означает «часто встречаются в одних плейлистах».44 Это позволяет рекомендовать музыку, которая контекстуально подходит, даже если жанры формально различаются.

### **7.3 Alibaba: Графовые эмбеддинги миллиардного масштаба**

Для гиганта электронной коммерции Alibaba характерны миллиарды товаров. Стандартная факторизация там не работает. Они используют методы **Graph Embedding** (на основе случайных блужданий по графу сессий пользователей, алгоритм DeepWalk/Node2Vec).

- **Side Information:** Чтобы бороться с разреженностью (многие товары никто не покупал), в эмбеддинг «подмешивается» информация о бренде, категории и цене (GES — Graph Embedding with Side information). Это позволяет генерировать векторы даже для абсолютно новых товаров.15

### **7.4 Uber Eats и Pinterest: Визуальные и гео-графовые модели**

- **Pinterest (PinSage):** Разработали алгоритм на основе графовых сверточных сетей (GCN). Эмбеддинг пина генерируется не просто по ID, а агрегируя информацию от его соседей по графу и визуальные признаки картинки. Это решает проблему холодного старта и визуального поиска.47
- **Uber Eats:** Использует **Graph Learning** для моделирования сложной связи «Пользователь \-\> Ресторан \-\> Блюдо \-\> Кухня». Эмбеддинги учитывают географические ограничения (пользователь не закажет из ресторана в другом городе), что накладывает жесткие пространственные маски на поиск ближайших соседей.49

## ---

**8\. Продвинутые инженерные вызовы**

### **8.1 Проблема холодного старта (Cold Start)**

Ахиллесова пята матричной факторизации — новые пользователи или предметы. Если для ID нет строки в матрице $W$, скалярное произведение невозможно.  
Стратегии решения:

1. **Популярное (Popularity Baseline):** Новым пользователям показывают топы, пока не накопятся клики.51
2. **Гибридные модели (Content-Based Hybrid):** Эмбеддинг нового фильма инициализируется как среднее арифметическое эмбеддингов его жанра и актеров. Это помещает его в правильную область латентного пространства до получения первых оценок.29
3. **Feature Hashing:** Использование хеширования признаков позволяет отображать новые категории в существующее пространство фиксированной размерности, избегая переобучения словаря.53

### **8.2 Масштабируемость и ANN**

Поиск топ-10 рекомендаций для пользователя путем умножения его вектора на 10 миллионов векторов товаров ($O(|I|)$) занимает секунды, что недопустимо.

- **Приближенный поиск ближайших соседей (ANN):** Используются алгоритмы HNSW (Hierarchical Navigable Small World) или библиотеки типа **Faiss** (от Facebook). Они строят индекс (граф или дерево), позволяющий находить ближайшие векторы за логарифмическое время ($O(\\log |I|)$) с минимальной потерей точности.54 В архитектуре YouTube этот этап называется «Генерация кандидатов».56

## ---

**9\. Метрики оценки качества**

Оценка рекомендательной системы многогранна. Низкая ошибка предсказания рейтинга не всегда коррелирует с удовлетворением пользователя.

**Таблица 2: Иерархия метрик рекомендательных систем**

| Тип метрики               | Примеры                         | Описание                                                                                                           | Применение                                    |
| :------------------------ | :------------------------------ | :----------------------------------------------------------------------------------------------------------------- | :-------------------------------------------- |
| **Точность предсказания** | RMSE, MAE                       | Насколько точно модель угадала оценку (3.5 vs 4.0).                                                                | Netflix Prize, академические бенчмарки.57     |
| **Качество ранжирования** | NDCG, MRR, MAP                  | Важен порядок. Топ-1 должен быть релевантнее Топ-5.                                                                | Поисковая выдача, Топ-N рекомендации.58       |
| **Классификация**         | Precision@K, Recall@K           | Доля релевантных товаров в выдаче длины K.                                                                         | E-commerce, "Вам может понравиться".60        |
| **Поведенческие**         | Serendipity, Novelty, Diversity | Неожиданность и новизна. Рекомендовать "Звездные войны" фанату фантастики — точно, но бесполезно (он и так знает). | Удержание пользователей, открытие каталога.61 |

## ---

**Заключение**

Создание эмбеддингов с использованием nn.Embedding и матричной факторизации — это фундамент, на котором строятся современные рекомендательные системы. Переход от эвристик к обучению плотных представлений позволил алгоритмам «понимать» семантику контента и предпочтений, связывая пользователей и предметы в едином векторном пространстве.

Хотя базовые модели MF остаются мощным инструментом, индустрия движется к гибридным архитектурам (NCF, Graph Neural Networks, Two-Tower Models), которые способны интегрировать контекст, мультимодальные данные и сложные нелинейные паттерны. Успех в построении такой системы зависит не только от выбора архитектуры, но и от скрупулезной инженерной работы: правильной инициализации, выбора функции потерь, настройки оптимизатора под разреженные данные и построения эффективного пайплайна инференса с использованием ANN. Как показывает опыт Netflix, Spotify и Alibaba, именно в деталях реализации скрывается разница между просто работающим алгоритмом и системой, генерирующей миллиардную прибыль.

### ---

**Техническое Приложение: Справочник по реализации в PyTorch**

Ниже приведена сводная таблица ключевых компонентов для реализации обсужденных архитектур.

**Таблица 3: Компоненты PyTorch для коллаборативной фильтрации**

| Компонент                  | Модуль / Функция PyTorch       | Комментарий по применению                                                  |
| :------------------------- | :----------------------------- | :------------------------------------------------------------------------- |
| **Слой эмбеддинга**        | torch.nn.Embedding(num, dim)   | Основной строительный блок. Используйте sparse=True для огромных словарей. |
| **Скалярное произведение** | torch.sum(u \* v, dim=1)       | Базовая операция взаимодействия в MF. Быстрая и эффективная по памяти.     |
| **Смещения (Bias)**        | torch.nn.Embedding(num, 1\)    | Критически важны для снятия глобальных трендов популярности.               |
| **Loss (Регрессия)**       | torch.nn.MSELoss()             | Для явных рейтингов (звезды 1-5).                                          |
| **Loss (Ранжирование)**    | torch.nn.BCEWithLogitsLoss()   | Для неявных данных (0/1). Включает сигмоиду для численной стабильности.    |
| **Оптимизатор**            | torch.optim.SparseAdam()       | Специализированный вариант Adam для разреженных тензоров градиентов.       |
| **Регуляризация**          | Параметр weight_decay в оптим. | Реализует L2-регуляризацию, предотвращая взрыв весов эмбеддингов.          |

Использованные источники:

1

#### **Источники**

1. The Netflix Prize and Singular Value Decomposition | CS-301, дата последнего обращения: декабря 21, 2025, [https://pantelis.github.io/cs301/docs/common/lectures/recommenders/netflix/](https://pantelis.github.io/cs301/docs/common/lectures/recommenders/netflix/)
2. What Is Matrix Factorization? (Definition, Examples) | Built In, дата последнего обращения: декабря 21, 2025, [https://builtin.com/articles/matrix-factorization](https://builtin.com/articles/matrix-factorization)
3. Recommendation System \- Matrix Factorization (SVD) Explained \- Towards Data Science, дата последнего обращения: декабря 21, 2025, [https://towardsdatascience.com/recommendation-system-matrix-factorization-svd-explained-c9a50d93e488/](https://towardsdatascience.com/recommendation-system-matrix-factorization-svd-explained-c9a50d93e488/)
4. Netflix Prize \- Wikipedia, дата последнего обращения: декабря 21, 2025, [https://en.wikipedia.org/wiki/Netflix_Prize](https://en.wikipedia.org/wiki/Netflix_Prize)
5. Introduction to Latent Matrix Factorization Recommender Systems \- Medium, дата последнего обращения: декабря 21, 2025, [https://medium.com/data-science/introduction-to-latent-matrix-factorization-recommender-systems-8dfc63b94875](https://medium.com/data-science/introduction-to-latent-matrix-factorization-recommender-systems-8dfc63b94875)
6. MATRIX FACTORIZATION TECHNIQUES FOR RECOMMENDER SYSTEMS \- DataJobs.com, дата последнего обращения: декабря 21, 2025, [https://datajobs.com/data-science-repo/Recommender-Systems-\[Netflix\].pdf](https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf)
7. Recommender System — Matrix Factorization | by Denise Chen | TDS Archive | Medium, дата последнего обращения: декабря 21, 2025, [https://medium.com/data-science/recommendation-system-matrix-factorization-d61978660b4b](https://medium.com/data-science/recommendation-system-matrix-factorization-d61978660b4b)
8. What are latent factors in matrix factorization? \- Milvus, дата последнего обращения: декабря 21, 2025, [https://milvus.io/ai-quick-reference/what-are-latent-factors-in-matrix-factorization](https://milvus.io/ai-quick-reference/what-are-latent-factors-in-matrix-factorization)
9. How does nn.Embedding work? \- PyTorch Forums, дата последнего обращения: декабря 21, 2025, [https://discuss.pytorch.org/t/how-does-nn-embedding-work/88518](https://discuss.pytorch.org/t/how-does-nn-embedding-work/88518)
10. NN embedding layer \- Data Science Stack Exchange, дата последнего обращения: декабря 21, 2025, [https://datascience.stackexchange.com/questions/32460/nn-embedding-layer](https://datascience.stackexchange.com/questions/32460/nn-embedding-layer)
11. Embedding — PyTorch 2.9 documentation, дата последнего обращения: декабря 21, 2025, [https://docs.pytorch.org/docs/stable/generated/torch.nn.Embedding.html](https://docs.pytorch.org/docs/stable/generated/torch.nn.Embedding.html)
12. What is nn.Embedding really?. In this brief article I will show how… | by Gautam Ethiraj | Medium, дата последнего обращения: декабря 21, 2025, [https://medium.com/@gautam.e/what-is-nn-embedding-really-de038baadd24](https://medium.com/@gautam.e/what-is-nn-embedding-really-de038baadd24)
13. pytorch-tutorials/collaborative-filtering-nn.ipynb at master \- GitHub, дата последнего обращения: декабря 21, 2025, [https://github.com/yanneta/pytorch-tutorials/blob/master/collaborative-filtering-nn.ipynb](https://github.com/yanneta/pytorch-tutorials/blob/master/collaborative-filtering-nn.ipynb)
14. How to learn the embeddings in Pytorch and retrieve it later \- Stack Overflow, дата последнего обращения: декабря 21, 2025, [https://stackoverflow.com/questions/53124809/how-to-learn-the-embeddings-in-pytorch-and-retrieve-it-later](https://stackoverflow.com/questions/53124809/how-to-learn-the-embeddings-in-pytorch-and-retrieve-it-later)
15. Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba, дата последнего обращения: декабря 21, 2025, [https://www.kdd.org/kdd2018/accepted-papers/view/billion-scale-commodity-embedding-for-e-commerce-recommendation-in-alibaba](https://www.kdd.org/kdd2018/accepted-papers/view/billion-scale-commodity-embedding-for-e-commerce-recommendation-in-alibaba)
16. Recommendation Systems • Evaluation Metrics and Loss Functions \- aman.ai, дата последнего обращения: декабря 21, 2025, [https://aman.ai/recsys/metrics/](https://aman.ai/recsys/metrics/)
17. Step by Step Coding Guide to Build a Neural Collaborative Filtering (NCF) Recommendation System with PyTorch \- MarkTechPost, дата последнего обращения: декабря 21, 2025, [https://www.marktechpost.com/2025/04/11/step-by-step-coding-guide-to-build-a-neural-collaborative-filtering-ncf-recommendation-system-with-pytorch/](https://www.marktechpost.com/2025/04/11/step-by-step-coding-guide-to-build-a-neural-collaborative-filtering-ncf-recommendation-system-with-pytorch/)
18. Neural Collaborative Filtering \- GeeksforGeeks, дата последнего обращения: декабря 21, 2025, [https://www.geeksforgeeks.org/deep-learning/neural-collaborative-filtering/](https://www.geeksforgeeks.org/deep-learning/neural-collaborative-filtering/)
19. Recommendation Systems using Neural Collaborative Filtering (NCF) explained with codes | by Mehul Gupta | Data Science in Your Pocket | Medium, дата последнего обращения: декабря 21, 2025, [https://medium.com/data-science-in-your-pocket/recommendation-systems-using-neural-collaborative-filtering-ncf-explained-with-codes-21a97e48a2f7](https://medium.com/data-science-in-your-pocket/recommendation-systems-using-neural-collaborative-filtering-ncf-explained-with-codes-21a97e48a2f7)
20. Neural Collaborative Filtering vs. Matrix Factorization Revisited \- Google Research, дата последнего обращения: декабря 21, 2025, [https://research.google/pubs/neural-collaborative-filtering-vs-matrix-factorization-revisited/](https://research.google/pubs/neural-collaborative-filtering-vs-matrix-factorization-revisited/)
21. Rethinking Neural vs. Matrix-Factorization Collaborative Filtering: the Theoretical Perspectives \- Proceedings of Machine Learning Research, дата последнего обращения: декабря 21, 2025, [http://proceedings.mlr.press/v139/xu21d/xu21d.pdf](http://proceedings.mlr.press/v139/xu21d/xu21d.pdf)
22. RMSE vs MSE loss function \- the optimization solutions are equivalent?, дата последнего обращения: декабря 21, 2025, [https://stats.stackexchange.com/questions/532414/rmse-vs-mse-loss-function-the-optimization-solutions-are-equivalent](https://stats.stackexchange.com/questions/532414/rmse-vs-mse-loss-function-the-optimization-solutions-are-equivalent)
23. What is Loss Function? | IBM, дата последнего обращения: декабря 21, 2025, [https://www.ibm.com/think/topics/loss-function](https://www.ibm.com/think/topics/loss-function)
24. Different Loss Methods for the Recommender Systems | by Mandeep Singh \- Medium, дата последнего обращения: декабря 21, 2025, [https://medium.com/@mandeep0405/different-loss-methods-for-the-recommender-systems-e6a75de414a6](https://medium.com/@mandeep0405/different-loss-methods-for-the-recommender-systems-e6a75de414a6)
25. comparison of SGD and ALS in collaborative filtering \- Cross Validated, дата последнего обращения: декабря 21, 2025, [https://stats.stackexchange.com/questions/201279/comparison-of-sgd-and-als-in-collaborative-filtering](https://stats.stackexchange.com/questions/201279/comparison-of-sgd-and-als-in-collaborative-filtering)
26. Comparing SGD and Adam Optimizers in PyTorch | CodeSignal Learn, дата последнего обращения: декабря 21, 2025, [https://codesignal.com/learn/courses/advanced-neural-tuning/lessons/comparing-sgd-and-adam-optimizers-in-pytorch](https://codesignal.com/learn/courses/advanced-neural-tuning/lessons/comparing-sgd-and-adam-optimizers-in-pytorch)
27. For very large dataset, should I use ADAM optimizer or SGD? : r/deeplearning \- Reddit, дата последнего обращения: декабря 21, 2025, [https://www.reddit.com/r/deeplearning/comments/w4rim7/for_very_large_dataset_should_i_use_adam/](https://www.reddit.com/r/deeplearning/comments/w4rim7/for_very_large_dataset_should_i_use_adam/)
28. Matrix Factorization Techniques. The Netflix Problem: How Does an… | by Himasharandil | Medium, дата последнего обращения: декабря 21, 2025, [https://medium.com/@himasharandil/matrix-factorization-techniques-ab01539c2906](https://medium.com/@himasharandil/matrix-factorization-techniques-ab01539c2906)
29. Collaborative filtering and embeddings in Pytorch | by Edward Tan \- Medium, дата последнего обращения: декабря 21, 2025, [https://medium.com/@eddiewctan/collaborative-filtering-and-embeddings-3d6a49034965](https://medium.com/@eddiewctan/collaborative-filtering-and-embeddings-3d6a49034965)
30. Matrix factorization | Machine Learning \- Google for Developers, дата последнего обращения: декабря 21, 2025, [https://developers.google.com/machine-learning/recommendation/collaborative/matrix](https://developers.google.com/machine-learning/recommendation/collaborative/matrix)
31. Size of embedding for categorical variables \- Part 1 (2019) \- fast.ai Course Forums, дата последнего обращения: декабря 21, 2025, [https://forums.fast.ai/t/size-of-embedding-for-categorical-variables/42608](https://forums.fast.ai/t/size-of-embedding-for-categorical-variables/42608)
32. Good rule of thumb for embedding for categorical data \- Part 1 (2019) \- Fast.ai Forums, дата последнего обращения: декабря 21, 2025, [https://forums.fast.ai/t/good-rule-of-thumb-for-embedding-for-categorical-data/37464](https://forums.fast.ai/t/good-rule-of-thumb-for-embedding-for-categorical-data/37464)
33. Embedding Layer Size Rule \- Advanced (Part 1 v3) \- Fast.ai Forums, дата последнего обращения: декабря 21, 2025, [https://forums.fast.ai/t/embedding-layer-size-rule/50691](https://forums.fast.ai/t/embedding-layer-size-rule/50691)
34. Alibaba Cloud Model Studio:Embedding, дата последнего обращения: декабря 21, 2025, [https://www.alibabacloud.com/help/en/model-studio/embedding](https://www.alibabacloud.com/help/en/model-studio/embedding)
35. How do you perform hyperparameter tuning for recommender system models? \- Milvus, дата последнего обращения: декабря 21, 2025, [https://milvus.io/ai-quick-reference/how-do-you-perform-hyperparameter-tuning-for-recommender-system-models](https://milvus.io/ai-quick-reference/how-do-you-perform-hyperparameter-tuning-for-recommender-system-models)
36. Embedding in Recommender Systems: A Survey \- arXiv, дата последнего обращения: декабря 21, 2025, [https://arxiv.org/html/2310.18608v2](https://arxiv.org/html/2310.18608v2)
37. Towards Fair and Rigorous Evaluations: Hyperparameter Optimization for Top-N Recommendation Task with Implicit Feedback \- arXiv, дата последнего обращения: декабря 21, 2025, [https://arxiv.org/html/2408.07630v1](https://arxiv.org/html/2408.07630v1)
38. Understanding matrix factorization for recommendation (part 1\) \- preliminary insights on PCA | Nicolas Hug, дата последнего обращения: декабря 21, 2025, [https://nicolas-hug.com/blog/matrix_facto_1](https://nicolas-hug.com/blog/matrix_facto_1)
39. Introduction to t-SNE: Nonlinear Dimensionality Reduction and Data Visualization, дата последнего обращения: декабря 21, 2025, [https://www.datacamp.com/tutorial/introduction-t-sne](https://www.datacamp.com/tutorial/introduction-t-sne)
40. Visualizing feature vectors/embeddings using PCA and t-SNE | Towards Data Science, дата последнего обращения: декабря 21, 2025, [https://towardsdatascience.com/visualizing-feature-vectors-embeddings-using-pca-and-t-sne-ef157cea3a42/](https://towardsdatascience.com/visualizing-feature-vectors-embeddings-using-pca-and-t-sne-ef157cea3a42/)
41. Introducing t-SNE Embedding Visualization on Datature: Discover Image Similarity and Patterns, дата последнего обращения: декабря 21, 2025, [https://datature.com/blog/introducing-t-sne-embedding-visualization-on-datature-discover-image-similarity-and-patterns](https://datature.com/blog/introducing-t-sne-embedding-visualization-on-datature-discover-image-similarity-and-patterns)
42. Visualizing Embeddings With t-SNE \- Kaggle, дата последнего обращения: декабря 21, 2025, [https://www.kaggle.com/code/colinmorris/visualizing-embeddings-with-t-sne](https://www.kaggle.com/code/colinmorris/visualizing-embeddings-with-t-sne)
43. What is the role of latent factors in recommender systems? \- Milvus, дата последнего обращения: декабря 21, 2025, [https://milvus.io/ai-quick-reference/what-is-the-role-of-latent-factors-in-recommender-systems](https://milvus.io/ai-quick-reference/what-is-the-role-of-latent-factors-in-recommender-systems)
44. ITEM2VEC: NEURAL ITEM EMBEDDING FOR COLLABORATIVE FILTERING Oren Barkan^\* and Noam Koenigstein\* ^Tel Aviv University \*Microsoft \- arXiv, дата последнего обращения: декабря 21, 2025, [https://arxiv.org/pdf/1603.04259](https://arxiv.org/pdf/1603.04259)
45. Inside Spotify's Recommendation System: A Complete Guide (2025 Update), дата последнего обращения: декабря 21, 2025, [https://www.music-tomorrow.com/blog/how-spotify-recommendation-system-works-complete-guide](https://www.music-tomorrow.com/blog/how-spotify-recommendation-system-works-complete-guide)
46. Top Highlighted Recommendation System Ideas from Alibaba | by Tech First \- Medium, дата последнего обращения: декабря 21, 2025, [https://techfirst.medium.com/top-highlighted-recommendation-system-ideas-from-alibaba-39da1d965d07](https://techfirst.medium.com/top-highlighted-recommendation-system-ideas-from-alibaba-39da1d965d07)
47. PinnerSage: Multi-Modal User Embedding Framework for Recommendations at Pinterest \- Stanford Computer Science, дата последнего обращения: декабря 21, 2025, [https://cs.stanford.edu/people/jure/pubs/pinnersage-kdd20.pdf](https://cs.stanford.edu/people/jure/pubs/pinnersage-kdd20.pdf)
48. PinnerSage: Multi-Modal User Embedding Framework for Recommendations at Pinterest, дата последнего обращения: декабря 21, 2025, [https://medium.com/pinterest-engineering/pinnersage-multi-modal-user-embedding-framework-for-recommendations-at-pinterest-bfd116b49475](https://medium.com/pinterest-engineering/pinnersage-multi-modal-user-embedding-framework-for-recommendations-at-pinterest-bfd116b49475)
49. Innovative Recommendation Applications Using Two Tower Embeddings at Uber, дата последнего обращения: декабря 21, 2025, [https://www.uber.com/blog/innovative-recommendation-applications-using-two-tower-embeddings/](https://www.uber.com/blog/innovative-recommendation-applications-using-two-tower-embeddings/)
50. Food Discovery with Uber Eats: Using Graph Learning to Power Recommendations, дата последнего обращения: декабря 21, 2025, [https://www.uber.com/en-US/blog/uber-eats-graph-learning/](https://www.uber.com/en-US/blog/uber-eats-graph-learning/)
51. Machine Learning Solutions for Cold Start Problem in Recommender Systems, дата последнего обращения: декабря 21, 2025, [https://www.expressanalytics.com/blog/cold-start-problem](https://www.expressanalytics.com/blog/cold-start-problem)
52. How to solve the cold start problem in recommender systems \- Things Solver, дата последнего обращения: декабря 21, 2025, [https://thingsolver.com/blog/the-cold-start-problem/](https://thingsolver.com/blog/the-cold-start-problem/)
53. Cold start problem in Recommender systems and how to deal with them \- Medium, дата последнего обращения: декабря 21, 2025, [https://medium.com/@kumarkishalaya/cold-start-problem-in-recommender-systems-and-how-to-deal-with-them-ea2ec841440e](https://medium.com/@kumarkishalaya/cold-start-problem-in-recommender-systems-and-how-to-deal-with-them-ea2ec841440e)
54. Evolution and Scale of Uber's Delivery Search Platform | Uber Blog, дата последнего обращения: декабря 21, 2025, [https://www.uber.com/blog/evolution-and-scale-of-ubers-delivery-search-platform/](https://www.uber.com/blog/evolution-and-scale-of-ubers-delivery-search-platform/)
55. Understanding the softmax output in Youtube's recommender, дата последнего обращения: декабря 21, 2025, [https://datascience.stackexchange.com/questions/60396/understanding-the-softmax-output-in-youtubes-recommender](https://datascience.stackexchange.com/questions/60396/understanding-the-softmax-output-in-youtubes-recommender)
56. Deep Neural Networks for YouTube Recommendations \- Google Research, дата последнего обращения: декабря 21, 2025, [https://research.google.com/pubs/archive/45530.pdf](https://research.google.com/pubs/archive/45530.pdf)
57. Difference between using RMSE and nDCG to evaluate Recommender Systems?, дата последнего обращения: декабря 21, 2025, [https://stackoverflow.com/questions/24212843/difference-between-using-rmse-and-ndcg-to-evaluate-recommender-systems](https://stackoverflow.com/questions/24212843/difference-between-using-rmse-and-ndcg-to-evaluate-recommender-systems)
58. Difference between using RMSE and nDCG to evaluate Recommender Systems, дата последнего обращения: декабря 21, 2025, [https://datascience.stackexchange.com/questions/369/difference-between-using-rmse-and-ndcg-to-evaluate-recommender-systems](https://datascience.stackexchange.com/questions/369/difference-between-using-rmse-and-ndcg-to-evaluate-recommender-systems)
59. A Comprehensive Survey of Evaluation Techniques for Recommendation Systems \- arXiv, дата последнего обращения: декабря 21, 2025, [https://arxiv.org/html/2312.16015v2](https://arxiv.org/html/2312.16015v2)
60. 10 metrics to evaluate recommender and ranking systems \- Evidently AI, дата последнего обращения: декабря 21, 2025, [https://www.evidentlyai.com/ranking-metrics/evaluating-recommender-systems](https://www.evidentlyai.com/ranking-metrics/evaluating-recommender-systems)
61. Serendipity: Accuracy's Unpopular Best Friend in Recommenders \- Eugene Yan, дата последнего обращения: декабря 21, 2025, [https://eugeneyan.com/writing/serendipity-and-accuracy-in-recommender-systems/](https://eugeneyan.com/writing/serendipity-and-accuracy-in-recommender-systems/)
62. Serendipity in Recommender Systems: A Systematic Literature Review \- JCST, дата последнего обращения: декабря 21, 2025, [https://jcst.ict.ac.cn/fileup/1000-9000/PDF/2021-2-13-0135.pdf](https://jcst.ict.ac.cn/fileup/1000-9000/PDF/2021-2-13-0135.pdf)
