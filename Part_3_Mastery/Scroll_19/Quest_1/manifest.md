# Я заставил три нейросети работать на одной GPU. Вот 4 неожиданных урока, которые я выучил.

## Магия AI на одном кристалле

У многих начинающих AI-инженеров есть амбициозная мечта: создать сложный продукт, который использует мощь сразу нескольких нейросетей. Голосовой ассистент, система анализа документов, умный чат-бот — все они требуют слаженной работы нескольких моделей. Но что делать, если в вашем распоряжении не кластер из топовых ускорителей, а всего одна, не самая мощная видеокарта?

Именно с этой проблемой я и столкнулся. Моей задачей было оживить **«конвейер големов»** — метафорическую систему, где три AI-модели последовательно передают друг другу задачу. В роли моих големов выступили: **«Голем-Писец»** (на базе `openai/whisper-tiny`), **«Голем-Толкователь»** (zero-shot классификатор `valhalla/distilbart-mnli-12-3`) и **«Голем-Оракул»** (`distilgpt2`). Сам выбор моделей, особенно `distilgpt2`, уже намекал на главную тему — работа с ограниченными ресурсами, ведь приставка «distil» означает, что это облегченная и более эффективная версия своей старшей сестры.

Заставить их работать вместе на одном **«кристалле маны» (GPU)** оказалось настоящим квестом. В процессе я получил четыре неочевидных озарения, которые изменили мой взгляд на AI-инжиниринг. И сейчас я ими поделюсь.

---

### 1. Урок №1: Принцип «Мусор на входе — Мусор на выходе» — это жестокая реальность

#### Даже идеальная система ломается на первом шаге

Все мы слышали о принципе **«Garbage In, Garbage Out» (GIGO)**, но одно дело — знать о нем в теории, и совсем другое — увидеть его разрушительную силу на практике. В моем эксперименте это стало главным озарением.

Вот что произошло. На первом шаге конвейера Голем-Писец (`openai/whisper-tiny`) прослушал аудиозапись команды, но из-за шума или нечеткости произношения распознал ее абсолютно неверно — как одно слово: **`' Mead.'`**.

С этого момента система начала творить идеально логичный, но совершенно бессмысленный каскад действий:

1.  **Толкователь**, получив на вход бессмыслицу `' Mead.'`, не смог найти релевантного намерения и, в попытке выполнить свою задачу, выбрал наиболее вероятную, но в данном контексте абсолютно случайную метку — *«узнать погоду»*. Технически, он свою работу выполнил.
2.  **Оракул**, получив команду «узнать погоду», честно сгенерировал тематически верный, но абсолютно нерелевантный первоначальному запросу ответ. Он тоже сработал идеально в рамках своей задачи.

Каждый голем в цепи был исправен и отработал без ошибок. Но один сбой в самом начале привел к тому, что конечный результат оказался полностью бесполезным.

> Ты доказал, что даже если каждый компонент системы технически исправен, ошибка на первом шаге **каскадом разрушает** весь последующий результат.

---

### 2. Урок №2: Управление памятью — это ритуал «призыва и изгнания»

#### Как заставить нескольких Големов работать на одном Кристалле Маны (GPU)

Основная техническая проблема была очевидна: три нейросети просто **не помещались в видеопамять (VRAM)** моей GPU одновременно. Попытка загрузить их всех сразу приводила к немедленному исчерпанию ресурсов.

Решение нашлось в последовательном подходе, который я мысленно назвал **ритуалом «призыва и изгнания»**. Вместо того чтобы держать всех големов активными, я организовал их работу поочередно:

1.  **Призыв:** Сначала я «пробуждал» первого Голема (`openai/whisper-tiny`), загружая его в VRAM.
2.  **Использование:** Он выполнял свою задачу — транскрибировал аудио.
3.  **Изгнание:** Сразу после этого я полностью выгружал его из памяти, освобождая «ману» (VRAM) для следующего.

Этот ритуал повторялся для каждого голема в цепочке. Технически это реализуется с помощью простых, но мощных команд: **`del model`** удаляет объект модели из Python, а **`torch.cuda.empty_cache()`** принудительно очищает кэш видеопамяти в PyTorch. Такой подход позволяет управлять жизненным циклом моделей и выполнять сложные задачи даже при самых суровых ограничениях.

---

### 3. Урок №3: Экономия VRAM — это ваша прямая бизнес-ценность

#### Почему умение «изгонять Големов» делает вас ценным инженером

Навык, описанный в предыдущем уроке, — это не просто технический трюк. Это **прямая и измеримая бизнес-ценность**. В реальном мире AI-продукты работают на облачной инфраструктуре, и стоимость аренды мощных GPU — одна из главных статей расходов.

Умение строить сложные AI-цепочки и заставлять их эффективно работать на менее производительном (а значит, более дешевом) оборудовании **напрямую снижает затраты** компании. Инженер, который понимает, как управлять памятью и оптимизировать использование ресурсов, может сэкономить проекту огромные деньги.

> Инженер, который может заставить несколько моделей работать на одной, недорогой GPU, **невероятно ценен** для любой компании.

---

### 4. Урок №4: «Монолит» или «Микросервисы»? Не существует единственно верного пути

#### Два разных пути архитектуры для ваших AI-систем

Последовательный вызов моделей на одном устройстве показался мне медленным. Это подвело меня к вопросу, который в нашем квесте задал «Техномант»: *неужели в больших компаниях, где ресурсы почти безграничны, тоже ждут, пока одна модель выгрузится, чтобы загрузить другую?* «Ответ Мастера» пролил свет на два фундаментально разных подхода.

1.  **«Монолитный конвейер».** Это именно тот подход, который я реализовал. Все «големы» вызываются последовательно в рамках одного процесса на одном устройстве. Он идеален для прототипирования, работы на локальной машине или в условиях жестких ресурсных ограничений.
2.  **«Микросервисы».** В этом подходе каждый «голем» живет на своем собственном «алтаре» (сервере), будучи запечатанным в «амулет» (контейнер, например, Docker). Они общаются друг с другом не напрямую через память, а по сети, обмениваясь «магическими посланиями» (API-запросами). Этот путь выбирают для создания высоконагруженных, масштабируемых систем, где важна максимальная скорость отклика и независимость компонентов.

Мой квест научил меня первому, фундаментальному подходу. Понимание его принципов и ограничений — это необходимая база для того, чтобы в будущем строить более сложные микросервисные архитектуры.

---

## Что дальше?

Этот эксперимент доказал, что создание мощных AI-продуктов — это не только про выбор самых продвинутых моделей. Это в первую очередь про **грамотную инженерию**, продуманную архитектуру и умение виртуозно работать с ограничениями. Научившись «призывать и изгонять» модели, вы открываете дверь в мир сложных систем, которые можно создавать даже на скромном оборудовании.

*Теперь, когда вы знаете секрет «монолитного конвейера», какую сложную AI-систему вы бы попробовали создать на своей машине?*
