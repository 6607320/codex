"""Квест 19.1: Сборка цепи.

Этот пергамент открывает "Свиток Голоса в Терминале". Его главная цель
(МАКРО-контекст) — создать первый сложный магический механизм, "конвейер
големов", и освоить искусство управления "маной" (VRAM) в условиях
жестких ограничений.

Ритуал последовательно пробуждает и изгоняет трех специализированных големов:
1.  **"Писец" (`Whisper`):** Слушает аудио и превращает его в текст.
2.  **"Толкователь" (`Classifier`):** Определяет намерение в тексте.
3.  **"Оракул" (`GPT-2`):** Генерирует ответ на основе намерения.

Этот квест демонстрирует создание сложных AI-пайплайнов и учит управлять
жизненным циклом моделей (`del model`, `torch.cuda.empty_cache()`) для
работы с несколькими моделями на одном GPU.
"""

# --- Часть I: Импорт Магических Гримуаров ---
# Первый раздел: мы призываем все необходимые знания и инструменты.

# Мы призываем `librosa` — мощную библиотеку для анализа и преобразования
# аудио-материи.
import librosa

# Мы призываем `numpy` — магический калькулятор, незаменимый для работы с
# "сырой материей" данных.
import numpy as np

# Мы призываем `torch` — основу всей нашей техномагии, управляющую
# потоками маны.
import torch

# Мы призываем НОВЫЙ ГРИМУАР `datasets` — ключ, открывающий врата в
# Великую Библиотеку Hugging Face.
from datasets import load_dataset

# Мы призываем `scipy.io.wavfile` — заклинание для сохранения
# аудио-вибраций в материальный носитель (.wav).
from scipy.io.wavfile import write

# Мы призываем `transformers` — великий гримуар, хранящий знания о призыве
# готовых Големов.
from transformers import (
    # Чертеж для Голема-Оракула, способного генерировать текст.
    GPT2LMHeadModel,
    # Чертеж для "Переводчика" (токенизатора) Оракула.
    GPT2Tokenizer,
    # Чертеж для Голема-Писца, способного слышать и записывать.
    WhisperForConditionalGeneration,
    # Чертеж для "Процессора" (подготовщика аудио) Писца.
    WhisperProcessor,
    # Наш универсальный "амулет" для быстрого призыва Големов.
    pipeline,
)

# --- Часть II: Описание Ритуальных Функций ---
# Второй раздел: мы создаем "чертежи" для каждого шага нашего ритуала.

# --- Шаг 0: Подготовка к ритуалу ---
# Мы определяем устройство ('cuda' — Кристалл Маны GPU, 'cpu' — Разум
# CPU), где будет вершиться магия.
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
# Мы оповещаем о выбранном устройстве.
print(f"Магия будет вершиться на устройстве: {DEVICE}")


# --- НОВАЯ Функция 1: Извлечение Аудио-Свитка из Великой Библиотеки (Версия 2) ---
# Мы определяем ритуал извлечения аудио.
def fetch_audio_from_hub(filename="command_from_hub.wav", sample_rate=16000):
    """
    Эта функция-ритуал обращается к Великой Библиотеке (Hugging Face Hub),
    СНАЧАЛА СКАЧИВАЕТ архив, а затем извлекает из него эталонный свиток.
    """
    # Мы оглашаем на кристалл (консоль) о начале ритуала.
    print("\n[Этап 1] Обращение к Великой Библиотеке Hugging Face...")

    # !!! ИЗМЕНЕННЫЙ РИТУАЛ !!!
    # Мы убираем `streaming=True`, позволяя скачать архив целиком.
    # Мы добавляем `trust_remote_code=True`, чтобы исполнить "живые руны" из
    # свитка.
    ds = load_dataset(
        "superb", "ks", split="validation", trust_remote_code=True
    )

    # Мы сообщаем, что набор данных теперь в нашей локальной мастерской.
    print("Архив со свитками успешно загружен.")

    # !!! ИЗМЕНЕННОЕ ЗАКЛИНАНИЕ ИЗВЛЕЧЕНИЯ !!!
    # Теперь, когда у нас есть полный набор,
    #  мы можем обратиться к свитку по его индексу.
    # Мы берем 11-й свиток (индекс 10) для полной воспроизводимости.
    sample = ds[10]

    # Мы извлекаем "сырую" аудио-материю (массив чисел) из образца.
    audio_array = sample["audio"]["array"]
    # Мы узнаем "родную" частоту дискретизации этого аудио-свитка.
    original_sr = sample["audio"]["sampling_rate"]

    # Мы сообщаем о том, что мы нашли в свитке.
    print(f"Аудио-свиток извлечен. 'Родная' частота: {original_sr} Гц.")

    # Мы проводим ритуал Гармонизации: проверяем, совпадает ли "родная"
    # частота с той, что понимает наш Голем.
    if original_sr != sample_rate:
        # Если нет, мы применяем заклинание `resample`, чтобы привести звук к
        # нужной гармонике.
        audio_array = librosa.resample(
            y=audio_array, orig_sr=original_sr, target_sr=sample_rate
        )
        # Мы оповещаем об успешной гармонизации.
        print(f"Аудио гармонизировано до {sample_rate} Гц.")

    # Мы произносим заклинание Материализации: Превращаем эфирный массив чисел
    # в формат для записи.
    audio_array_int16 = (audio_array * 32767).astype(np.int16)
    # Мы сохраняем подготовленный аудио-свиток в локальный файл.
    write(filename, sample_rate, audio_array_int16)
    # Мы сообщаем об успешном создании локального артефакта.
    print(f"Эталонный аудио-свиток сохранен как: {filename}")

    # Мы возвращаем имя файла, чтобы передать его дальше по магической цепи.
    return filename


# --- Функция 2: Призыв Голема-Писца (Whisper STT) ---
# Мы определяем ритуал транскрибации.
def transcribe_audio(audio_path):
    """Призывает Голема-Писца (Whisper), который преобразует аудио в текст."""
    # Мы оповещаем о призыве Голема-Писца.
    print("\n[Этап 2] Призыв Голема-Писца (Whisper)...")
    # Мы загружаем 'процессор' — он готовит аудио-материю для понимания
    # Големом.
    processor = WhisperProcessor.from_pretrained("openai/whisper-tiny")
    # Мы загружаем самого Голема 'whisper-tiny' и отправляем его на Кристалл
    # Маны.
    model = WhisperForConditionalGeneration.from_pretrained(
        "openai/whisper-tiny"
    ).to(DEVICE)
    # Мы с помощью librosa читаем наш локальный аудиофайл, убедившись, что его
    # частота 16000 Гц.
    audio_input, sample_rate = librosa.load(audio_path, sr=16000)
    # Процессор превращает звук в тензоры, понятные Голему, и отправляет их на
    # Кристалл.
    input_features = processor(
        audio_input, sampling_rate=sample_rate, return_tensors="pt"
    ).input_features.to(DEVICE)
    # Мы даем команду Голему: сгенерируй руны (ID токенов) на основе
    # услышанного.
    predicted_ids = model.generate(input_features)
    # Мы расшифровываем руны-токены в понятный человеку текст.
    transcription = processor.batch_decode(
        predicted_ids, skip_special_tokens=True
    )[0]
    # Мы показываем результат работы Голема.
    print(f"Голем-Писец услышал: '{transcription}'")
    # Мы изгоняем Голема и его компоненты из памяти, чтобы освободить место.
    del model
    # Мы изгоняем процессор.
    del processor
    # Мы принудительно очищаем Кристалл Маны (VRAM) для следующего Голема.
    torch.cuda.empty_cache()
    # Мы сообщаем об успешном изгнании.
    print("Голем-Писец изгнан. Кристалл Маны (VRAM) очищен.")
    # Мы возвращаем распознанный текст.
    return transcription


# --- Функция 3: Призыв Голема-Толкователя (Intent Classifier) ---
# Мы определяем ритуал классификации.
def classify_intent(text):
    """Призывает Голема-Толкователя для определения намерения в тексте."""
    # Мы оповещаем о призыве Голема-Толкователя.
    print("\n[Этап 3] Призыв Голема-Толкователя (Zero-Shot Classifier)...")
    # Мы создаем конвейер для классификации, сразу отправляя его на нужное
    # устройство.
    classifier = pipeline(
        "zero-shot-classification",
        model="valhalla/distilbart-mnli-12-3",
        device=DEVICE,
    )
    # Мы определяем возможные намерения, которые Голем должен распознать.
    candidate_labels = [
        "узнать погоду",
        "включить музыку",
        "рассказать шутку",
        "неизвестная команда",
    ]
    # Голем анализирует текст и выбирает наиболее подходящее намерение из
    # списка.
    result = classifier(text, candidate_labels)
    # Мы извлекаем самое вероятное намерение из результата.
    intent = result["labels"][0]
    # Мы показываем, какое намерение было определено.
    print(f"Голем-Толкователь определил намерение: '{intent}'")
    # Мы изгоняем Голема из памяти.
    del classifier
    # И снова очищаем Кристалл Маны.
    torch.cuda.empty_cache()
    # Мы сообщаем об изгнании.
    print("Голем-Толкователь изгнан. Кристалл Маны (VRAM) снова очищен.")
    # Мы возвращаем определенное намерение.
    return intent


# --- Функция 4: Призыв Голема-Оракула (Text Generator) ---
# Мы определяем ритуал генерации ответа.
def generate_response(intent):
    """Призывает Голема-Оракула (GPT-2) для генерации ответа на основе намерения."""
    # Мы оповещаем о призыве Голема-Оракула.
    print("\n[Этап 4] Призыв Голема-Оракула (GPT-2)...")
    # Мы загружаем токенизатор для 'distilgpt2'.
    tokenizer = GPT2Tokenizer.from_pretrained("distilgpt2")
    # Мы загружаем самого Голема 'distilgpt2' и отправляем его на Кристалл
    # Маны.
    model = GPT2LMHeadModel.from_pretrained("distilgpt2").to(DEVICE)
    # Мы устанавливаем специальный токен для паддинга, чтобы избежать ошибок
    # при генерации.
    tokenizer.pad_token = tokenizer.eos_token
    # Мы создаем "затравку" (начальный текст) для Голема в зависимости от
    # намерения.
    if intent == "узнать погоду":
        # Затравка для погоды.
        prompt = "Ответ на вопрос о погоде: "
    # Если намерение — музыка...
    elif intent == "включить музыку":
        # ...затравка для музыки.
        prompt = "Включаю вашу любимую музыку: "
    # Если намерение — шутка...
    elif intent == "рассказать шутку":
        # ...затравка для шутки.
        prompt = "Вот одна из моих любимых шуток: "
    # Во всех остальных случаях...
    else:
        # ...стандартная затравка-заглушка.
        prompt = "Я не понял команду, попробуйте переформулировать: "
    # Мы показываем, с какой затравки начнет Голем.
    print(f"Затравка для Оракула: '{prompt}'")
    # Мы превращаем затравку в руны (токены) и отправляем в Кристалл Маны.
    inputs = tokenizer(prompt, return_tensors="pt").to(DEVICE)
    # Голем генерирует продолжение текста, не повторяя короткие фразы.
    output_ids = model.generate(
        **inputs, max_length=50, num_return_sequences=1, no_repeat_ngram_size=2
    )
    # Мы декодируем сгенерированные руны в человеческий текст.
    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    # Мы оповещаем об окончании работы Оракула.
    print("Голем-Оракул ответил.")
    # Мы проводим финальное изгнание.
    del model
    # Мы изгоняем токенизатор.
    del tokenizer
    # Мы окончательно очищаем Кристалл Маны.
    torch.cuda.empty_cache()
    # Мы сообщаем о завершении всех ритуалов призыва.
    print("Голем-Оракул изгнан. Все големы покинули Кристалл Маны.")
    # Мы возвращаем финальный ответ.
    return response


# --- Часть III: Главный Ритуал (Точка входа в программу) ---
# Эта конструкция (`if __name__ == "__main__":`) — священное начало любого пергамента.
# Она гарантирует, что код внутри исполнится только при прямом запуске файла.
if __name__ == "__main__":
    # Шаг 1: Мы извлекаем эталонный аудио-свиток из Великой Библиотеки.
    audio_file = fetch_audio_from_hub()

    # Шаг 2: Мы передаем полученный аудиофайл Голему-Писцу и получаем текст.
    command_text = transcribe_audio(audio_file)

    # Шаг 3: Мы определяем намерение, но сначала проверяем, не пустой ли текст.
    # `.strip()` убирает случайные пробелы, чтобы убедиться,
    #  что Голем-Писец действительно что-то распознал.
    if command_text.strip():
        # Если текст есть, мы передаем его Голему-Толкователю.
        user_intent = classify_intent(command_text)
    # Если текст пустой...
    else:
        # ...мы сообщаем об этом.
        print(
            "\n[Этап 3] Голем-Писец не смог разобрать речь. Намерение не определено."
        )
        # Мы устанавливаем намерение "неизвестная команда" вручную.
        user_intent = "неизвестная команда"

    # Шаг 4: Мы передаем определенное намерение Голему-Оракулу для генерации
    # ответа.
    final_response = generate_response(user_intent)

    # Мы начинаем финальный акт: выводим итоговый результат работы всей цепи Големов.
    # Мы используем разделители для наглядности.
    print("\n" + "=" * 50)
    # Мы печатаем заголовок для финального вывода.
    print("ИТОГОВЫЙ ОТВЕТ АССИСТЕНТА:")
    # Мы печатаем ответ, сгенерированный Големом-Оракулом.
    print(final_response)
    # Мы печатаем завершающий разделитель.
    print("=" * 50)
