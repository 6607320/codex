# Великий Кодекс Техномагии
# Имя этого священного документа.
# Свиток 19: Голос в Терминале
# Название текущего раздела Кодекса.
# Квест 19.2: Оживление в терминале (Симулированная версия)
# Каноническое имя Квеста, как оно записано в Кодексе.

# --- Часть I: Импорт Магических Гримуаров ---
# Первый раздел: мы призываем все необходимые знания и инструменты.

# Мы призываем `os` — гримуар для взаимодействия с операционной системой.
import os

# Мы призываем `time` — гримуар для управления временем.
import time

# Мы призываем `librosa` — мощную библиотеку для анализа и преобразования
# аудио-материи.
import librosa

# Мы призываем `torch` — основу всей нашей техномагии.
import torch

# Мы призываем НОВЫЙ ГРИМУАР `gtts` (Google Text-to-Speech) —
# духа-глашатая, способного превращать текст в речь.
from gtts import gTTS

# Мы призываем `transformers` — великий гримуар, хранящий знания о призыве
# готовых Големов.
from transformers import (
    # Чертеж для Голема-Оракула.
    GPT2LMHeadModel,
    # Чертеж для "Переводчика" Оракула.
    GPT2Tokenizer,
    # Чертеж для Голема-Писца.
    WhisperForConditionalGeneration,
    # Чертеж для "Процессора" Писца.
    WhisperProcessor,
    # Наш универсальный "амулет".
    pipeline,
)

# `numpy` (с псевдонимом `np`) — магический калькулятор, незаменимый для работы с "сырой материей" данных.
import numpy as np


# --- Часть II: Описание Ритуальных Функций ---
# Второй раздел: мы создаем "чертежи" для каждого шага нашего ритуала.
# Функции-чертежи для наших Големов остаются теми же, что и в прошлых квестах.
# Их модульность — наша сила. Мы просто вызываем их, не меняя внутренностей.

# --- Шаг 0: Подготовка к ритуалу ---
# Мы определяем устройство ('cuda' — Кристалл Маны GPU, 'cpu' — Разум
# CPU), где будет вершиться магия.
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
# Мы оповещаем о выбранном устройстве.
print(f"Магия будет вершиться на устройстве: {DEVICE}")


# --- НОВАЯ Функция 1: Создание Театральных Реквизитов (Аудиофайлов) ---
# Мы определяем подготовительный ритуал.
def prepare_dialogue_props(dialogue_script, lang="ru"):
    """
    Создает аудиофайлы для каждой реплики в нашем "сценарии".
    """
    # Мы оглашаем на кристалл (консоль) о начале подготовительного ритуала.
    print("\n[Подготовка] Создание реквизита (аудиофайлов)...")
    # Мы произносим заклинание из гримуара `os`: "Создай папку 'dialogue_audio', если ее еще не существует".
    # `exist_ok=True` — мудрая руна, которая предотвращает ошибку, если папка уже есть.
    os.makedirs("dialogue_audio", exist_ok=True)

    # Мы запускаем цикл, который пройдет по каждой реплике в нашем списке-сценарии.
    # `enumerate` — хитрый дух-счетчик, который дает нам и номер реплики (`i`), и ее текст (`text`).
    for i, text in enumerate(dialogue_script):
        # Мы создаем уникальное имя для файла, используя номер реплики.
        filename = f"dialogue_audio/step_{i}.mp3"
        # Мы сообщаем, какую именно реплику мы сейчас создаем.
        print(f"-> Создаю реплику '{text}' -> {filename}")
        # Мы призываем духа-глашатая `gTTS`, передавая ему текст и язык ('ru' —
        # русский).
        tts = gTTS(text=text, lang=lang)
        # Мы приказываем духу "материализовать" свою речь в mp3-файл.
        tts.save(filename)
    # Мы сообщаем, что вся подготовка завершена.
    print("...реквизит готов.\n")


# --- Функция 2: Призыв Голема-Писца (Whisper STT) ---
# Мы определяем ритуал транскрибации.
def transcribe_audio(audio_path):
    """Призывает Голема-Писца (Whisper), который преобразует аудио в текст."""
    # Мы оглашаем о призыве.
    print("[Акт 1] Призыв Голема-Писца (Whisper)...")
    # Мы загружаем 'процессор' — он готовит аудио-материю для понимания
    # Големом.
    processor = WhisperProcessor.from_pretrained("openai/whisper-tiny")
    # Мы загружаем самого Голема 'whisper-tiny' и отправляем его на Кристалл
    # Маны.
    model = WhisperForConditionalGeneration.from_pretrained(
        "openai/whisper-tiny"
    ).to(DEVICE)
    # Мы с помощью librosa читаем наш аудиофайл (mp3 или wav).
    audio_input, sample_rate = librosa.load(audio_path, sr=16000)
    # Процессор превращает звук в тензоры, понятные Голему.
    input_features = processor(
        audio_input, sampling_rate=sample_rate, return_tensors="pt"
    ).input_features.to(DEVICE)
    # Мы даем команду Голему: "Сгенерируй руны (токены) на основе услышанного".
    predicted_ids = model.generate(input_features)
    # Мы расшифровываем руны-токены в понятный человеку текст.
    transcription = processor.batch_decode(
        predicted_ids, skip_special_tokens=True
    )[0]
    # Мы показываем результат работы Голема.
    print(f"-> Услышано: '{transcription}'")
    # Мы изгоняем Голема и его компоненты из памяти.
    del model, processor
    # Мы принудительно очищаем Кристалл Маны (VRAM).
    torch.cuda.empty_cache()
    # Мы сообщаем об успешном изгнании.
    print("...Голем-Писец изгнан.")
    # Мы возвращаем распознанный текст.
    return transcription


# --- Функция 3: Призыв Голема-Толкователя (Intent Classifier) ---
# Мы определяем ритуал классификации.
def classify_intent(text):
    """Призывает Голема-Толкователя для определения намерения в тексте."""
    # Мы оглашаем о призыве.
    print("[Акт 2] Призыв Голема-Толкователя...")
    # Мы создаем конвейер для классификации.
    classifier = pipeline(
        "zero-shot-classification",
        model="valhalla/distilbart-mnli-12-3",
        device=DEVICE,
    )
    # Мы определяем возможные намерения, которые Голем должен распознать.
    candidate_labels = [
        "узнать погоду",
        "включить музыку",
        "рассказать шутку",
        "неизвестная команда",
    ]
    # Голем анализирует текст и выбирает наиболее подходящее намерение.
    result = classifier(text, candidate_labels)
    # Мы извлекаем самое вероятное намерение из результата.
    intent = result["labels"][0]
    # Мы показываем, какое намерение было определено.
    print(f"-> Намерение определено как: '{intent}'")
    # Мы изгоняем Голема из памяти.
    del classifier
    # Мы очищаем Кристалл Маны.
    torch.cuda.empty_cache()
    # Мы сообщаем об изгнании.
    print("...Голем-Толкователь изгнан.")
    # Мы возвращаем определенное намерение.
    return intent


# --- Функция 4: Призыв Голема-Оракула (Text Generator) ---
# Мы определяем ритуал генерации ответа.
def generate_response(intent):
    """Призывает Голема-Оракула (GPT-2) для генерации ответа на основе намерения."""
    # Мы оглашаем о призыве.
    print("[Акт 3] Призыв Голема-Оракула...")
    # Мы загружаем токенизатор для 'distilgpt2'.
    tokenizer = GPT2Tokenizer.from_pretrained("distilgpt2")
    # Мы загружаем самого Голема 'distilgpt2'.
    model = GPT2LMHeadModel.from_pretrained("distilgpt2").to(DEVICE)
    # Мы устанавливаем специальный токен для паддинга.
    tokenizer.pad_token = tokenizer.eos_token
    # Мы используем словарь для чистоты и расширяемости кода.
    prompts = {
        "узнать погоду": "Ответ на вопрос о погоде: ",
        "включить музыку": "Конечно, включаю вашу любимую музыку: ",
        "рассказать шутку": "Вот одна из моих любимых шуток: ",
        "неизвестная команда": "Я не совсем понял команду, попробуйте переформулировать: ",
    }
    # `.get()` — безопасный способ получить значение, если намерения нет в словаре.
    prompt = prompts.get(intent, prompts["неизвестная команда"])
    # Мы превращаем затравку в руны (токены).
    inputs = tokenizer(prompt, return_tensors="pt").to(DEVICE)
    # Голем генерирует продолжение текста.
    output_ids = model.generate(
        **inputs, max_length=50, num_return_sequences=1, no_repeat_ngram_size=2
    )
    # Мы декодируем сгенерированные руны в человеческий текст.
    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    # Мы изгоняем Голема и его компоненты.
    del model, tokenizer
    # Мы очищаем Кристалл Маны.
    torch.cuda.empty_cache()
    # Мы сообщаем об изгнании.
    print("...Голем-Оракул изгнан.")
    # Мы возвращаем финальный ответ.
    return response


# --- Часть III: Ритуал "Театра Марионеток" ---
# Эта конструкция (`if __name__ == "__main__":`) — священное начало любого
# пергамента.
if __name__ == "__main__":
    # Мы создаем наш "сценарий" — список команд, которые мы хотим
    # протестировать по очереди.
    dialogue_script = [
        # Первая реплика.
        "Расскажи мне шутку",
        # Вторая реплика.
        "Какая сегодня погода?",
        # Третья реплика, которая будет нашим "словом-ключом" для выхода.
        "Хватит",
    ]

    # Шаг 1: Мы вызываем подготовительный ритуал, чтобы создать все аудиофайлы
    # заранее.
    prepare_dialogue_props(dialogue_script)

    # Мы печатаем приветствие Мастера, которое выводится один раз при запуске.
    print("=" * 50)
    # Мы сообщаем о начале симуляции.
    print("Симуляция диалога с ассистентом начинается.")
    # Мы печатаем финальный разделитель для красоты.
    print("=" * 50 + "\n")

    # Мы запускаем цикл, который пройдет по каждой реплике в нашем
    # списке-сценарии.
    for i, text_command in enumerate(dialogue_script):
        # Мы используем заклинание 'сна' из гримуара `time`, чтобы создать паузу в 2 секунды.
        # Это делает симуляцию более похожей на реальный диалог.
        time.sleep(2)
        # Мы имитируем реплику пользователя, выводя ее в терминал.
        print(f">>> Пользователь говорит: '{text_command}'")

        # Мы указываем путь к нужному аудиофайлу, используя его номер.
        audio_file = f"dialogue_audio/step_{i}.mp3"

        # Шаг 2: Мы вызываем ритуал распознавания речи, передавая ему
        # аудиофайл.
        command_text = transcribe_audio(audio_file)

        # Шаг 3: Мы проверяем, не содержит ли распознанный текст наше
        # "слово-ключ".
        if "стоп" in command_text.lower() or "хватит" in command_text.lower():
            # Если да, мы выводим прощальное сообщение.
            print("\nАссистент: Повинуюсь, Мастер. Завершаю работу.")
            # И мы разрушаем цикл с помощью заклинания `break`.
            break

        # Шаг 4: Мы проверяем, не пуст ли результат распознавания.
        if command_text.strip():
            # Если текст есть, мы запускаем остальную часть конвейера.
            user_intent = classify_intent(command_text)
            # Мы генерируем ответ.
            final_response = generate_response(user_intent)
        # Если текст пустой...
        else:
            # ...мы формируем ответ-извинение.
            final_response = "Я ничего не услышал."

        # Шаг 5: Мы выводим финальный ответ ассистента.
        print("\n" + "-" * 20 + " ОТВЕТ АССИСТЕНТА " + "-" * 20)
        # Мы печатаем ответ, который был либо сгенерирован, либо является
        # сообщением об ошибке.
        print(final_response)
        # Мы печатаем красивую нижнюю рамку.
        print("-" * (43 + len(" ОТВЕТ АССИСТЕНТА ")) + "\n")
