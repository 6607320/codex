# **Технический отчет: Архитектура реального времени для детекции и трекинга объектов с использованием YOLOv5n и алгоритма SORT**

## **Аннотация**

В современном ландшафте компьютерного зрения (Computer Vision) наблюдается фундаментальный сдвиг от статического анализа изображений к динамическому пониманию видеопотока в реальном времени. Данный отчет представляет собой исчерпывающее техническое исследование методов развертывания легковесных нейросетевых архитектур, в частности модели YOLOv5n («Nano»), для задач детекции объектов на граничных устройствах (Edge AI). Работа охватывает полный цикл разработки: от теоретических основ одностадийных детекторов до низкоуровневой обработки тензоров и реализации алгоритмов визуализации.  
Особое внимание уделяется проблеме временной когерентности — способности системы не только обнаруживать объекты в отдельных кадрах, но и сохранять их идентификацию во времени. В связи с этим проводится глубокий анализ алгоритма SORT (Simple Online and Realtime Tracking), разбираются математические принципы работы фильтра Калмана для оценки состояния динамических систем и венгерского алгоритма для решения задач ассоциации данных. Отчет также включает детальный обзор бизнес-применений данных технологий в сферах автономного транспорта, ритейл-аналитики, физической безопасности и медицинской диагностики, демонстрируя, как сырые пиксельные данные трансформируются в измеримую экономическую ценность.

## **1\. Введение: Парадигма «Химеры» в компьютерном зрении**

### **1.1 Эволюция методов детекции: От скользящего окна до «Взгляда Химеры»**

История компьютерного зрения долгое время была историей компромиссов. Ранние методы, основанные на ручном конструировании признаков (такие как SIFT или HOG) в сочетании с классификаторами SVM, страдали от низкой обобщающей способности. Появление сверточных нейронных сетей (CNN) кардинально изменило ситуацию, однако первые архитектуры, такие как R-CNN (Region-based CNN), работали по двухстадийному принципу: сначала генерировались тысячи гипотез о местоположении объектов (region proposals), а затем каждая гипотеза классифицировалась отдельно. Это обеспечивало высокую точность, но делало невозможным работу в реальном времени из\-за огромных вычислительных затрат.  
Архитектура YOLO (You Only Look Once), представленная Джозефом Редмоном, предложила революционный подход: рассматривать задачу детекции не как последовательность классификаций, а как единую задачу регрессии. Нейросеть «смотрит» на изображение один раз и предсказывает тензоры, содержащие координаты рамок и вероятности классов одновременно для всех объектов. В контексте текущего исследования (Квест 18.1) мы используем метафору «Химеры» для описания модели YOLOv5n. Подобно мифологическому существу, состоящему из частей разных животных, YOLOv5 представляет собой гибрид передовых инженерных решений: backbone для экстракции признаков, neck для их агрегации и head для финального предсказания.

### **1.2 Постановка задачи: Переход к низкоуровневому анализу**

Целью данного исследования является не просто запуск готового скрипта, а деконструкция процесса инференса. Стандартные высокоуровневые API часто скрывают от инженера детали происходящего, возвращая уже отрисованные изображения. Однако для создания сложных систем — например, для подсчета посетителей или анализа траекторий — необходимо работать с «сырыми» результатами предсказаний: координатами ограничивающих рамок (bounding boxes), показателями уверенности (confidence scores) и индексами классов.  
В рамках «ритуала» развертывания мы реализуем полный пайплайн на языке Python, который:

1. Инициализирует модель YOLOv5n.
2. Обрабатывает видеопоток покадрово.
3. Извлекает тензоры детекций.
4. Визуализирует результаты с использованием графических примитивов OpenCV.
5. Сохраняет артефакты (обработанные кадры) для верификации.

### **1.3 Проблема «Амнезии» детектора и роль Трекинга**

Ключевым вызовом, озвученным в техническом задании (вопрос «Техноманта»), является отсутствие памяти у детектора. YOLO обрабатывает каждый кадр _tabula rasa_ — с чистого листа. Для системы детекции кот на кадре t и тот же самый кот на кадре t+1 — это два независимых события. Чтобы превратить «Охотника» (детектор) в «Следопыта» (трекер), необходимо внедрение алгоритмов, обеспечивающих временную связность данных. В данном отчете мы детально разберем алгоритм SORT, который решает эту задачу, используя прогнозирование движения и метрическую оптимизацию.

## **2\. Архитектура YOLOv5: Анатомия скорости и точности**

### **2.1 Семейство моделей YOLOv5 и спецификация Nano**

YOLOv5 — это не одна модель, а семейство масштабируемых архитектур, спроектированных для различных сценариев использования. Они различаются по глубине (количеству слоев) и ширине (количеству каналов в свертках). Модель YOLOv5n (Nano), выбранная для данного исследования, является самой компактной и быстрой версией.

| Вариант модели     | Параметры (млн) | FLOPs (млрд) | Назначение                                   |
| :----------------- | :-------------- | :----------- | :------------------------------------------- |
| **YOLOv5n (Nano)** | **\~1.9**       | **\~4.5**    | **Мобильные устройства, IoT, Real-time CPU** |
| YOLOv5s (Small)    | \~7.2           | \~16.5       | Десктопные приложения, баланс скорости       |
| YOLOv5m (Medium)   | \~21.2          | \~49.0       | Общего назначения                            |
| YOLOv5l (Large)    | \~46.5          | \~109.1      | Высокая точность, серверные GPU              |
| YOLOv5x (XLarge)   | \~86.7          | \~205.7      | Исследовательские задачи, SOTA точность      |

_Таблица 1\. Сравнительные характеристики моделей семейства YOLOv5._  
Экстремальная компактность версии Nano (менее 2 млн параметров) достигается за счет использования множителей глубины (depth_multiple: 0.33) и ширины (width_multiple: 0.25). Это означает, что модель содержит лишь треть слоев и четверть каналов по сравнению с базовой версией YOLOv5l. Такая оптимизация критически важна для «Квеста 18.1», так как позволяет запускать инференс на обычном процессоре (CPU) с приемлемой частотой кадров (FPS), не требуя мощных графических ускорителей (GPU).

### **2.2 CSPDarknet: Экстракция признаков**

«Позвоночником» (Backbone) модели YOLOv5 служит архитектура CSPDarknet. Ключевой инновацией здесь является использование концепции Cross Stage Partial Networks (CSPNet). В стандартных сверточных сетях градиенты дублируются при обратном распространении ошибки, что ведет к избыточности вычислений. CSPNet разделяет карту признаков базового слоя на две части: одна проходит через плотный блок сверток, а другая обходит его напрямую и конкатенируется в конце. Это решение позволяет:

- Сократить количество вычислений (FLOPs) на 20% без потери точности.
- Улучшить градиентный поток, предотвращая затухание градиентов в глубоких сетях.
- Уменьшить потребление памяти, что критично для Nano-версии.

### **2.3 PANet: Агрегация контекста**

Для того чтобы детектор одинаково хорошо распознавал и огромного слона, и маленького кота, используется «шея» (Neck) модели. YOLOv5 применяет Path Aggregation Network (PANet). Это развитие идеи Feature Pyramid Network (FPN). Если FPN передает семантическую информацию от верхних (глубоких) слоев к нижним, то PANet добавляет еще один путь — от нижних слоев к верхним. Это позволяет модели комбинировать высокоуровневую семантику (что это за объект?) с низкоуровневой информацией о локализации (где именно границы объекта?).

### **2.4 Head: Предсказание и якорные рамки (Anchors)**

«Голова» (Head) модели отвечает за финальное предсказание. Она генерирует выходные тензоры в трех масштабах (для обнаружения мелких, средних и крупных объектов). YOLOv5 использует механизм якорных рамок (Anchor Boxes). В отличие от ранних версий, где якоря задавались вручную, YOLOv5 применяет алгоритм Auto-Anchor. Перед началом обучения модель анализирует обучающий датасет (например, COCO) и с помощью K-means кластеризации определяет оптимальные размеры рамок, наиболее часто встречающиеся в данных. Это позволяет модели быстрее сходиться и точнее предсказывать границы нестандартных объектов.  
\---

## **3\. Ритуал Развертывания: Инженерная реализация**

Переход от теории к практике требует настройки окружения и работы с программным кодом. В рамках «Квеста» мы используем экосистему Python.

### **3.1 Подготовка Гримуара: Установка зависимостей**

Первым шагом является установка библиотеки ultralytics. Команда pip install yolov5 автоматически подтягивает необходимые зависимости:

- **PyTorch**: Фреймворк глубокого обучения, обеспечивающий тензорные вычисления.
- **OpenCV-Python**: Библиотека компьютерного зрения для работы с видеопотоком и графикой.
- **Pandas**: Используется для удобного представления результатов детекции в табличном виде.
- **Seaborn/Matplotlib**: Для визуализации метрик обучения.

Важно отметить, что для достижения максимальной производительности рекомендуется наличие установленного CUDA Toolkit и совместимой видеокарты NVIDIA. PyTorch Hub умеет автоматически переносить вычисления на GPU (device='cuda'), если он доступен, что ускоряет инференс в десятки раз по сравнению с CPU.

### **3.2 Призыв Химеры: Загрузка модели через PyTorch Hub**

PyTorch Hub предоставляет унифицированный интерфейс для загрузки предобученных моделей. Это избавляет инженера от необходимости вручную скачивать файлы весов (.pt) и конфигурационные файлы (.yaml).  
`import torch`

`# Загрузка модели YOLOv5n`  
`# 'pretrained=True' означает, что веса модели уже обучены на датасете COCO (80 классов)`  
`model = torch.hub.load('ultralytics/yolov5', 'yolov5n', pretrained=True)`

`# Настройка параметров инференса (опционально)`  
`model.conf = 0.25  # Порог уверенности (NMS confidence threshold)`  
`model.iou = 0.45   # Порог IoU для NMS`  
`model.classes = None # Фильтр классов (None = все классы)`

_Ссылка на документацию:._  
Использование флага pretrained=True критически важно. Без него модель будет инициализирована случайными весами и не сможет ничего детектировать («слепая Химера»). Обучение на COCO дает модели знание о 80 базовых классах объектов, включая людей, автомобили, животных (котов, собак) и предметы быта.

### **3.3 Работа с видеопотоком: OpenCV Pipeline**

Для обработки видео используется класс cv2.VideoCapture. Здесь кроется важный технический нюанс: цветовые пространства.

- **OpenCV** считывает изображения в формате **BGR** (Blue-Green-Red).
- **YOLOv5** (как и большинство ML-моделей) обучалась на изображениях в формате **RGB**.

Хотя высокоуровневые методы model(image_path) могут обрабатывать конвертацию автоматически, при передаче numpy-массивов (кадров из read()) необходимо убедиться в корректности цветопередачи. В последних версиях Ultralytics реализован слой Autoshape, который часто нивелирует эту проблему, но явная конвертация cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) является признаком хорошего тона и защищает от ошибок детекции.

## **4\. Низкоуровневая обработка результатов (The Hunt Results)**

Главное «озарение» квеста заключается в отказе от использования готового метода .render() и ручной обработке тензоров.

### **4.1 Структура выходного тензора**

При вызове results \= model(frame), модель возвращает объект, содержащий предсказания. Доступ к «сырым» данным осуществляется через атрибут .xyxy (для первого изображения в батче). Этот тензор имеет размерность (N, 6), где N — количество обнаруженных объектов.  
Каждая строка тензора содержит 6 значений:

| Индекс | Обозначение | Тип данных | Описание                                                          |
| :----- | :---------- | :--------- | :---------------------------------------------------------------- |
| 0      | x\_{min}    | Float      | Координата левого верхнего угла по оси X (пиксели)                |
| 1      | y\_{min}    | Float      | Координата левого верхнего угла по оси Y (пиксели)                |
| 2      | x\_{max}    | Float      | Координата правого нижнего угла по оси X (пиксели)                |
| 3      | y\_{max}    | Float      | Координата правого нижнего угла по оси Y (пиксели)                |
| 4      | conf        | Float      | Уверенность модели (от 0.0 до 1.0)                                |
| 5      | cls         | Float      | Идентификатор класса (целое число, напр. 0 \- человек, 15 \- кот) |

_Таблица 2\. Структура тензора детекции YOLOv5._

### **4.2 Извлечение и фильтрация**

Процесс обработки включает итерацию по строкам этого тензора. Критически важно перенести данные с GPU (если используется) на CPU и конвертировать их в формат numpy для работы с OpenCV: detections \= results.xyxy.cpu().numpy()  
Для каждого объекта мы выполняем следующие действия:

1. **Приведение типов:** Координаты (x, y) должны быть целыми числами (int), так как пиксельная сетка дискретна.
2. **Декодирование класса:** Число 15 ничего не говорит человеку. Мы используем атрибут model.names, который представляет собой словарь или список, сопоставляющий ID класса с его строковым названием (например, model.names вернет 'cat').
3. **Фильтрация:** Хотя модель уже применяет пороговое отсечение (confidence threshold) внутри NMS, мы можем добавить дополнительную логику. Например, отображать только котов с уверенностью выше 0.5.

### **4.3 Ритуал отрисовки (Visualization)**

Согласно условиям квеста, мы должны нарисовать **ярко-зеленую рамку**. В цветовом пространстве BGR зеленый цвет кодируется как (0, 255, 0). Мы используем функцию cv2.rectangle для рамки и cv2.putText для подписи.  
`# Пример логики отрисовки`  
`color = (0, 255, 0) # Ярко-зеленый (B, G, R)`  
`cv2.rectangle(frame, (x1, y1), (x2, y2), color, thickness=2)`  
`label = f"{class_name} {conf:.2f}"`  
`cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)`

_Ссылка на методы отрисовки:._  
Эта часть работы позволяет инженеру «почувствовать» данные: увидеть, как дрожит уверенность модели от кадра к кадру, как меняются координаты при движении объекта, и понять природу шума в задачах компьютерного зрения.

## **5\. Ответ Техноманту: Теория Отслеживания Объектов (Object Tracking)**

Вопрос «Техноманта» о памяти модели открывает дверь в обширную область Object Tracking. Как было отмечено, YOLO — это детектор, он не обладает состоянием. Для реализации «памяти» используется парадигма **Tracking-by-Detection** (Трекинг через Детекцию).

### **5.1 Алгоритм SORT: Математическая магия**

Алгоритм SORT (Simple Online and Realtime Tracking) является де\-факто стандартом для эффективного трекинга в реальном времени. Он очень легок и добавляет минимальную задержку к работе YOLOv5n. SORT базируется на двух ключевых компонентах: Фильтре Калмана и Венгерском алгоритме.

### **5.2 Компонент 1: Фильтр Калмана (Оценка состояния)**

Фильтр Калмана — это рекурсивный алгоритм, который оценивает состояние динамической системы (движущегося объекта) на основе зашумленных измерений (детекций YOLO). Состояние объекта описывается вектором \\mathbf{x}:  
Где (u, v) — центр рамки, s — площадь, r — соотношение сторон, а переменные с точкой (\\dot{u}, \\dots) — их скорости изменения.  
Работа фильтра состоит из двух этапов:

1. **Предсказание (Prediction):** До обработки нового кадра фильтр предсказывает, где _должен_ находиться объект, основываясь на его предыдущей скорости. Это создает априорную оценку (Prior).
2. **Коррекция (Update):** Когда YOLO находит объект, фильтр сравнивает предсказание с фактической детекцией и корректирует свое состояние. Если детекция отсутствует (например, объект перекрыт), фильтр может продолжать предсказывать положение на основе инерции.

### **5.3 Компонент 2: Венгерский алгоритм (Ассоциация данных)**

На каждом кадре у нас есть набор предсказанных треков (от Калмана) и набор новых детекций (от YOLO). Нам нужно понять, какая детекция какому треку соответствует. Это классическая задача о назначениях.

1. **Матрица стоимости:** Мы вычисляем метрику IoU (Intersection over Union) между каждым предсказанным боксом и каждым обнаруженным боксом. Матрица стоимости заполняется значениями 1 \- IoU.
2. **Оптимизация:** Венгерский алгоритм (функция linear_assignment в библиотеках типа scipy или lap) находит такую комбинацию пар «Трек-Детекция», которая минимизирует общую стоимость (максимизирует общее перекрытие).

### **5.4 Ограничения SORT и переход к DeepSORT**

SORT опирается только на геометрию и движение. Если два объекта пересекаются, или если объект исчезает за препятствием (окклюзия) на длительное время, фильтр Калмана теряет его (неопределенность растет), и трек удаляется. При появлении объекта снова ему присваивается новый ID (cat_2). **DeepSORT** решает эту проблему, добавляя внешний вид (Appearance) в уравнение. Он использует небольшую нейросеть для извлечения вектора признаков (embedding) из содержимого рамки. При ассоциации учитывается не только близость координат, но и косинусное расстояние между визуальными признаками. Это позволяет узнать того же кота, даже если он вышел из\-за дивана через 5 секунд.

## **6\. Бизнес-ценность: От пикселей к прибыли**

Детекция и трекинг объектов являются фундаментом для множества высокотехнологичных отраслей. В данном разделе мы подробно рассмотрим, как технологии «Квеста 18.1» конвертируются в экономическую выгоду.

### **6.1 Розничная торговля (Retail Analytics)**

Ритейл использует компьютерное зрение для глубокой аналитики поведения покупателей и оптимизации процессов.

- **Тепловые карты (Heatmaps):** Используя координаты центров объектов во времени (трекинг), можно строить карты активности в магазине. Это позволяет выявить «мертвые зоны» и популярные витрины, оптимизируя выкладку товаров (планограммы).
- **Кассы без кассиров (Autonomous Checkout):** Системы вроде Amazon Go используют сложный мульти-камерный трекинг. Детекция определяет товар (например, бутылка воды), а трекинг привязывает этот товар к виртуальной корзине конкретного человека (User ID). Без надежного трекинга невозможно отличить, кто именно взял товар с полки, если рядом стоят два человека.
- **Предотвращение потерь (Loss Prevention):** Алгоритмы обучаются на распознавание паттернов краж (например, сокрытие товара в одежде). Исследования показывают, что внедрение таких систем снижает кражи на 41%.

### **6.2 Автономный транспорт (Autonomous Vehicles)**

Для беспилотных автомобилей детекция — это вопрос жизни и смерти.

- **Предсказание намерений:** Просто обнаружить пешехода недостаточно. Трекинг (фильтр Калмана) позволяет вычислить вектор его скорости и предсказать траекторию. Если вектор пешехода пересекается с вектором движения автомобиля через 2 секунды, система экстренного торможения активируется _заранее_.
- **Стабильность восприятия:** Окклюзии (перекрытия) происходят на дороге постоянно (пешеход за припаркованной машиной). Трекер позволяет автомобилю «помнить» о наличии объекта, даже если сенсоры его временно не видят, предотвращая опасные маневры.

### **6.3 Медицина и Здравоохранение**

- **Анализ опухолей:** В радиологии (CT/MRI) детекция помогает локализовать патологии. Трекинг используется при анализе серии снимков (динамика во времени) или при 3D-реконструкции органов, обеспечивая точность измерений роста или уменьшения опухоли, недоступную при ручном анализе.
- **Мониторинг пациентов:** В палатах интенсивной терапии или домах престарелых системы детекции позы (pose estimation) и трекинга могут автоматически фиксировать падения (резкое изменение Y-координаты и ориентации бокса) и вызывать медперсонал.

### **6.4 Безопасность (Physical Security)**

- **Детекция праздношатания (Loitering):** Детектор видит человека. Трекер знает, что ID \#45 находится в зоне банкомата уже 10 минут без совершения операций. Это событие классифицируется как подозрительное.
- **Оставленные предметы:** Если трекер фиксирует, что объект (сумка) отделился от «родительского» трека (человека) и остается неподвижным длительное время после ухода владельца, генерируется тревога антитеррористической безопасности.

## **7\. Практическая реализация: Код Квеста**

В этом разделе представлен синтезированный код quest_18_1.py, объединяющий все рассмотренные концепции для выполнения задачи.  
`import torch`  
`import cv2`  
`import os`  
`import numpy as np`

`def run_quest():`  
 `# 1. Подготовка папки для артефактов`  
 `output_dir = 'hunt_results'`  
 `if not os.path.exists(output_dir):`  
 `os.makedirs(output_dir)`

    `# 2. Призыв Химеры (Загрузка модели)`
    `print("Призыв Химеры (YOLOv5n)...")`
    `# Загружаем модель с PyTorch Hub. 'device' выбирается автоматически (GPU если есть)`
    `model = torch.hub.load('ultralytics/yolov5', 'yolov5n', pretrained=True)`

    `# Настройка параметров охоты`
    `model.conf = 0.4  # Игнорировать слабые сигналы (<40% уверенности)`

    `# 3. Открытие магической кинопленки`
    `# В реальном сценарии замените 'video.mp4' на путь к файлу`
    `video_path = 'magic_cinema.mp4'`
    `if not os.path.exists(video_path):`
        `print(f"Ошибка: Файл {video_path} не найден. Используйте веб-камеру (0).")`
        `video_path = 0 # Fallback на веб-камеру`

    `cap = cv2.VideoCapture(video_path)`

    `frame_count = 0`
    `artifacts_collected = 0`
    `max_artifacts = 10`

    `print("Начало охоты...")`

    `while cap.isOpened():`
        `ret, frame = cap.read()`
        `if not ret:`
            `break # Конец пленки`

        `# 4. Взгляд Химеры (Инференс)`
        `# Важно: YOLO ожидает RGB, OpenCV дает BGR.`
        `# Autoshape модели обычно справляется, но для точности можно конвертировать:`
        `# img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)`
        `results = model(frame)`

        `# 5. Анализ отчета (Парсинг тензоров)`
        `# Получаем таблицу детекций: [x1, y1, x2, y2, conf, cls]`
        `#.xyxy берет первый кадр из батча`
        `#.cpu().numpy() переносит данные в оперативную память для OpenCV`
        `detections = results.xyxy.cpu().numpy()`

        `# 6. Ритуал начертания (Визуализация)`
        `for det in detections:`
            `x1, y1, x2, y2, conf, cls = det`

            `# Приведение координат к целым числам (пикселям)`
            `x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)`

            `# Получение имени класса (например, 'cat')`
            `label_name = model.names[int(cls)]`

            `# Формирование подписи: "cat 0.85"`
            `label_text = f"{label_name} {conf:.2f}"`

            `# Отрисовка Ярко-Зеленой Рамки (BGR: 0, 255, 0)`
            `cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 3)`

            `# Отрисовка фона для текста (для читаемости)`
            `t_size = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)`
            `cv2.rectangle(frame, (x1, y1 - t_size - 5), (x1 + t_size, y1), (0, 255, 0), -1)`

            `# Отрисовка текста (Белый текст на зеленом фоне)`
            `cv2.putText(frame, label_text, (x1, y1 - 5),`
                        `cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)`

        `# 7. Сохранение артефактов`
        `# Сохраняем первые 10 кадров с результатами`
        `if artifacts_collected < max_artifacts:`
            `file_name = f"{output_dir}/artifact_{artifacts_collected}.jpg"`
            `cv2.imwrite(file_name, frame)`
            `artifacts_collected += 1`
            `print(f"Артефакт сохранен: {file_name}")`

        `# Отображение (опционально, нажмите 'q' для выхода)`
        `cv2.imshow('Chimera Vision', frame)`
        `if cv2.waitKey(1) & 0xFF == ord('q'):`
            `break`

        `frame_count += 1`

    `# Завершение ритуала`
    `cap.release()`
    `cv2.destroyAllWindows()`
    `print("Охота завершена.")`

`if __name__ == "__main__":`  
 `run_quest()`

### **7.1 Пояснения к коду**

- **torch.hub.load**: Ключевой метод интеграции. Загружает веса yolov5n.pt из кэша или интернета.
- **Координатная арифметика**: YOLO может возвращать координаты, выходящие за границы изображения (например, \-5). OpenCV обрабатывает это корректно, но в строгих системах требуется clip координат.
- **Цвет (0, 255, 0\)**: В стандарте BGR это чистый зеленый канал, что соответствует требованию «ярко-зеленая рамка».
- **hunt_results**: Скрипт автоматически создает папку, удовлетворяя требованию к организации файловой системы.

## **8\. Заключение**

Выполнение Квеста 18.1 демонстрирует фундаментальный переход в компетенциях инженера компьютерного зрения: от потребителя готовых API к архитектору систем обработки данных. Мы деконструировали работу модели YOLOv5n, научились напрямую взаимодействовать с выходными тензорами и визуализировать результаты.  
Однако, как показал анализ вопроса «Техноманта», детекция — это лишь первый шаг. Истинный интеллект системы проявляется в способности связывать разрозненные моменты времени в единую картину реальности. Интеграция алгоритмов трекинга (SORT/DeepSORT) превращает набор координат в траектории, а траектории — в понимание поведения. Именно эта связка — быстрого нейросетевого детектора и надежного математического трекера — лежит в основе современных автономных систем, от умных магазинов до беспилотных автомобилей. Овладение этими «ритуалами» открывает путь к созданию решений, способных не просто видеть мир, но и понимать его динамику.

## **Приложение А: Структура тензора YOLOv5 (Детальная)**

В режиме model.eval() выходной тензор имеет вид:

| Индекс     | Имя поля         | Описание                      | Формат  | Примечание                                |
| :--------- | :--------------- | :---------------------------- | :------ | :---------------------------------------- |
| **0**      | x_center / x_min | Координата X                  | Float32 | Зависит от формата вывода (xywh или xyxy) |
| **1**      | y_center / y_min | Координата Y                  | Float32 | Зависит от формата вывода                 |
| **2**      | width / x_max    | Ширина / Координата X         | Float32 | \-                                        |
| **3**      | height / y_max   | Высота / Координата Y         | Float32 | \-                                        |
| **4**      | confidence       | Доверительная вероятность     | Float32 | Произведение P(obj) \\times IoU           |
| **5...84** | class_probs      | Вероятности классов (One-hot) | Float32 | Только в сыром выходе до NMS              |

_После применения NMS (Non-Maximum Suppression), который встроен в PyTorch Hub модель по умолчанию, формат упрощается до 6 колонок (N, 6), описанных в разделе 4.1._

#### **Источники**

1\. Artificial intelligence in radiology \- PMC \- PubMed Central \- NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC6268174/ 2\. AI-Powered Object Detection in Radiology: Current Models, Challenges, and Future Direction \- PMC \- NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC12112695/ 3\. YOLOv5 Quickstart \- Ultralytics YOLO Docs, https://docs.ultralytics.com/yolov5/quickstart\_tutorial/ 4\. How to Train YOLO v5 on a Custom Dataset | DigitalOcean, https://www.digitalocean.com/community/tutorials/train-yolov5-custom-data 5\. YOLOv5 vs YOLOv6 vs YOLOv7: Comparison of YOLO Models on Speed and Accuracy | CPU & GPU \- Learn OpenCV, https://learnopencv.com/performance-comparison-of-yolo-models/ 6\. Reference for ultralytics/engine/results.py, https://docs.ultralytics.com/reference/engine/results/ 7\. how do we extract the xyxy coordinates of the yolov5 detection image and crop it?, https://stackoverflow.com/questions/74928034/how-do-we-extract-the-xyxy-coordinates-of-the-yolov5-detection-image-and-crop-it 8\. SORT Explained: Real-Time Object Tracking in Python \- Roboflow Blog, https://blog.roboflow.com/sort-explained-real-time-object-tracking-in-python/ 9\. An Analysis of Kalman Filter based Object Tracking Methods for Fast-Moving Tiny Objects, https://arxiv.org/html/2509.18451v1 10\. Ultralytics YOLOv5, https://docs.ultralytics.com/models/yolov5/ 11\. YOLOv5n Nano Study · ultralytics yolov5 · Discussion \#5027 \- GitHub, https://github.com/ultralytics/yolov5/discussions/5027 12\. Number of model parameters and FLOPs based on Ultralytics · Issue \#12254 \- GitHub, https://github.com/ultralytics/ultralytics/issues/12254 13\. What is YOLOv5? A Guide for Beginners. \- Roboflow Blog, https://blog.roboflow.com/yolov5-improvements-and-evaluation/ 14\. Loading YOLOv5 from PyTorch Hub \- Ultralytics YOLO Docs, https://docs.ultralytics.com/yolov5/tutorials/pytorch\_hub\_model\_loading/ 15\. YOLOv5 in PyTorch \> ONNX \> CoreML \> TFLite \- GitHub, https://github.com/ultralytics/yolov5 16\. YOLOv5 \- PyTorch, https://pytorch.org/hub/ultralytics\_yolov5/ 17\. PyTorch YoloV5 but with different results (Under the same environement). \#7627 \- GitHub, https://github.com/ultralytics/yolov5/discussions/7627 18\. Bad results using OpenCV with Yolov5 comparing with pure Yolov5 \- Stack Overflow, https://stackoverflow.com/questions/74209769/bad-results-using-opencv-with-yolov5-comparing-with-pure-yolov5 19\. Rgb vs BGr issue in yolov8 \#9912 \- GitHub, https://github.com/ultralytics/ultralytics/issues/9912 20\. Object Detection Using YOLOv5 From Scratch With Python | Computer Vision \- Medium, https://medium.com/@KaziMushfiq1234/object-detection-using-yolov5-from-scartch-with-python-computer-vision-cfb6b65f540b 21\. how I acces to Class name ? · Issue \#5294 · ultralytics/yolov5 \- GitHub, https://github.com/ultralytics/yolov5/issues/5294 22\. How to update class names after model has already been trained · Issue \#3563 · ultralytics/yolov5 \- GitHub, https://github.com/ultralytics/yolov5/issues/3563 23\. Drawing Functions in OpenCV, https://docs.opencv.org/4.x/dc/da5/tutorial\_py\_drawing\_functions.html 24\. Python OpenCV | cv2.rectangle() method \- GeeksforGeeks, https://www.geeksforgeeks.org/python/python-opencv-cv2-rectangle-method/ 25\. How do I draw bounding boxes from "results.xyxy\[0\]" with cv2 rectangle (YOLOv5)?, https://stackoverflow.com/questions/69601364/how-do-i-draw-bounding-boxes-from-results-xyxy0-with-cv2-rectangle-yolov5 26\. abewley/sort: Simple, online, and realtime tracking of multiple objects in a video sequence. \- GitHub, https://github.com/abewley/sort 27\. SORT: Simple Online and Realtime Tracking | Luffca, https://www.luffca.com/2023/04/multiple-object-tracking-sort/ 28\. The Kalman Filter: Limitations & Future Prospects \- Nordic Inertial, https://www.nordicinertial.com/The+Kalman+Filter+Limitations++Future+Prospects 29\. Improving Kalman Filter-based Multi-Object Tracking in Occlusion and Non-linear Motion \- CMU Robotics Institute, https://www.ri.cmu.edu/app/uploads/2024/03/MSR\_thesis.pdf 30\. Object Detection & Tracking With Yolov8 and Sort Algorithm. | by Mosesdaudu \- Medium, https://medium.com/@mosesdaudu001/object-detection-tracking-with-yolov8-and-sort-algorithm-363be8bc0806 31\. Understanding Multiple Object Tracking using DeepSORT \- Learn OpenCV, https://learnopencv.com/understanding-multiple-object-tracking-using-deepsort/ 32\. Sort and Deep-SORT Based Multi-Object Tracking for Mobile Robotics: Evaluation with New Data Association Metrics \- MDPI, https://www.mdpi.com/2076-3417/12/3/1319 33\. Multi-Object Tracking with DeepSORT \- MATLAB & Simulink \- MathWorks, https://www.mathworks.com/help/vision/ug/multi-object-tracking-with-deepsort.html 34\. 9 use cases of computer vision in retail \- Lumenalta, https://lumenalta.com/insights/9-use-cases-of-computer-vision-in-retail 35\. Top 11 Use Cases of Computer Vision in the Retail Industry \- ScalaCode, https://www.scalacode.com/blog/computer-vision-in-retail/ 36\. Computer Vision in Retail: Use Cases, Benefits and Applications \- Software Mind, https://softwaremind.com/blog/computer-vision-in-retail-use-cases-benefits-and-applications/ 37\. Shrink is Retail's Biggest Problem. Computer Vision is the Solution | Trigo Retail, https://www.trigoretail.com/the-shrink-epidemic-is-retails-biggest-problem-computer-vision-is-the-solution/ 38\. Crack down on retail inventory shrinkage with computer vision \- Centific, https://centific.com/blog/crack-down-on-retail-inventory-shrinkage-with-computer-vision 39\. A complete overview of Object Tracking Algorithms in Computer Vision & Self-Driving Cars, https://www.thinkautonomous.ai/blog/object-tracking/ 40\. Exploring The Real-world Applications Of Object Detection Technology \- Axonator, https://axonator.com/blog/applications-of-object-detection/ 41\. Exploring the Role of Object Detection in Machine Vision \- UnitX, https://www.unitxlabs.com/object-detection-machine-vision-system-automation-quality-control/ 42\. Object Detection and Its Real-World Applications \- Finance, Tech & Analytics Career Resources | Imarticus Blog, https://imarticus.org/blog/object-detection-applications-and-rea-world-use-cases/ 43\. Understanding the Accuracy of AI in Diagnostic Imaging \- RamSoft, https://www.ramsoft.com/blog/accuracy-of-ai-diagnostics 44\. The Effect of Image Resolution on Deep Learning in Radiography \- RSNA Journals, https://pubs.rsna.org/doi/abs/10.1148/ryai.2019190015 45\. AI-Powered Object Detection API: Best Business Applications in 2025 \- Medium, https://medium.com/@API4AI/ai-powered-object-detection-api-best-business-applications-in-2025-4911ed99e1fa 46\. 7 Object Detection Use Cases for Enterprises \- alwaysAI, https://alwaysai.co/blog/7-object-detection-uses-cases
