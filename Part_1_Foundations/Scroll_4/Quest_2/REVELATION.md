# **Исчерпывающий анализ методов обнаружения движения: от межкадровой разности до оптического потока в среде OpenCV**

## **Введение в алгоритмическое восприятие движения**

В современном ландшафте компьютерного зрения способность системы воспринимать и интерпретировать динамику сцены является фундаментальным требованием для широкого спектра приложений — от автоматизированных систем видеонаблюдения и мониторинга дорожного трафика до навигации автономных роботов и интерактивных интерфейсов. Квест 4.2, ставящий задачу обнаружения движения посредством вычисления разницы между кадрами и применения техник вычитания фона с использованием библиотеки OpenCV, затрагивает глубинные принципы цифровой обработки сигналов. Движение в видеопотоке, по своей сути, представляет собой временнýю вариацию пространственного распределения интенсивности пикселей. Однако простая констатация факта изменения значений пикселей недостаточна для построения робастных систем; необходимо различать _семантически значимое движение_ (перемещение объектов интереса) и _паразитные шумы_ (изменения освещения, колебания камеры, цифровой шум сенсора).  
Данный отчет представляет собой всестороннее исследование методологий обнаружения движения, начиная с базовых алгоритмов межкадровой разности (Frame Differencing) и заканчивая сложными адаптивными моделями вычитания фона (Background Subtraction) и методами оптического потока (Optical Flow). Особое внимание уделяется реализации данных алгоритмов в среде OpenCV, анализу их устойчивости к воздействию внешней среды, а также стратегиям компенсации эго-движения камеры (ego-motion), которое представляет собой одну из наиболее сложных проблем в мобильной робототехнике и анализе видео с носимых устройств.

### **1.1 Философия и физика цифрового движения**

С точки зрения сенсора камеры, видеопоток представляет собой трехмерный массив данных I(x, y, t), где x и y — пространственные координаты, а t — индекс времени. "Движение" фиксируется тогда, когда функция интенсивности I изменяется по оси t для фиксированных x и y. Однако, как отмечается в технической литературе, вариации цифровых значений сенсора не всегда коррелируют с физическим движением объектов. Изменения могут быть вызваны:

1. **Квантовым шумом (Shot noise):** Стохастические флуктуации количества фотонов, попадающих на фотодиод, особенно заметные при низкой освещенности.
2. **Глобальным освещением:** Прохождение облака, включение искусственного света или автоматическая регулировка экспозиции камеры вызывают массовое изменение значений пикселей, которое наивные алгоритмы могут интерпретировать как движение огромного объекта.
3. **Компрессионными артефактами:** Алгоритмы сжатия видео (например, H.264) используют блоки макроблоков и предсказание движения, что может порождать "блочные" артефакты, воспринимаемые алгоритмами компьютерного зрения как изменения текстуры.

Таким образом, задача обнаружения движения трансформируется из простого сравнения чисел в задачу статистического моделирования и фильтрации сигналов, где цель состоит в максимизации отношения сигнал/шум, где "сигнал" — это перемещение физического тела.

## **2\. Метод межкадровой разности (Frame Differencing)**

Метод межкадровой разности является наиболее интуитивным и вычислительно эффективным способом регистрации динамики в сцене. В его основе лежит предположение, что фон стационарен, и любое существенное отклонение текущего кадра от предыдущего свидетельствует о наличии движущегося объекта.

### **2.1 Математическая формулировка и реализация в OpenCV**

Основная операция заключается в вычислении абсолютной разности между интенсивностью пикселей текущего кадра Frame_t и предыдущего кадра Frame\_{t-1}. В библиотеке OpenCV для этой цели используется функция cv2.absdiff. Критически важно использовать именно абсолютную разность, а не простое вычитание. В 8-битных изображениях (формат uint8), которые являются стандартом для видеопотоков, значения ограничены диапазоном $$. При обычном вычитании отрицательные результаты (когда текущий пиксель темнее предыдущего) будут либо обрезаны до нуля, либо вызовут переполнение (wrap-around), что приведет к некорректным данным.  
Формула вычисления разностной карты D(x, y) выглядит следующим образом:  
Реализация этого метода требует сохранения копии предыдущего кадра в памяти. На первом шаге итерации, когда предыдущий кадр отсутствует, он инициализируется текущим кадром, что приводит к нулевой разности (черный экран). Это явление "черного изображения" при идентичных кадрах является нормальным поведением алгоритма absdiff и служит индикатором отсутствия изменений.

### **2.2 Проблема апертуры и эффект "призрака"**

Использование непосредственно предыдущего кадра (t-1) в качестве референсного имеет специфические последствия для морфологии обнаруженного объекта. Поскольку сравниваются два состояния движущегося объекта, область перекрытия (где объект находился и в момент t-1, и в момент t) может иметь нулевую разность, если текстура объекта однородна. Это приводит к тому, что детектируются только _края_ объекта — передняя кромка (где объект перекрыл фон) и задняя кромка (где фон открылся). Внутренняя часть объекта остается "прозрачной" для детектора. Этот феномен, известный как "проблема апертуры" в широком смысле, делает метод простой межкадровой разности (consecutive frame difference) непригодным для задач, где требуется выделить полный силуэт объекта.  
Альтернативой является использование фиксированного опорного кадра (обычно самого первого кадра видеопотока, Frame_0), который, как предполагается, содержит чистый фон без объектов. В этом случае формула преобразуется в:  
Этот подход позволяет выделить весь силуэт движущегося объекта. Однако он страдает от проблемы "призраков" (ghosting): если предмет мебели (часть фона) будет передвинут, алгоритм будет вечно детектировать движение в двух местах — в новой позиции предмета (так как она отличается от Frame_0) и в старой позиции (так как там теперь открылся фон, отличный от предмета в Frame_0). Кроме того, любые изменения глобального освещения делают статический опорный кадр неактуальным, приводя к ложному срабатыванию детектора по всему кадру.

### **2.3 Трехкадровая разность**

Для повышения точности и подавления шумов часто применяется метод трехкадровой разности. В этом сценарии вычисляются две разностные маски: \\Delta_1 \= |Frame_t \- Frame\_{t-1}| и \\Delta_2 \= |Frame\_{t-1} \- Frame\_{t-2}|. Результирующая маска движения получается путем логического умножения (операция AND) этих двух разностей: M \= \\Delta_1 \\land \\Delta_2. Это позволяет отфильтровать изменения, которые не коррелируют во времени, и более четко локализовать актуальное положение движущегося объекта, уменьшая шлейф "призрака" за ним.

## **3\. Предварительная обработка и цветовые пространства**

Сырые данные с камеры редко пригодны для прямого анализа из\-за присутствия высокочастотного шума и избыточности цветовой информации.

### **3.1 Выбор цветового пространства: Grayscale vs. RGB**

Стандартный видеопоток поступает в формате BGR (Blue-Green-Red). Для задач обнаружения движения использование всех трех цветовых каналов часто является избыточным и вычислительно затратным.

- **Оттенки серого (Grayscale):** Преобразование изображения в одноканальное полутоновое (градации серого) с помощью cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) является стандартной практикой. Это сокращает объем обрабатываемых данных в три раза (с 24 бит на пиксель до 8 бит), что критично для производительности на встраиваемых системах типа Raspberry Pi. Более того, движение обычно хорошо различимо по изменению яркости (Luminance).
- **Ограничения Grayscale:** Если движущийся объект имеет тот же уровень яркости, что и фон, но отличается по цвету (изолюминантность), метод оттенков серого не сможет его обнаружить. В таких редких случаях анализ в цветовом пространстве HSV (Hue, Saturation, Value) с акцентом на канал оттенка (Hue) может дать лучшие результаты, хотя и ценой производительности.
- **Практическая рекомендация:** Для большинства приложений безопасности и мониторинга трафика использование градаций серого является оптимальным балансом между скоростью и точностью.

### **3.2 Пространственная фильтрация: Размытие по Гауссу**

Ключевым этапом подготовки изображения является подавление сенсорного шума. Даже при съемке статической сцены значения пикселей флуктуируют. Если применить absdiff к необработанным кадрам, результирующая маска будет усеяна "солью и перцем" — мелкими точками ложного движения. Для борьбы с этим применяется размытие по Гауссу (cv2.GaussianBlur). Эта операция свертки сглаживает изображение, усредняя значения пикселей с их соседями согласно нормальному распределению.

- **Размер ядра:** Выбор размера ядра (kernel size) имеет решающее значение. В примерах часто используется ядро (21, 21). Такое крупное ядро эффективно подавляет высокочастотный шум и мелкие детали текстуры, оставляя только крупные формы. Это помогает алгоритму игнорировать незначительные изменения (например, легкое дрожание листвы) и фокусироваться на существенных объектах.
- **Влияние на контуры:** Следует учитывать, что сильное размытие "размазывает" границы объекта, делая их менее точными, но более непрерывными, что полезно для последующего этапа поиска контуров.

## **4\. Пороговая обработка (Thresholding): Бинаризация изменений**

После вычисления разностной карты и ее сглаживания мы получаем изображение, где пиксели имеют значения от 0 (нет изменений) до 255 (максимальное изменение). Чтобы принять решение, является ли изменение "движением", необходимо применить пороговое преобразование (Thresholding).

### **4.1 Выбор порогового значения**

Функция cv2.threshold преобразует полутоновую разностную карту в бинарную маску.  
Эмпирически установлено, что пороговое значение T в диапазоне 25–30 является оптимальным для большинства камер видеонаблюдения внутри помещений.

- **Обоснование значения 25:** Значение 25 составляет примерно 10% от динамического диапазона (255). Это достаточно высоко, чтобы отсечь остаточный шум сенсора (который после размытия по Гауссу обычно не превышает 5–10 единиц), но достаточно низко, чтобы уловить движение объектов, которые не имеют радикального контраста с фоном.
- **Тип порога:** Используется cv2.THRESH_BINARY, который превращает все пиксели выше порога в белые (объект), а ниже — в черные (фон).

### **4.2 Адаптивные методы и Otsu**

Существует метод бинаризации Оцу (Otsu's method), который автоматически вычисляет порог на основе гистограммы изображения, предполагая бимодальное распределение (два пика: фон и передний план). Однако в разностных картах гистограмма часто унимодальна (огромный пик в нуле и длинный "хвост" изменений). В таких условиях метод Оцу может давать непредсказуемые результаты, завышая или занижая порог. Поэтому фиксированный порог (Hard Thresholding) часто оказывается более надежным для простых детекторов движения.

## **5\. Вычитание фона (Background Subtraction): Продвинутые модели**

Метод фиксированного опорного кадра, рассмотренный ранее, неспособен адаптироваться к изменениям среды. Техники вычитания фона (Background Subtraction, BS) решают эту проблему, создавая динамическую модель фона, которая обновляется с каждым новым кадром.

### **5.1 Адаптивная модель бегущего среднего**

Простейшая форма адаптивного фона — это взвешенное скользящее среднее (Running Average). Текущая модель фона B_t обновляется согласно формуле:  
где \\alpha — коэффициент обучения (learning rate).

- Малое значение \\alpha делает модель инертной, она медленно реагирует на изменения.
- Большое значение \\alpha позволяет быстро адаптироваться, но может привести к тому, что медленно движущийся объект станет частью фона ("растворится" в нем).

В OpenCV эта концепция реализована в функции cv2.accumulateWeighted. Она позволяет фону "привыкать" к изменениям освещения, например, к постепенному рассвету или закату.

### **5.2 Смесь Гауссиан (MOG2)**

Одним из наиболее мощных алгоритмов, доступных в OpenCV, является cv2.createBackgroundSubtractorMOG2. Этот алгоритм моделирует историю каждого пикселя не одним значением, а смесью (комбинацией) нескольких Гауссовых распределений (Mixture of Gaussians).

- **Мультимодальность:** Это позволяет алгоритму корректно обрабатывать пиксели, которые имеют несколько стабильных состояний. Классический пример — листва дерева, колеблющаяся на ветру. Пиксель попеременно принимает цвет листа и цвет неба. MOG2 запоминает оба этих состояния как "фон", и движение детектируется только если пиксель принимает третье, отличное значение (например, человек прошел перед деревом).
- **Детекция теней:** Важным преимуществом MOG2 является встроенная способность детектировать тени. Тени движущихся объектов перемещаются вместе с ними, и простые методы разности классифицируют их как часть объекта, искажая его форму и размер. MOG2 анализирует хроматические искажения и может помечать тени серым цветом (значение 127), отличая их от основного объекта (255). Это критически важно для точного подсчета объектов или анализа их формы.

### **5.3 Сравнительный анализ методов**

В Таблице 1 представлено сравнение рассмотренных методов выделения движения.

| Характеристика          | Межкадровая разность                           | Статический фон          | MOG2 (Вычитание фона)                |
| :---------------------- | :--------------------------------------------- | :----------------------- | :----------------------------------- |
| **Принцип**             | $                                              | Frame_t \- Frame\_{t-1}  | $                                    |
| **Память**              | Низкая (1 кадр)                                | Низкая (1 кадр)          | Средняя (история распределений)      |
| **Адаптация к свету**   | Нет                                            | Нет (требует сброса)     | Да (автоматическая)                  |
| **Детекция теней**      | Нет                                            | Нет                      | Да                                   |
| **Шум листвы/волн**     | Высокая чувствительность (ложные срабатывания) | Высокая чувствительность | Низкая (моделируется как фон)        |
| **Проблема "призрака"** | Да (краевой эффект)                            | Да (если фон меняется)   | Нет (объект поглощается со временем) |

## **6\. Морфологическая обработка и анализ контуров**

Бинарная маска, полученная после пороговой обработки или вычитания фона, редко бывает идеальной. Она содержит шумы (мелкие белые точки), а сам объект может иметь разрывы ("дыры"). Для устранения этих дефектов применяются морфологические операции.

### **6.1 Дилатация и Эрозия**

- **Дилатация (Dilate):** Операция "наращивания" белых областей. Применяется для "заплавления" дыр внутри движущегося объекта и объединения близко расположенных фрагментов в единый блоб. В OpenCV используется cv2.dilate с определенным количеством итераций (например, 2 итерации). Это делает объект более цельным, что упрощает последующую детекцию.
- **Эрозия (Erode):** Обратная операция, сужающая границы. Полезна для удаления мелкого шума на фоне. Комбинация эрозии и последующей дилатации называется "Открытием" (Opening) и часто используется для очистки маски перед анализом.

### **6.2 Выделение контуров и фильтрация**

Финальным этапом является переход от растровой маски к векторному описанию объектов с помощью функции cv2.findContours.

- **Режим поиска:** Обычно используется флаг cv2.RETR_EXTERNAL, который возвращает только внешние контуры объектов, игнорируя вложенные (например, дырки от бублика). Это логично, так как для детекции движения важен общий силуэт.
- **Аппроксимация:** Флаг cv2.CHAIN_APPROX_SIMPLE сжимает данные контура, оставляя только ключевые точки (вершины), что экономит память.

После нахождения контуров применяется фильтрация по площади (cv2.contourArea). Контуры, площадь которых меньше определенного минимума (например, 500 пикселей для разрешения 640x480), отбрасываются как шум. Для оставшихся контуров вычисляются ограничивающие прямоугольники (cv2.boundingRect), которые затем отрисовываются на оригинальном кадре для визуализации.

## **7\. Вызов Эго-Движения (Ego-Motion): Когда камера движется**

Все вышеописанные методы (Frame Differencing, Background Subtraction) базируются на фундаментальной аксиоме: **камера неподвижна**. Если камера перемещается (панорамирование, наклон, движение на дроне или автомобиле), весь фон смещается. С точки зрения математики разности, это эквивалентно тому, что _каждый_ пиксель изображения изменил свое значение. Маска движения становится полностью белой (или заполняется краями всех статических объектов), делая выделение реально движущихся объектов невозможным.  
Это явление называется эго-движением (ego-motion). Для решения этой задачи требуется компенсация глобального движения.

1. **Оценка глобального движения:** Алгоритм должен вычислить, как сместился фон между кадрами. Это делается путем нахождения ключевых точек на фоне и вычисления матрицы трансформации (Homography или Affine transform).
2. **Варпинг (Warping):** Предыдущий кадр геометрически трансформируется ("искажается") так, чтобы его фон совпал с фоном текущего кадра.
3. **Компенсированная разность:** После выравнивания фонов применяется стандартная разность. В идеале, фон вычитается в ноль, и остаются только объекты, имеющие независимое движение.

Однако, классические методы разности часто пасуют перед сложностью этой задачи, уступая место методам оптического потока.

## **8\. Оптический поток (Optical Flow): Векторный анализ поля скоростей**

Оптический поток представляет собой векторное поле, описывающее видимое движение каждого пикселя между двумя кадрами. В отличие от скалярной разности (которая говорит "здесь что-то изменилось"), оптический поток говорит "этот пиксель сместился на вектор (dx, dy)".

### **8.1 Уравнение оптического потока**

В основе метода лежит предположение о постоянстве яркости (Brightness Constancy Assumption): интенсивность точки объекта не меняется при перемещении за малый промежуток времени dt.  
Раскладывая правую часть в ряд Тейлора и отбрасывая члены высших порядков, мы получаем фундаментальное уравнение оптического потока:  
где f_x, f_y — пространственные градиенты яркости, f_t — временной градиент, а (u, v) — искомый вектор скорости (dx/dt, dy/dt).

### **8.2 Разреженный поток: Метод Лукаса-Канаде (Lucas-Kanade)**

Метод Лукаса-Канаде решает это уравнение для набора характерных точек (углов), предполагая, что поток постоянен в локальной окрестности точки.

- **Применение:** Идеален для трекинга конкретных объектов или точек. Он быстр и эффективен.
- **Ограничение:** Он является "разреженным" (Sparse), то есть не дает информации о движении пикселей между отслеживаемыми точками. Это делает его менее пригодным для выделения плотной маски силуэта объекта.

### **8.3 Плотный поток: Метод Фарнебака (Farneback)**

Для получения полной картины движения используется алгоритм Гуннара Фарнебака (Gunnar Farneback), реализованный в OpenCV как cv2.calcOpticalFlowFarneback. Он вычисляет вектор смещения для _каждого_ пикселя кадра.

- **Полиномиальное разложение:** Алгоритм аппроксимирует окрестность каждого пикселя квадратичными полиномами и анализирует смещение этих полиномов.
- **Визуализация:** Результат обычно визуализируется в цветовом пространстве HSV, где направление вектора кодируется цветом (Hue), а скорость — насыщенностью или яркостью (Value). Это позволяет мгновенно оценить потоки движения в сцене.
- **Преимущество:** Плотный поток позволяет сегментировать движущиеся объекты даже при движущейся камере, так как векторы движения фона будут когерентны (согласованы), а векторы объектов будут выделяться из общего поля.

## **9\. Реализация и стратегии оптимизации**

При реализации описанных алгоритмов на практике, особенно на ограниченных вычислительных ресурсах (Raspberry Pi, IoT устройства), необходимо учитывать ряд технических нюансов.

### **9.1 Управление ресурсами и памятью**

При длительной работе скрипта обнаружения движения критически важно корректно управлять видеопотоком.

- **Освобождение захвата:** Вызов cap.release() и cv2.destroyAllWindows() обязателен при завершении работы для корректного освобождения драйвера камеры.
- **Разрешение кадра:** Обработка видео в FullHD (1920x1080) для детекции движения избыточна. Уменьшение размера кадра (resize) до ширины 500 пикселей сокращает количество операций на порядок без существенной потери качества детекции крупных объектов. Это позволяет достичь реального времени (Real-Time) даже на слабых процессорах.

### **9.2 Сравнение производительности (Benchmarking)**

В Таблице 2 приведено качественное сравнение вычислительной сложности методов.

| Метод                     | Сложность        | Пригодность для Raspberry Pi | Типичный FPS (640x480)             |
| :------------------------ | :--------------- | :--------------------------- | :--------------------------------- |
| **Frame Differencing**    | O(N)             | Отличная                     | \> 60 FPS                          |
| **MOG2 Subtractor**       | O(K \\cdot N)    | Хорошая                      | 20-30 FPS                          |
| **Lucas-Kanade (Sparse)** | O(P \\cdot W^2)  | Хорошая                      | 30-50 FPS (зависит от числа точек) |
| **Farneback (Dense)**     | O(N \\cdot Poly) | Низкая                       | \< 5 FPS                           |

_Где N \- число пикселей, K \- число гауссиан, P \- число точек трекинга._  
Из таблицы видно, что для задач простого обнаружения (охранная система) методы разности и MOG2 являются безальтернативными лидерами по производительности. Плотный оптический поток требует аппаратного ускорения (GPU) для работы в реальном времени.

## **10\. Прикладные сценарии и выводы**

Интеграция рассмотренных методов открывает двери для создания сложных интеллектуальных систем.

### **10.1 Интеллектуальные транспортные системы (ITS)**

Комбинация вычитания фона (для детекции) и разреженного оптического потока (для оценки скорости) позволяет создавать недорогие комплексы мониторинга трафика. Система может считать автомобили, пересекающие виртуальную линию, и классифицировать их по размеру контура.

### **10.2 Охранные системы**

В охранных системах критична минимизация ложных срабатываний. Использование MOG2 с включенной детекцией теней и фильтрацией по площади контура позволяет игнорировать домашних животных или качающиеся ветки, реагируя только на вторжение человека. Зонирование (определение ROI — Region of Interest) позволяет исключить из анализа зоны с постоянным движением (например, вентиляторы).

### **10.3 Заключение**

Квест 4.2 по обнаружению движения с использованием OpenCV демонстрирует эволюцию компьютерного зрения от простой арифметики пикселей до сложного статистического моделирования.

- **Межкадровая разность** — это "рефлекс" системы: быстрый, простой, но подверженный ошибкам.
- **Вычитание фона (MOG2)** — это "память" системы: адаптивная, устойчивая к шумам и теням.
- **Оптический поток** — это "восприятие" системы: дающее понимание направления и скорости, необходимое для навигации.

Выбор конкретного метода всегда является компромиссом между вычислительной стоимостью и требованиями к робастности. Для большинства стационарных задач связка "Gaussian Blur \+ MOG2 \+ Morphological Ops \+ Contour Filtering" представляет собой золотой стандарт индустрии, обеспечивающий надежный результат при минимальных затратах ресурсов. Глубокое понимание каждого этапа этого конвейера — от физики сенсора до топологии контуров — позволяет инженеру превратить простой видеопоток в источник структурированных данных о динамике реального мира.

#### **Источники**

1\. Basic motion detection and tracking with Python and OpenCV \- PyImageSearch, https://pyimagesearch.com/2015/05/25/basic-motion-detection-and-tracking-with-python-and-opencv/ 2\. A Novel Moving Object Detection Algorithm Based on Robust Image Feature Threshold Segmentation with Improved Optical Flow Estimation \- MDPI, https://www.mdpi.com/2076-3417/13/8/4854 3\. Detect and visualize differences between two images with OpenCV Python \- Stack Overflow, https://stackoverflow.com/questions/56183201/detect-and-visualize-differences-between-two-images-with-opencv-python 4\. why does AbsDiff return a black image? (emgu cv) : r/opencv \- Reddit, https://www.reddit.com/r/opencv/comments/1zq5s3/why\_does\_absdiff\_return\_a\_black\_image\_emgu\_cv/ 5\. frame difference correct method edit \- OpenCV Q\&A Forum, https://answers.opencv.org/question/76363/frame-difference-correct-method/ 6\. Very basic motion detection through OpenCV: Frame difference does not appear to be working for me. : r/learnpython \- Reddit, https://www.reddit.com/r/learnpython/comments/10mzu2s/very\_basic\_motion\_detection\_through\_opencv\_frame/ 7\. cv2 returns black image · Issue \#23202 · opencv/opencv \- GitHub, https://github.com/opencv/opencv/issues/23202 8\. Motion Detection: Part 1 \- Frame Differencing \- Medium, https://medium.com/@itberrios6/introduction-to-motion-detection-part-1-e031b0bb9bb2 9\. Frame differencing for motion detection/moving object in image \- MATLAB Answers, https://www.mathworks.com/matlabcentral/answers/535714-frame-differencing-for-motion-detection-moving-object-in-image 10\. DIY surveillance: motion detection with OpenCV and Python | blog.kronis.dev, https://blog.kronis.dev/blog/diy-surveillance-motion-detection-with-opencv-and-python 11\. Moving Object Detection using Frame Differencing with OpenCV \- DebuggerCafe, https://debuggercafe.com/moving-object-detection-using-frame-differencing-with-opencv/ 12\. Detecting motion with OpenCV \- image analysis for beginners \- Towards Data Science, https://towardsdatascience.com/image-analysis-for-beginners-creating-a-motion-detector-with-opencv-4ca6faba4b42/ 13\. Which is the best color space to detect motion of a very small object using opencv, https://stackoverflow.com/questions/35083233/which-is-the-best-color-space-to-detect-motion-of-a-very-small-object-using-open 14\. How do you decide whether to utilize grayscale or colour images as input for computer vision tasks? \- GeeksforGeeks, https://www.geeksforgeeks.org/computer-vision/how-do-you-decide-whether-to-utilize-grayscale-or-colour-images-as-input-for-computer-vision-tasks/ 15\. opencv/python : motion detect weird thresholding \- Stack Overflow, https://stackoverflow.com/questions/14529439/opencv-python-motion-detect-weird-thresholding 16\. Exploring Thresholding Techniques in OpenCV | by Madhavaraju | Oct, 2025 | Medium, https://medium.com/@madhavarajumaddimani/exploring-thresholding-techniques-in-opencv-7e92dcb70502 17\. Image Thresholding using OpenCV, https://opencv.org/blog/image-thresholding-using-opencv/ 18\. Image Thresholding \- OpenCV Documentation, https://docs.opencv.org/4.x/d7/d4d/tutorial\_py\_thresholding.html 19\. Adaptive Thresholding with OpenCV ( cv2.adaptiveThreshold ) \- PyImageSearch, https://pyimagesearch.com/2021/05/12/adaptive-thresholding-with-opencv-cv2-adaptivethreshold/ 20\. \[CV2\] Motion Detection and Tracking in OpenCV: Frame Delta, MOG2, and Optical Flow Explained \- DEV Community, https://dev.to/jarvissan22/blog-cv2-video-and-motion-detection-and-tracking-j4c 21\. Motion Detection using OpenCV \- Scaler Topics, https://www.scaler.com/topics/motion-detection-opencv/ 22\. Moving Object Detection from Moving Camera Using Focus of Expansion Likelihood and Segmentation \- arXiv, https://arxiv.org/html/2507.13628v1 23\. How to compensate 'Ego-Motion' \- Stack Overflow, https://stackoverflow.com/questions/13337486/how-to-compensate-ego-motion 24\. Unsupervised Motion Detection from Optical Flow | Python in Plain English, https://python.plainenglish.io/unsupervised-motion-detection-8b523c53c49b 25\. Optical flow \- Wikipedia, https://en.wikipedia.org/wiki/Optical\_flow 26\. Optical Flow \- OpenCV Documentation, https://docs.opencv.org/3.4/d4/dee/tutorial\_optical\_flow.html 27\. OpenCV \- The Gunnar-Farneback optical flow \- GeeksforGeeks, https://www.geeksforgeeks.org/python/opencv-the-gunnar-farneback-optical-flow/ 28\. Optical Flow with OpenCV: Lucas-Kanade vs. Farneback \- Patsnap Eureka, https://eureka.patsnap.com/article/optical-flow-with-opencv-lucas-kanade-vs-farneback 29\. Optical Flow in OpenCV (C++/Python) | LearnOpenCV \#, https://learnopencv.com/optical-flow-in-opencv/ 30\. Optical Flow (Shi-Tomasi Corner Detection,Sparse(Lucas-kanade, Horn schunck) & Dense(Gunnar Farneback) )-Part I | by Venkatkumar (VK) | Medium, https://medium.com/@VK\_Venkatkumar/optical-flow-shi-tomasi-corner-detection-sparse-lucas-kanade-horn-schunck-dense-gunnar-e1dae9600df 31\. Moving Object Detection under a Moving Camera via Background Orientation Reconstruction \- PMC \- NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC7309005/ 32\. Camera motion estimation using optical flow | by Ivan Kunyankin \- Medium, https://medium.com/@ikunyankin/camera-motion-estimation-using-optical-flow-ce441d7ffec 33\. Optical Flow: Revolutionizing Motion Detection \- Viso Suite, https://viso.ai/deep-learning/optical-flow/ 34\. Transforming Traffic Management with Computer Vision \- Rapid Innovation, https://www.rapidinnovation.io/post/computer-vision-for-real-time-traffic-flow-analysis 35\. Computer Vision Applications in Intelligent Transportation Systems: A Survey \- MDPI, https://www.mdpi.com/1424-8220/23/6/2938 36\. What settings for Motion Area, Motion Threshold and Frame Analysis Interval in OpenCV Motion detection would you use? : r/Scrypted \- Reddit, https://www.reddit.com/r/Scrypted/comments/z4wite/what\_settings\_for\_motion\_area\_motion\_threshold/
