# **Квест 2.1: Ритуал Всеслышащего Уха и Инженерия Цифровой Тишины — Глубокий Технический Анализ**

## **1\. Введение: Пересечение Акустической Магии и Инженерной Реальности**

В современной экосистеме машинного обучения существует класс задач, которые на первый взгляд кажутся тривиальными, но при ближайшем рассмотрении открывают бездну архитектурной сложности. Квест 2.1, сформулированный как "Призыв Уха Всеслышания" через запуск модели whisper-tiny на входном сигнале абсолютной тишины, является эталонным примером такой задачи. За внешней простотой скрипта на Python скрывается сложнейшая динамика взаимодействия вероятностных генеративных моделей с детерминированной природой программных зависимостей.  
Данный отчет представляет собой исчерпывающее исследование, призванное не просто объяснить наблюдаемые феномены, но и деконструировать фундаментальные принципы работы систем автоматического распознавания речи (ASR) класса End-to-End. Мы проанализируем анатомию модели Whisper, природу её "галлюцинаций" в условиях отсутствия сигнала, а также жесткие требования к программному окружению, диктуемые эволюцией библиотек с открытым исходным кодом.  
Актуальность этого исследования обусловлена растущим внедрением ASR-систем в критически важные бизнес-процессы. Понимание того, почему модель "слышит" слова там, где их нет, и почему обновление одной библиотеки может обрушить весь производственный конвейер, отделяет любителя от профессионального инженера ML-систем.

## **2\. Анатомия Духа: Архитектурный Разбор Whisper-Tiny**

Модель whisper-tiny, являющаяся центральным объектом нашего исследования, представляет собой младшую ипостась в семействе моделей, разработанных OpenAI. Обученная на колоссальном объеме данных — 680 000 часов мультитaскингового и мультиязычного аудио — она наследует все архитектурные особенности своих "старших братьев", но в экстремально сжатом формате.

### **2.1. Трансформер как Фундамент Акустического Восприятия**

В отличие от традиционных гибридных систем ASR, которые полагались на скрытые марковские модели (HMM) и гауссовы смеси (GMM), или более поздних рекуррентных сетей (RNN/LSTM) с функцией потерь CTC (Connectionist Temporal Classification), Whisper построена на базе архитектуры Трансформера (Encoder-Decoder Transformer). Это решение кардинально меняет способ обработки временных рядов.

#### **2.1.1. Масштабирование и Параметризация**

Модель tiny содержит всего около 39 миллионов параметров. Для сравнения, флагманская модель large-v3 насчитывает 1.55 миллиарда параметров. Однако, структурно tiny сохраняет полную функциональность, жертвуя лишь глубиной и шириной слоев.  
Детальная конфигурация whisper-tiny выглядит следующим образом:

| Компонент Архитектуры                      | Значение | Комментарий                                                     |
| :----------------------------------------- | :------- | :-------------------------------------------------------------- |
| **Количество слоев Энкодера**              | 4        | Минимальное число слоев для извлечения признаков.               |
| **Количество слоев Декодера**              | 4        | Симметричная структура относительно энкодера.                   |
| **Число голов внимания (Attention Heads)** | 6        | Обеспечивает параллельную обработку различных аспектов сигнала. |
| **Размерность эмбеддинга (d\_{model})**    | 384      | Векторное представление каждого токена/фрейма.                  |
| **Размерность FFN (Feed-Forward Network)** | 1536     | Внутренний слой полносвязной сети в блоке трансформера.         |
| **Контекстное окно**                       | 1500     | Максимальное количество позиций энкодера (30 секунд аудио).     |

Такая компактность (файл весов занимает около 150 МБ на диске ) позволяет запускать модель на устройствах с крайне ограниченными ресурсами, таких как Raspberry Pi или мобильные телефоны, обеспечивая инференс в реальном времени.

### **2.2. Входной Интерфейс: От Волны к Спектрограмме**

Ритуал начинается с создания "магической тишины" — массива нулей np.zeros(16000). Этот шаг имеет глубокий физический и математический смысл.

#### **2.2.1. Стандарт 16 кГц**

Whisper жестко стандартизирует входные данные: аудио должно быть передискретизировано (resampled) до 16 000 Гц. Этот выбор не случаен. Согласно теореме Котельникова (Найквиста-Шеннона), частота дискретизации f_s позволяет восстановить сигнал с частотами до f_s/2, то есть до 8 кГц. Основная энергия человеческой речи сосредоточена в диапазоне от 300 Гц до 3400 Гц, а значимые обертоны простираются до 8 кГц. Использование 16 кГц является золотым стандартом для задач распознавания речи, обеспечивая баланс между качеством сигнала и вычислительной нагрузкой.  
Когда мы подаем массив из 16 000 нулей, мы симулируем ровно одну секунду идеальной цифровой тишины. В реальном мире такая тишина невозможна из\-за теплового шума микрофонов, но в цифровом пространстве это валидный, хоть и вырожденный сигнал.

#### **2.2.2. Логарифмическая Мел-спектрограмма**

Whisper не работает с сырой волной ("raw waveform") напрямую. Сырой сигнал трансформируется в частотно-временное представление — логарифмическую Мел-спектрограмму (Log-Mel Spectrogram). Этот процесс включает несколько этапов:

1. **Оконное преобразование:** Аудио разбивается на окна по 25 мс с шагом 10 мс.
2. **БПФ (STFT):** Применяется быстрое преобразование Фурье для получения спектра мощностей.
3. **Гребенка фильтров Мела:** Спектр проецируется на 80 частотных полос (бинов), распределенных по шкале Мела, которая имитирует нелинейность человеческого слуха.
4. **Логарифмирование:** Амплитуды логарифмируются для сжатия динамического диапазона.

Важный нюанс заключается в нормализации. Модель ожидает, что входные данные будут нормализованы до нулевого среднего и единичной дисперсии в глобальном масштабе обучающего датасета. Однако, при подаче массива нулей, логарифм нуля (log(0)) стремится к минус бесконечности (-\\infty). В реализации WhisperFeatureExtractor используется добавление малого числа (\\epsilon) или клиппинг (обрезка) значений снизу (например, до \-100 дБ). В результате "магическая тишина" превращается в тензор, заполненный одинаковыми минимальными значениями.

### **2.3. Энкодер: Свертка и Внимание**

Первым этапом обработки внутри нейросети является "ствол" (stem) энкодера, состоящий из двух сверточных слоев (Conv1D) с размером ядра 3 и шагом (stride) 1 или 2\. Эти слои выполняют роль обучаемого извлечения признаков, заменяя традиционные ручные методы обработки сигналов.  
Интересное различие в реализациях было отмечено исследователями: оригинальная модель OpenAI и версия Hugging Face имеют незначительные различия в расположении функции активации GeLU. В Hugging Face GeLU применяется после нормализации внимания, тогда как в OpenAI — внутри FFN. Хотя математически результаты схожи, это подчеркивает важность точности при портировании весов.

## **3\. Феномен Галлюцинаций: Почему Тишина Говорит?**

Ожидаемый результат квеста — получение на выходе текста вроде "you", "Thank you", "Subtitle by..." или просто случайного мусора — является не ошибкой ("багом"), а фундаментальным свойством (фичей) вероятностных моделей, обученных методом слабого учителя (weak supervision).

### **3.1. Декодер как Языковая Модель**

Декодер Whisper — это, по сути, авторегрессионная языковая модель (LLM), условленная (conditioned) на выходы энкодера. Его задача — предсказать следующий токен y_t на основе предыдущих токенов y\_{\<t} и скрытых состояний аудио Encoder(x):  
Когда на вход подается тишина, вектор Encoder(x) содержит крайне мало информации (низкая энтропия). Механизм кросс-внимания (Cross-Attention), который должен "смотреть" на соответствующие участки аудио при генерации слова, не находит значимых паттернов. В этот момент компонент Encoder(x) в уравнении становится слабым, и модель начинает полагаться на свои априорные знания — P(y_t | y\_{\<t}).  
Модель переходит в режим "безусловной генерации", пытаясь воспроизвести наиболее вероятные паттерны, которые она видела в обучающей выборке в ситуациях отсутствия четкой речи.

### **3.2. Эффект "YouTube-обучения" и Призраки Амара**

Whisper обучалась на данных из интернета, значительная часть которых — это видео с YouTube, подкасты и фильмы. В таких данных тишина (начало или конец видео, паузы) часто сопровождается специфическими субтитрами.  
Исследования и наблюдения сообщества выделяют кластеры типичных галлюцинаций на тишине:

- **"Thank you for watching" / "Thanks":** Стандартные завершающие фразы видеоблогеров. Модель выучила корреляцию: "Если аудио заканчивается (или тихое) \-\> Вероятно, следует фраза прощания".
- **"Subtitle by Amara.org" / "Captioning provided by...":** Это метаданные, часто вшитые в субтитры волонтерскими организациями. Поскольку модель обучалась методом "слабого контроля" (predicting the transcript given the audio), она не всегда отличает произнесенную речь от метаданных субтитров.
- **"You":** Часто встречающийся артефакт, возможно, являющийся осколком фразы "Thank you" или заполнителя паузы.

### **3.3. Механика Сбоя Внимания**

Недавние исследования проливают свет на механику этого процесса. Было обнаружено, что за галлюцинации в сегментах без речи ответственны конкретные головы внимания (attention heads) в декодере. В модели whisper-large-v3 всего 3 из 20 голов отвечают за более чем 75% галлюцинаций. Эти "безумные головы" (crazy heads) чрезмерно активируются при отсутствии сигнала, запуская генерацию текста.  
В whisper-tiny из\-за малого числа голов (всего 6\) эффект может быть еще более выраженным, так как у модели меньше "здоровых" голов, чтобы компенсировать ошибку "безумных".

### **3.4. Мешок Галлюцинаций (Bag of Hallucinations)**

Исследователи формализовали этот феномен, создав понятие "Bag of Hallucinations" (BoH) — список токенов и фраз, которые статистически чаще появляются при отсутствии речи. К ним относятся слова "the", "so", "thank you" и другие высокочастотные конструкции. Интересно, что простое удаление этих слов невозможно, так как они являются частью нормальной речи. Это подчеркивает сложность борьбы с галлюцинациями: модель не "ошибается" в терминах вероятности, она просто максимизирует правдоподобие на основе смещенных данных.

## **4\. Война Гримуаров: Управление Зависимостями в Python**

Вопрос Техноманта о необходимости "отката" версии библиотеки datasets до 2.16.1 затрагивает одну из самых болезненных тем в современной инженерии ML — управление зависимостями и совместимость экосистем.

### **4.1. Великий Раскол: soundfile против torchcodec**

Долгое время библиотека datasets от Hugging Face использовала связку библиотек soundfile (базируется на libsndfile) и librosa для декодирования аудиофайлов. Это был стабильный, проверенный временем стек, который хорошо работал на большинстве CPU и ОС.  
Однако в версии 2.17.0 произошло тектоническое изменение: библиотека перешла на использование нового бэкенда декодирования по умолчанию, основанного на torchcodec (или подготовила почву для него, изменив логику вызовов).

- **Сниппет** гласит: "Torchcodec decoding... torchcodec replaces soundfile for audio decoding."
- **Сниппет** подтверждает: "Your error started after upgrading to Datasets 4.x... switched audio decoding to TorchCodec."

Этот переход создал конфликт на уровне системных библиотек. Torchcodec часто требует наличия в системе специфических версий FFmpeg (например, версий 4, 5, 6 или 7\) и жесткой привязки к версии PyTorch. В то же время, "старый добрый" soundfile поставляется с прекомпилированными бинарниками (wheels), что делает его гораздо более устойчивым к разным окружениям.

### **4.2. Конфликт с torchaudio и librosa**

Ситуация усугубляется тем, что в нашем "Квесте" мы также используем torchaudio и librosa.

1. **torchaudio:** Имеет свои собственные привязки к FFmpeg и свои механизмы декодирования. Когда datasets (версии 2.17+) пытается использовать свои новые методы декодирования в одном пространстве имен с torchaudio, возникают конфликты символов или несовместимость версий общих библиотек.
2. **librosa:** Традиционно полагается на soundfile. Изменение бэкенда в datasets нарушает неявный контракт: инженер ожидает, что dataset\['audio'\]\['array'\] вернет numpy-массив, полученный через понятный механизм, а получает объект AudioDecoder, требующий специфической обработки или вызывающий ошибку при отсутствии нужных драйверов.

### **4.3. Стратегия "Гармоничных Версий"**

Ответ Мастера о "шаге назад" (downgrade) — это классическая стратегия стабилизации production-окружения. Фиксация версии datasets==2.16.1 гарантирует использование старого, проверенного механизма декодирования через soundfile, который идеально совместим с librosa и текущими версиями torchaudio.  
В профессиональной разработке это называется **Dependency Pinning** (фиксация зависимостей). Использование файлов requirements.txt с жесткими версиями (например, \==2.16.1) или poetry.lock является обязательным стандартом, чтобы избежать "дрейфа" окружения, когда код, работавший вчера, падает сегодня из\-за минорного обновления подзависимости.

## **5\. Бизнес-Контекст: Экономика и Производительность Whisper-Tiny**

Выбор модели whisper-tiny для бизнеса — это всегда компромисс между стоимостью, скоростью и качеством. Анализ рынка показывает четкие ниши для этой модели.

### **5.1. Бенчмаркинг Производительности**

Модель tiny — это "спидстер" семейства Whisper.

- **Скорость Инференса:** Согласно бенчмаркам, tiny работает в 10-32 раза быстрее, чем модель large. На современных CPU она способна обрабатывать аудио быстрее реального времени (Real-Time Factor \< 1.0).
- **Потребление Памяти:** Модель требует менее 1 ГБ VRAM (видеопамяти) , что позволяет запускать её на дешевых инстансах (например, NVIDIA T4) или даже на интегрированной графике.

| Модель       | Параметры | Относительная скорость | VRAM (прибл.) |
| :----------- | :-------- | :--------------------- | :------------ |
| **Tiny**     | 39 M      | \~10x \- 32x           | \~1 GB        |
| **Base**     | 74 M      | \~7x                   | \~1 GB        |
| **Small**    | 244 M     | \~4x                   | \~2 GB        |
| **Medium**   | 769 M     | \~2x                   | \~5 GB        |
| **Large-v3** | 1550 M    | 1x                     | \~10 GB       |

_Таблица 1: Сравнительные характеристики моделей Whisper_

### **5.2. Экономическая Эффективность: API против Self-Hosted**

Вопрос стоимости критичен для масштабирования.

- **OpenAI API:** Стандартная цена составляет около $0.36 за час аудио (исходя из $0.006/мин).
- **Собственный Хостинг (Self-Hosted):** Запуск whisper-tiny на облачном GPU (например, AWS g4dn.xlarge или Lambda Labs) может снизить стоимость транскрибации в 10-20 раз, особенно при пакетной обработке (batch processing). Сниппеты и указывают на возможность транскрибировать тысячи часов аудио за копейки ($0.00059/мин при грамотной архитектуре).
- **Бесплатно (Локально):** Для приложений, работающих на устройстве пользователя (On-Device), стоимость инференса фактически равна нулю для бизнеса (перекладывается на батарею пользователя). Whisper-tiny — одна из немногих моделей, способных работать в таком режиме.

### **5.3. Сценарии Использования (Use Cases)**

1. **Трикотаж и Маршрутизация Звонков:** В колл-центрах tiny может использоваться для мгновенного определения темы звонка (keyword spotting) для маршрутизации, пока более точная (и дорогая) модель делает полный транскрипт для аналитики.
2. **IoT и Smart Devices:** Голосовое управление в умных колонках, где задержка (latency) критичнее, чем идеальная точность редких слов.
3. **Черновая Обработка:** Быстрое создание субтитров для видео, которые затем правятся человеком.

## **6\. Инженерная Надежность: Укрощение Духа**

Понимание того, что Whisper галлюцинирует на тишине, диктует необходимость внедрения дополнительных инженерных слоев защиты. Использовать whisper-tiny "как есть" (out-of-the-box) в продакшене рискованно.

### **6.1. Voice Activity Detection (VAD)**

Самый надежный способ борьбы с галлюцинациями на тишине — не подавать тишину на вход модели. Внедрение пре-процессинга с использованием VAD (например, Silero VAD или Pyannote) позволяет вырезать участки тишины и подавать в Whisper только сегменты с активной речью. Это не только устраняет галлюцинации типа "Thank you", но и значительно ускоряет обработку, так как модели не нужно "переваривать" пустые данные.

### **6.2. Подавление Токенов (Token Suppression)**

Библиотека transformers позволяет передавать параметр suppress_tokens в конфигурацию генерации. Инженер может принудительно запретить модели генерировать определенные фразы, если они не релевантны контексту (например, запретить "Subscribe" для системы медицинской диктовки).

### **6.3. Температура и Пороги**

Настройка параметра temperature (случайность выборки) и использование порогов логарифмической вероятности (logprob_threshold) позволяют отсекать предсказания с низкой уверенностью. Whisper имеет встроенные механизмы, которые увеличивают температуру, если вероятность предсказанных токенов низка, но в случае с тишиной это иногда играет злую шутку, заставляя модель "креативить". Фиксация температуры на 0 (greedy decoding) часто дает более стабильные результаты.

## **7\. Заключение**

Квест "Призыв Уха Всеслышания" — это не просто упражнение в написании кода, а микрокосм реальной работы ML-инженера. Мы увидели, как:

1. **Архитектура определяет поведение:** Трансформер без входа работает как безусловный генератор, воспроизводя biases обучающей выборки.
2. **Данные определяют реальность:** "Галлюцинации" модели — это эхо миллионов YouTube-роликов, на которых она училась.
3. **Экосистема хрупка:** Конфликт версий datasets 2.16 vs 2.17 демонстрирует необходимость жесткого управления зависимостями.

Для успешного применения магии звука недостаточно просто знать заклинание (pipeline(...)). Необходимо понимать физику процесса, психологию модели и законы программной энтропии. Только так можно заставить духа слушать тишину и не слышать в ней призраков.

#### **Источники**

1\. Whisper hallucination \- how to recognize and solve? \- API \- OpenAI Developer Community, https://community.openai.com/t/whisper-hallucination-how-to-recognize-and-solve/218307 2\. Reduce Whisper Hallucination On Non-Speech By Calming Crazy Heads Down \- arXiv, https://arxiv.org/html/2505.12969v1 3\. openai/whisper-large-v3 \- Hugging Face, https://huggingface.co/openai/whisper-large-v3 4\. Introducing Whisper \- OpenAI, https://openai.com/index/whisper/ 5\. Whisper models for automatic speech recognition now available in Amazon SageMaker JumpStart | Artificial Intelligence \- AWS, https://aws.amazon.com/blogs/machine-learning/whisper-models-for-automatic-speech-recognition-now-available-in-amazon-sagemaker-jumpstart/ 6\. openai/whisper: Robust Speech Recognition via Large-Scale Weak Supervision \- GitHub, https://github.com/openai/whisper 7\. Which Whisper Model Should I Choose?, https://whisper-api.com/blog/models/ 8\. Whisper \- Hugging Face, https://huggingface.co/docs/transformers/model\_doc/whisper 9\. Robust Speech Recognition via Large-Scale Weak Supervision \- OpenAI, https://cdn.openai.com/papers/whisper.pdf 10\. Whisper: Functionality and Finetuning | by Okezie Okoye | Medium, https://medium.com/@okezieowen/whisper-functionality-and-finetuning-ba7f9444f55a 11\. A possible solution to Whisper hallucination · openai whisper · Discussion \#679 \- GitHub, https://github.com/openai/whisper/discussions/679 12\. OpenAI's Whisper filled in silent part at the beginning of audio recording (actual audio starts at "OK, great, thank you,") with deranged verbalization. Any understanding why it would do that? \- Reddit, https://www.reddit.com/r/OpenAI/comments/1c3b7e8/openais\_whisper\_filled\_in\_silent\_part\_at\_the/ 13\. A interesting behavior of OpenAI's whisper : r/LocalLLaMA \- Reddit, https://www.reddit.com/r/LocalLLaMA/comments/1fx7ri8/a\_interesting\_behavior\_of\_openais\_whisper/ 14\. Why does it keep transcribing “Thank you for watching\!” when I use voice recorder but am not saying anything? : r/ChatGPT \- Reddit, https://www.reddit.com/r/ChatGPT/comments/15mmraf/why\_does\_it\_keep\_transcribing\_thank\_you\_for/ 15\. Hallucination on audio with no speech · openai whisper · Discussion \#1606 \- GitHub, https://github.com/openai/whisper/discussions/1606 16\. Whisper model predicts "thank you" or "you" on silence \#24512 \- GitHub, https://github.com/huggingface/transformers/issues/24512 17\. Investigation of Whisper ASR Hallucinations Induced by Non-Speech Audio This research was supported by the National Science Centre, Poland under Grant 2021/42/E/ST7/00452, the National Centre for Research and Development, Poland under Grant INFOSTRATEG-IV/0029/2022, and by program ”Excellence initiative – research university” \- arXiv, https://arxiv.org/html/2501.11378v1 18\. Dataset library DatasetGenerationError \- huggingface \- Stack Overflow, https://stackoverflow.com/questions/77139005/dataset-library-datasetgenerationerror 19\. Load audio data \- Hugging Face, https://huggingface.co/docs/datasets/v2.8.0/en/audio\_load 20\. Process audio data — datasets 1.17.0 documentation \- Hugging Face, https://huggingface.co/docs/datasets/v1.17.0/audio\_process.html 21\. Releases · huggingface/datasets \- GitHub, https://github.com/huggingface/datasets/releases 22\. Issue with TorchCodec when fine-tuning Whisper ASR model \- Hugging Face Forums, https://discuss.huggingface.co/t/issue-with-torchcodec-when-fine-tuning-whisper-asr-model/169315 23\. load_dataset() in 4.0.0 failed when decoding audio \#7707 \- GitHub, https://github.com/huggingface/datasets/issues/7707 24\. Conflicting dependencies while installing torch==1.10.0, torchaudio==0.10.0, and torchvision==0.11.0 in my Python environment \- Stack Overflow, https://stackoverflow.com/questions/79327727/conflicting-dependencies-while-installing-torch-1-10-0-torchaudio-0-10-0-and 25\. Dependency Conflict When Installing Unsloth with Torch 2.2.0 · Issue \#1136 \- GitHub, https://github.com/unslothai/unsloth/issues/1136 26\. How to access path to audio recording in datasets 4.0? \- Hugging Face Forums, https://discuss.huggingface.co/t/how-to-access-path-to-audio-recording-in-datasets-4-0/168516 27\. edinburghcstr/ami · Dataset is not compatible with latest dataset release \- Hugging Face, https://huggingface.co/datasets/edinburghcstr/ami/discussions/5 28\. What is the Best Speech-to-Text Models Available and Which GPU Should I Deploy it on? | AI FAQ | Jarvis Labs, https://jarvislabs.ai/ai-faqs/what-is-the-best-speech-to-text-model-available-and-which-gpu-should-i-deploy-it-on 29\. API model whisper \- Real cost \- OpenAI Developer Community, https://community.openai.com/t/api-model-whisper-real-cost/469816 30\. \[P\] Whisper Large Benchmark: 137 DAYS of Audio Transcribed in 15 Hours for Just $117 ($0.00059/min) : r/MachineLearning \- Reddit, https://www.reddit.com/r/MachineLearning/comments/16ftd9v/p\_whisper\_large\_benchmark\_137\_days\_of\_audio/ 31\. Whisper Vs Chirp: The Hidden GPU Cost Of “Free” AI Models & Why Commercial Hosted Models Can Be Far Cheaper \- The GDELT Project, https://blog.gdeltproject.org/whisper-vs-chirp-the-hidden-gpu-cost-of-free-ai-models-why-commercial-hosted-models-can-be-far-cheaper/ 32\. openai/whisper · Distinguishing between speech and non speech \- Hugging Face, https://huggingface.co/spaces/openai/whisper/discussions/74 33\. Careless Whisper: Speech-to-Text Hallucination Harms \- ACM FAccT, https://facctconference.org/static/papers24/facct24-111.pdf 34\. How to avoid Hallucinations in Whisper transcriptions? \- OpenAI Developer Community, https://community.openai.com/t/how-to-avoid-hallucinations-in-whisper-transcriptions/125300?page=2
