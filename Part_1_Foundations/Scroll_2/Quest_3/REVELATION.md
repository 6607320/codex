# **Визуальная природа звука: Мел-спектрограммы как мост между цифровой обработкой сигналов и компьютерным зрением в глубоком обучении**

## **Аннотация**

Данный отчет представляет собой исчерпывающее техническое и теоретическое исследование процесса трансформации одномерных аудиосигналов в двумерные визуальные представления, известные как мел-спектрограммы. Исследование инициировано практической задачей «Квеста 2.3», целью которой является освоение инструментария для визуализации звука с использованием библиотек librosa и matplotlib, однако масштаб анализа выходит далеко за рамки простой генерации изображений. В документе детально рассматривается фундаментальная физика цифрового звука, математический аппарат оконного преобразования Фурье (STFT), психоакустические основы шкалы мел и, что наиболее важно, применение методов компьютерного зрения — в частности, сверточных нейронных сетей (CNN) и трансферного обучения (Transfer Learning) — для решения задач аудиоанализа. Мы доказываем, что современный искусственный интеллект в области аудио фактически «видит» звук, превращая задачи распознавания речи, классификации жанров и диагностики оборудования в задачи анализа текстур и паттернов на спектрограммах.

---

## **1\. Введение: Конвергенция чувств в искусственном интеллекте**

### **1.1. Смена парадигмы: От временных рядов к тензорам изображений**

Исторически обработка аудиосигналов (Digital Signal Processing, DSP) развивалась как дисциплина, сфокусированная на анализе временных рядов. Классические алгоритмы, такие как скрытые марковские модели (HMM) и смеси гауссовых распределений (GMM), рассматривали звук строго как функцию изменения амплитуды во времени. Эти методы, хотя и эффективные для своего времени, обладали ограниченной способностью моделировать сложные иерархические зависимости в данных. Однако революция глубокого обучения, начавшаяся в 2012 году с успехом архитектуры AlexNet в распознавании изображений, создала предпосылки для фундаментального сдвига парадигмы в аудиоанализе. Исследователи осознали, что если звук можно надежно и обратимо отобразить в двумерную матрицу, сохраняющую как временную, так и частотную структуру, то мощнейший инструментарий компьютерного зрения можно будет перепрофилировать для акустических задач.1

Этот отчет посвящен **мел-спектрограмме** — де\-факто стандарту такой конвертации. В отличие от сырой волновой формы (waveform), которая представляет собой график амплитуды от времени, спектрограмма добавляет третье измерение: частоту. Когда ось частот искажается в соответствии со шкалой мел (Mel scale) — перцептивной шкалой, имитирующей нелинейное восприятие высоты тона человеческим ухом, — спектрограмма становится не просто графиком, а биомиметическим слепком слухового восприятия.3 Это преобразование является критическим этапом инженерии признаков (Feature Engineering), который выравнивает представление данных с биологической реальностью слуха, делая их значительно более «понятными» для нейронных сетей, обучающихся на паттернах человеческой речи и звуков окружающей среды.5

### **1.2. Архитектура «Квеста»: От кода к смыслу**

В основе данного исследования лежит практический алгоритм, описанный в условиях задачи («Квест 2.3»), использующий библиотеку librosa для анализа аудиофрагмента из датасета librispeech_asr. Этот алгоритм служит скелетом, на который мы нарастим теоретическую базу. Процесс включает четыре критических акта, каждый из которых представляет собой отдельный слой абстракции:

1. **Аквизиция (Acquisition):** Загрузка сырых аудиоданных в виде одномерного массива чисел с плавающей точкой.
2. **Спектральная трансформация:** Применение оконного преобразования Фурье (STFT) для перехода из временной области в частотную.
3. **Перцептивное взвешивание:** Отображение линейных частот на шкалу мел и амплитуд на логарифмическую шкалу децибел.
4. **Визуальное кодирование:** Генерация двумерной матрицы (изображения), где оси представляют время и частоту, а интенсивность пикселя — энергию сигнала.

В данном отчете мы покажем, что «магия», описанная в легенде квеста — превращение аудиозадач в задачи для зрения, — основана на глубокой математической двойственности времени и частоты и способности сверточных сетей (CNN) извлекать иерархические признаки (от краев до текстур) независимо от природы входных данных.7

---

## **2\. Физика и математика цифрового звука**

Прежде чем анализировать изображение звука, необходимо понять субстрат, из которого оно формируется, — цифровой аудиосигнал. Понимание природы дискретизации и квантования является фундаментом для интерпретации артефактов, которые могут появиться на спектрограмме.

### **2.1. Дискретная временная область**

Звук в физическом мире представляет собой непрерывное изменение давления воздуха (волну сжатия и разрежения). Для цифровой обработки мы обязаны дискретизировать этот непрерывный сигнал. Датасет librispeech_asr, используемый в анализе, предоставляет аудиозаписи с частотой дискретизации 16 000 Гц (16 кГц).9 Этот параметр, обозначаемый как $f\_s$, является критическим. Согласно теореме Котельникова-Найквиста-Шеннона, частота дискретизации $f\_s$ позволяет идеально восстановить частоты сигнала вплоть до $f\_s / 2$, известной как частота Найквиста.

Для 16 кГц аудиопотока теоретический потолок захватываемых частот составляет 8 кГц (8000 Гц). Это покрывает подавляющее большинство информации, содержащейся в человеческой речи. Основная энергия гласных и согласных звуков сосредоточена в диапазоне от 300 Гц до 3400 Гц (стандарт телефонии), однако важные обертоны, определяющие тембр голоса, и высокочастотные шумные согласные (фрикативы, такие как «с» или «ф») простираются выше, вплоть до 8–10 кГц и далее.11 Ограничение в 8 кГц означает, что «воздух» и сверхвысокие гармоники будут отсечены, что мы увидим на верхней границе нашей спектрограммы.

В предоставленном Python-коде переменная audio_data представляет собой массив NumPy ($y$), содержащий эти дискретные отсчеты амплитуды:

$$y \= \[x\_0, x\_1, x\_2,..., x\_n\]$$

Где каждое значение $x\_i$ отражает мгновенное давление воздуха в момент времени $t \= i / f\_s$. Хотя этот одномерный массив содержит абсолютно всю информацию о звуке, он «непрозрачен» для анализа паттернов. Глядя на список чисел или график волны, даже эксперт не сможет точно определить высоту тона или отделить голос от шума кондиционера, если их амплитуды сопоставимы. Нейронная сеть, работающая с сырым сигналом, была бы вынуждена заново «изобретать» спектральный анализ, тратя ресурсы модели на обучение базовым физическим принципам. Спектрограмма выполняет эту работу предварительно (pre-computation), явно обнажая частотный контент.

### **2.2. Преобразование Фурье: Разрушение временной блокировки**

Фундаментальным инструментом для выявления частотного состава сигнала является Дискретное преобразование Фурье (DFT). Его математическая суть заключается в декомпозиции сложного сигнала на сумму простых синусоидальных и косинусоидальных волн различных частот.

$$X\_k \= \\sum\_{n=0}^{N-1} x\_n \\cdot e^{-i 2\\pi k n / N}$$

Однако стандартное DFT обладает фатальным недостатком для анализа речи или музыки: оно предоставляет усредненный частотный контент для всего анализируемого фрагмента сразу, полностью теряя информацию о времени. Если в записи сначала звучит барабан, а потом флейта, стандартное DFT сообщит: «в сигнале присутствуют частоты барабана и частоты флейты», но не сможет сказать, когда именно они звучали. Это делает классическое DFT бесполезным для распознавания речи (ASR) или детекции событий, где последовательность звуков несет основной смысл. Именно здесь на сцену выходит STFT.

---

## **3\. Спектрограмма: Картография времени и частоты**

Для решения проблемы потери временной информации используется Оконное преобразование Фурье (Short-Time Fourier Transform, STFT), которое является математическим двигателем функции librosa.feature.melspectrogram.

### **3.1. Механизм STFT**

Суть STFT заключается в разделении длинного аудиосигнала на короткие, перекрывающиеся сегменты (фреймы) равной длины. Преобразование Фурье применяется отдельно к каждому такому сегменту. Это позволяет сохранить локальную временную информацию. Результатом является матрица, где столбцы представляют временные фреймы, а строки — частотные бины (bins).4

Математическая формулировка STFT для сигнала $x\[n\]$ и оконной функции $w\[n\]$ выглядит следующим образом:

$$\\text{STFT}\\{x\[n\]\\}(m, k) \= \\sum\_{n=-\\infty}^{\\infty} x\[n\] w e^{-j2\\pi k n/N}$$

Где:

- $N$ — длина фрейма (контролируется параметром n_fft в коде).
- $R$ — шаг смещения окна, или «хоп» (контролируется параметром hop_length).
- $w\[n\]$ — оконная функция (например, окно Ханна), которая подавляет краевые эффекты.

### **3.2. Анализ параметров n_fft и hop_length**

Выбор параметров в функции librosa — это не просто техническая деталь, а поиск компромисса, диктуемого физикой. Это фундаментально влияет на разрешение получаемого изображения.

Параметр n_fft (Размер окна БПФ)  
В коде librosa значение по умолчанию часто составляет 2048 сэмплов.14 Этот параметр определяет количество отсчетов в одном сегменте анализа.

- _Второе смысловое дно (Инсайт):_ Здесь мы сталкиваемся с акустическим принципом неопределенности (аналогом принципа Гейзенберга). Большое значение n_fft обеспечивает высокое частотное разрешение (мы можем различить 440 Гц и 445 Гц), но дает низкое временное разрешение (быстрые события «размазываются» внутри длинного окна). Напротив, маленькое n_fft дает превосходную точность во времени (четкие границы ударов), но частоты становятся размытыми.
- _Контекст:_ Для речи (Librispeech) типичное окно составляет 20–30 миллисекунд. При частоте 16 кГц окно в 2048 сэмплов соответствует $2048 / 16000 \\approx 128$ мс. Это достаточно длинное окно, которое отлично подходит для анализа гармоник голоса, но может сглаживать очень быстрые переходы.

Параметр hop_length (Шаг сдвига)  
Этот параметр определяет, на сколько сэмплов сдвигается окно для следующего кадра. Типичное значение составляет n_fft / 4 (например, 512 сэмплов).13

- _Влияние:_ Меньший hop_length приводит к большему количеству столбцов в выходном изображении (более высокая временная «частота кадров» спектрограммы). Это критически важно для детекции фонем с резкой атакой, таких как взрывные согласные (/п/, /б/, /к/), но увеличивает вычислительную нагрузку и размер файла. При hop_length 512 и частоте 16 кГц мы получаем примерно 31 столбец спектрограммы в секунду ($16000 / 512 \\approx 31.25$ Гц).

### **3.3. Оконная функция и борьба с утечкой**

Код неявно использует оконную функцию (обычно окно Ханна — hann), на которую умножается сегмент сигнала перед БПФ.

$$w(n) \= 0.5 \\left( 1 \- \\cos \\left( \\frac{2\\pi n}{N-1} \\right) \\right)$$

Если просто «вырезать» кусок синусоиды прямоугольным окном, на краях возникнут резкие скачки, которые БПФ интерпретирует как наличие множества высокочастотных компонент, которых на самом деле нет в сигнале. Это явление называется спектральной утечкой (spectral leakage). Окно Ханна плавно сводит края сегмента к нулю, минимизируя этот эффект и делая спектрограмму «чистой» и пригодной для обучения моделей.14

---

## **4\. Психоакустика и Шкала Мел: Как слышит человек (и Голем)**

Результатом чистого STFT является линейная спектрограмма. Вертикальная ось на ней представляет частоты в Герцах (Гц), расположенные линейно (0, 100, 200... 8000 Гц). Однако квест требует создания именно _Мел_\-спектрограммы. Это различие является ключевым для эффективности AI-моделей.

### **4.1. Логарифмическая природа восприятия**

Человеческий слух (и, следовательно, структура речи и музыки, создаваемых человеком) не линеен. Перцептивное (воспринимаемое) расстояние между звуками 100 Гц и 200 Гц (октава) для нас звучит таким же, как расстояние между 1000 Гц и 2000 Гц (тоже октава). Однако в линейной шкале Герц первый интервал занимает 100 единиц, а второй — 1000\.  
Если подать линейную спектрограмму в нейросеть, модель будет тратить 90% своих ресурсов на анализ высокочастотного шума, где для человеческого уха мало информативных деталей, в то время как критически важные низкие частоты (фундаментальный тон голоса, первые форманты) будут сжаты в узкую полоску пикселей внизу изображения.4

### **4.2. Определение Шкалы Мел**

Шкала Мел (от англ. melody) была предложена Стивенсом, Волком и Ньюманом в 1937 году на основе экспериментов с восприятием высоты тона. Это шкала, в которой равные разности высот воспринимаются слушателем как равные.4  
Формула конвертации частоты $f$ (в Гц) в мелы $m$ выглядит примерно так:

$$m \= 2595 \\log\_{10} \\left( 1 \+ \\frac{f}{700} \\right)$$

Это преобразование растягивает низкие частоты и сжимает высокие, выравнивая информационную плотность изображения.

### **4.3. Мел-фильтрбанк (Filterbank)**

В строке кода:

Python

mel_spectrogram \= librosa.feature.melspectrogram(y=audio_data, sr=sampling_rate)

происходит матричное умножение. Функция создает «банк фильтров» (обычно 128 треугольных фильтров, параметр n_mels), распределенных по шкале мел. Линейный спектр умножается на эту матрицу.

- **Результат:** Вертикальная ось изображения теперь имеет размерность 128 (количество мел-бинов), а не 1025 (как было бы при n_fft=2048 в линейном спектре). Мы не только привели данные к человеческому восприятию, но и значительно уменьшили размерность данных (сжатие), сохранив при этом самые важные признаки.17

### **4.4. Шкала Децибел (Амплитуда)**

Последний шаг преобразования в коде:

Python

log_mel_spectrogram \= librosa.power_to_db(mel_spectrogram, ref=np.max)

Подобно тому как восприятие высоты тона логарифмично, восприятие громкости также подчиняется закону Вебера-Фехнера. Звук с удвоенной амплитудой давления не кажется нам в два раза громче. Динамический диапазон человеческого слуха огромен — от шепота до рева двигателя разница в давлении составляет миллионы раз.  
Перевод квадрата амплитуды (мощности) в децибелы (dB) сжимает этот диапазон:

$$S\_{dB} \= 10 \\log\_{10} \\left( \\frac{S}{S\_{ref}} \\right)$$

Без этого шага спектрограмма выглядела бы почти полностью черной с редкими яркими белыми точками (пиками громкости), скрывая все тонкие детали: дыхание, фоновый шум, мягкие окончания слов.3 Использование ref=np.max означает, что самый громкий звук на спектрограмме будет принят за 0 дБ (максимум), а все остальные звуки будут иметь отрицательные значения (например, \-40 дБ, \-80 дБ).

---

## **5\. Визуализация артефакта: Что скрывает spectrogram.png**

Открыв созданный артефакт spectrogram.png, мы видим тепловую карту (heatmap) звука. Для инженера машинного обучения умение «читать» эту карту так же важно, как для врача умение читать рентгеновский снимок.

### **5.1. Анатомия изображения**

В таблице ниже приведена структура визуального представления спектрограммы:

| Ось / Элемент            | Физический смысл    | Визуальная интерпретация                                                      |
| :----------------------- | :------------------ | :---------------------------------------------------------------------------- |
| **Ось X (Горизонталь)**  | Время ($t$)         | Движение слева направо соответствует ходу аудиозаписи.                        |
| **Ось Y (Вертикаль)**    | Частота (Mel Scale) | Низ изображения — бас, гул; верх — свист, шипение. Распределение нелинейное.  |
| **Цвет / Интенсивность** | Амплитуда (dB)      | Яркие цвета (желтый/красный) — громкие звуки. Темные (синий/черный) — тишина. |

### **5.2. Визуальная интерпретация акустических признаков**

Опытный глаз (и обученная CNN) различает на спектрограмме специфические текстуры 11:

- **Гласные звуки (Vowels):** Выглядят как четкие горизонтальные «стопки» линий. Самая нижняя линия — это **Фундаментальная частота (F0)**, определяющая высоту голоса (ноту). Линии над ней — это **Гармоники** (целые кратные F0). Специфические усиления яркости определенных гармоник называются **Формантами** (F1, F2, F3). Именно взаимное расположение формант позволяет нам отличать звук «А» от звука «О» или «И».
- **Фрикативные согласные (Fricatives — с, ш, ф):** Выглядят как «облака» высокочастотного шума или статики в верхней части спектрограммы. Они не имеют гармонической структуры (горизонтальных линий). Звук «С» обычно имеет энергию выше по частоте, чем «Ш».11
- **Взрывные согласные (Plosives — п, б, т, д):** Визуализируются как узкая вертикальная линия (всплеск энергии) поперек всех частот, которой предшествует микро-пауза (темная зона) — момент смыкания губ перед взрывом воздуха.
- **Тишина:** Сплошные темные вертикальные полосы.

### **5.3. Инсайт второго порядка: Текстура как Тембр**

Вопрос Техноманта о том, можно ли научить модель «видеть» звук, имеет глубокий смысл. Для сверточной сети понятие «тембр» (окраска звука) эквивалентно понятию «текстура» в изображении.

- «Шершавый» или хриплый голос на спектрограмме выглядит как размытые, нечеткие гармонические линии с добавлением шума между ними.
- «Чистый», звонкий голос выглядит как набор очень тонких, ярких, четко очерченных горизонтальных линий на черном фоне.
- Механический скрежет выглядит как хаотичная смесь вертикальных и горизонтальных структур.

Следовательно, задача классификации звуков сводится к задаче классификации визуальных текстур, с чем CNN справляются превосходно.21

---

## **6\. Сверточные нейронные сети: Всевидящее Око Аудио-Големов**

Центральная идея квеста и ответ Мастера касаются применения Computer Vision (CV) к аудио. Рассмотрим механику этого процесса.

### **6.1. Сила свертки в аудиодомене**

Сверточные сети (CNN) созданы для обнаружения локальных паттернов, инвариантных к сдвигу (Translation Invariance). В фотографии кот остается котом, находится ли он в левом верхнем или правом нижнем углу. Как это работает в аудио?

1. **Инвариантность во времени (Ось X):** Слово «Привет» должно быть распознано как «Привет», независимо от того, произнесено оно на 1-й секунде записи или на 10-й. CNN естественным образом решают эту задачу, «скользя» фильтрами вдоль оси времени.7
2. **Инвариантность по частоте (Ось Y):** Здесь ситуация сложнее. Если мелодия сдвигается вверх по оси Y (транспонируется на октаву выше), её паттерн сохраняет форму, но меняет положение. В отличие от изображений, где объект может быть где угодно, в звуке абсолютная высота тона часто имеет значение (например, мужской голос против детского). Тем не менее, _локальные_ отношения (расстояние между гармониками) сохраняются, что позволяет сверткам эффективно обучаться.23

### **6.2. Иерархия признаков: Что видит сеть?**

Исследования визуализации фильтров CNN, обученных на аудиоданных, показывают поразительное сходство с фильтрами для обычных изображений 22:

- **Слой 1 (Low-level):** Детекторы краев (Edges). В аудио вертикальные края — это атаки звуков (удары барабана), горизонтальные края — это устойчивые тона.
- **Слой 2 (Mid-level):** Детекторы углов и кривых. В аудио это частотные модуляции — вибрато, глиссандо (скольжение тона), интонационные изгибы речи.
- **Слой 3+ (High-level):** Сложные текстуры. Сеть собирает предыдущие признаки в целостные объекты — «формантная структура гласной А», «шумовой паттерн тарелки Hi-Hat», «гармоническая сетка скрипки».

### **6.3. Трансферное обучение (Transfer Learning): От ImageNet к AudioSet**

Ответ Мастера Техноманту о возможности использования модели, обученной на кошках и собаках, является абсолютно верным и подтверждается современной практикой.1  
Модели архитектур ResNet-50, VGG-16 или EfficientNet, предварительно обученные на гигантском визуальном датасете ImageNet, уже умеют идеально выделять линии, границы, градиенты и сложные текстуры.

- **Методика:** Мы берем такую модель, «замораживаем» её начальные слои (feature extractors) и заменяем только последний классификационный слой (который выдавал «кошка/собака») на новый, соответствующий нашей аудиозадаче (например, «мужчина/женщина» или «речь/шум»).
- **Преимущество:** Поскольку спектрограмма — это изображение, состоящее из линий и текстур, веса ImageNet прекрасно подходят для старта. Это позволяет обучать мощные аудиомодели на очень малых наборах данных, так как сети не нужно с нуля учиться тому, «что такое линия».6

---

## **7\. Кейс-стади I: Классификация пола по голосу**

Запрос пользователя упоминает конкретную задачу: научить модель отличать мужской голос от женского. Рассмотрим, как именно это происходит на уровне признаков мел-спектрограммы.

### **7.1. Визуальная сигнатура биологического пола**

Физиологические различия в голосовом аппарате мужчин и женщин находят прямое отражение в геометрии спектрограммы.26 Основные различия сводятся к длине и массе голосовых связок и длине голосового тракта.

**Таблица 2: Сравнительный анализ спектральных признаков голосов**

| Признак                          | Акустическая природа                                                       | Визуальное проявление на мел-спектрограмме                                       |
| :------------------------------- | :------------------------------------------------------------------------- | :------------------------------------------------------------------------------- |
| **Фундаментальная частота (F0)** | Скорость смыкания голосовых связок.                                        | Положение самой нижней горизонтальной линии в гармоническом стеке.               |
| **F0 (Мужской)**                 | Более длинные и массивные связки ($\\approx$ 85–180 Гц).                   | Нижняя линия находится очень близко к оси X (дну изображения).                   |
| **F0 (Женский)**                 | Более короткие и легкие связки ($\\approx$ 165–255 Гц).                    | Нижняя линия заметно смещена вверх по оси Y.                                     |
| **Интервалы гармоник**           | Расстояние между кратными частотами ($2 \\times F0, 3 \\times F0 \\dots$). | Вертикальное расстояние между горизонтальными полосами («ступеньками лестницы»). |
| **Интервалы (Мужской)**          | Низкий F0 \= плотные гармоники.                                            | Линии расположены густо, текстура кажется более «полосатой» и плотной.           |
| **Интервалы (Женский)**          | Высокий F0 \= широкие гармоники.                                           | Линии расположены разреженно, с большими промежутками.                           |
| **Форманты (Резонансы)**         | Усиление частот в глотке и ротовой полости.                                | Яркие «пятна» или полосы поверх гармонической сетки.                             |
| **Форманты (Мужской)**           | Длинный тракт ($\\approx$ 17 см) \= более низкие резонансы.                | Энергия формант сосредоточена ниже.                                              |
| **Форманты (Женский)**           | Короткий тракт ($\\approx$ 14 см) \= более высокие резонансы.              | Пятна яркости сдвинуты вверх.                                                    |

### **7.2. Как CNN принимает решение**

Когда CNN смотрит на спектрограмму, она не измеряет частоту в Герцах явно. Она обучается на паттернах:

1. **Плотность линий:** Сеть замечает, что изображения класса «Мужчина» имеют густую горизонтальную штриховку (плотные гармоники), а класса «Женщина» — редкую.
2. **Центр тяжесть яркости:** Сеть выучивает, что у женских голосов яркие пятна (форманты) находятся выше по оси Y.
3. **Текстурные особенности:** Женские голоса часто имеют более выраженную аспирацию (придыхание), что выглядит как легкая шумовая дымка на высоких частотах. Мужские голоса могут демонстрировать эффект _vocal fry_ (скрипучесть) в концах фраз, что выглядит как распад сплошных горизонтальных линий на отдельные вертикальные штрихи.29

Использование мел-спектрограмм в сочетании с CNN для этой задачи дает точность выше 95-97%, значительно превосходя старые методы, основанные на ручном расчете Pitch.1

---

## **8\. Бизнес-ценность и промышленные применения**

В условиях квеста перечислены три направления бизнес-ценности: отладка, инженерия признаков и новые задачи. Расширим эти пункты, опираясь на исследования, чтобы показать реальный экономический эффект технологии.

### **8.1. Отладка и диагностика моделей (Debugging)**

В реальной разработке ASR-систем (Automatic Speech Recognition) инженер проводит часы, глядя на спектрограммы.

- **Сценарий:** Модель ошибается в распознавании слова, хотя на слух оно звучит четко.
- **Визуальный анализ:** Инженер открывает спектрограмму и видит узкую горизонтальную полосу на частоте 60 Гц (сетевой фон) или 1000 Гц (писк электроники). Ухо может адаптироваться и «отфильтровать» этот шум, но для модели это сигнал с высокой энергией, который перекрывает полезные признаки.
- **Решение:** Визуализация позволяет мгновенно обнаружить артефакты клиппинга (обрезание сигнала, выглядящее как «квадратные» волны с бесконечным спектром шума) или пропуски пакетов (черные вертикальные полосы).20

### **8.2. Промышленная диагностика и Predictive Maintenance**

Это одна из самых быстрорастущих областей применения аудиоанализа («Новые задачи»).

- **Проблема:** Как понять, что подшипник на конвейере скоро сломается, не останавливая производство?
- **Решение:** Звук работающего мотора превращается в мел-спектрограмму.
  - _Норма:_ Ровные горизонтальные линии (гул мотора).
  - _Дефект:_ Появление периодических вертикальных «щелчков» (удар шарика в подшипнике о трещину) или новой горизонтальной линии на высокой частоте (свист ремня).
- **AI-подход:** Модель обучается на тысячах часов «нормальной» работы (Anomaly Detection). Любое отклонение в визуальном паттерне спектрограммы интерпретируется как аномалия. Использование мел-шкалы здесь оправдано тем, что многие механические шумы имеют низкочастотную природу, а высокочастотные свисты (утечки газа) хорошо локализуются.31

### **8.3. Биоакустика и экологический мониторинг**

Распознавание птиц (как в приложении Merlin или BirdNET) — это классическая задача компьютерного зрения.

- **Специфика:** Голоса птиц часто очень высоки и быстры. Мел-спектрограмма позволяет охватить весь диапазон, но сжать его до разумных размеров.
- **Визуальные глифы:** Песня каждой птицы имеет уникальную визуальную подпись. Дрозд выдает сложные мелодические свисты (извилистые линии), дятел — серию быстрых ударов (вертикальный штрих-код). CNN классифицирует эти «иероглифы» так же, как рукописный текст.34

### **8.4. Медицинская диагностика**

Анализ звуков сердца (фонокардиограмма) и легких.

- **Сердце:** Нормальный ритм — это два четких вертикальных события («тук-тук»). Шумы в сердце (murmurs) выглядят как размытые облака шума между ударами. Спектрограммы позволяют диагностировать пороки клапанов с точностью, сопоставимой с экспертами-кардиологами.36
- **Легкие:** Во время пандемии COVID-19 появились алгоритмы, различающие типы кашля по спектральной текстуре («сухой» vs «влажный» кашель).38

---

## **9\. Технические детали реализации: За кулисами librosa**

Для профессионального применения важно понимать не только концепцию, но и нюансы реализации функций librosa, использованных в квесте.

### **9.1. Роль power_to_db и динамический диапазон**

Строка librosa.power_to_db часто недооценивается новичками.

- **Суть:** Аудиосигналы имеют колоссальный динамический диапазон. Разница в энергии между тихим шелестом и громким криком может составлять 60–100 дБ (в миллион и более раз по мощности).
- **Без логарифмирования:** На линейной спектрограмме (по амплитуде) громкий крик будет белым пятном, а всё остальное — абсолютно черным. Нейросеть просто не увидит тихие звуки, которые формируют контекст (фоновый шум, эхо, дыхание).
- **С логарифмированием:** Функция сжимает диапазон так, что и громкие, и тихие звуки становятся различимыми текстурами. Это аналог HDR (High Dynamic Range) в фотографии.16

### **9.2. Центрирование и паддинг (center=True)**

По умолчанию librosa использует параметр center=True. Это означает, что сигнал дополняется (pad) нулями с обеих сторон перед анализом.

- **Зачем:** Это гарантирует, что временная метка $t$-го фрейма спектрограммы совпадает с реальным временем $t \\times hop\\\_length$. Без этого спектрограмма была бы короче исходного звука и смещена во времени, что критично при задачах выравнивания (например, наложении текста субтитров на звук).14

### **9.3. Выбор количества мел-фильтров (n_mels)**

В коде этот параметр скрыт (по умолчанию 128), но он важен.

- **128:** Золотой стандарт для речи и звуков среды (AudioSet, Librispeech). Обеспечивает баланс между разрешением и размером данных.
- **40-64:** Часто использовалось в старых системах (MFCC) для распознавания речи. Сейчас считается недостаточным для глубоких CNN.
- **256+:** Используется для сложного музыкального анализа (разделение инструментов), где нужно различать близкие ноты.15

---

## **10\. Заключение**

Выполнение Квеста 2.3 демонстрирует, что в современном AI границы между модальностями данных стираются. Мел-спектрограмма — это не просто картинка для красоты; это математически обоснованная проекция физической реальности (звуковых волн) в перцептивное пространство (человеческий слух), закодированная в формате тензора (изображения).

Мы подтвердили гипотезу о том, что звук можно «видеть». Анализ показал, что:

1. **Инвариантность работает:** Сверточные сети (CNN) успешно переносят свои навыки поиска краев и текстур с фотографий на спектрограммы.
2. **Биология имеет значение:** Использование шкалы Мел критически важно для эффективного распределения внимания нейросети в полезном диапазоне частот.
3. **Универсальность метода:** Один и тот же подход (Спектрограмма \-\> CNN) решает задачи от гендерной классификации до диагностики промышленных турбин.

Таким образом, ответ на вопрос Техноманта утвердительный: «Всевидящее Око» действительно может стать «Всеслышащим Ухом», если мы дадим ему правильные очки — мел-спектрограмму.

### **Перспективы развития**

Логичным следующим шагом (Квест 2.4?) было бы изучение обратной задачи: синтез звука из спектрограммы (Neural Vocoding) или применение Трансформеров (Audio Spectrogram Transformer, AST), которые начинают вытеснять CNN, рассматривая кусочки спектрограммы как последовательность «визуальных слов».

---

**Таблица 3: Сравнение представлений аудио для Deep Learning**

| Представление                  | Размерность        | Плюсы                                               | Минусы                                                       | Лучшее применение                                             |
| :----------------------------- | :----------------- | :-------------------------------------------------- | :----------------------------------------------------------- | :------------------------------------------------------------ |
| **Raw Waveform** (Сырая волна) | 1D (Амплитуда)     | Нет потерь информации; фаза сохранена.              | Огромная размерность; трудно учить паттерны; тяжелые модели. | End-to-end синтез (WaveNet), удаление шума.                   |
| **Линейная спектрограмма**     | 2D (Частота/Время) | Видны все частоты равнозначно.                      | Избыточность ВЧ; не соответствует слуху.                     | Анализ сигналов (не речи), дефектоскопия ВЧ.                  |
| **Мел-спектрограмма**          | 2D (Мел/Время)     | Компактность; биомиметичность; совместимость с CNN. | Потеря фазы; потеря точности ВЧ.                             | **Распознавание речи, классификация звуков.**                 |
| **MFCC**                       | 2D (Коэфф./Время)  | Декорреляция признаков; очень компактно.            | Потеря текстуры и шума; устаревает для DL.                   | Простые системы ASR, ограниченные ресурсы (микроконтроллеры). |

#### **Источники**

1. Environmental Sound Classification Using Convolutional Neural Network Based Transfer Learning \- IEEE Xplore, дата последнего обращения: ноября 27, 2025, [https://ieeexplore.ieee.org/document/10993691/](https://ieeexplore.ieee.org/document/10993691/)
2. \[D\] Why do (most) text-to-speech models use mel spectrograms for conditioning? \- Reddit, дата последнего обращения: ноября 27, 2025, [https://www.reddit.com/r/MachineLearning/comments/kdwzc5/d_why_do_most_texttospeech_models_use_mel/](https://www.reddit.com/r/MachineLearning/comments/kdwzc5/d_why_do_most_texttospeech_models_use_mel/)
3. Audio Deep Learning Made Simple \- Why Mel Spectrograms perform better, дата последнего обращения: ноября 27, 2025, [https://ketanhdoshi.github.io/Audio-Mel/](https://ketanhdoshi.github.io/Audio-Mel/)
4. Understanding the Mel Spectrogram | by Leland Roberts | Analytics Vidhya | Medium, дата последнего обращения: ноября 27, 2025, [https://medium.com/analytics-vidhya/understanding-the-mel-spectrogram-fca2afa2ce53](https://medium.com/analytics-vidhya/understanding-the-mel-spectrogram-fca2afa2ce53)
5. Audio Deep Learning Made Simple (Part 2): Why Mel Spectrograms perform better, дата последнего обращения: ноября 27, 2025, [https://towardsdatascience.com/audio-deep-learning-made-simple-part-2-why-mel-spectrograms-perform-better-aad889a93505/](https://towardsdatascience.com/audio-deep-learning-made-simple-part-2-why-mel-spectrograms-perform-better-aad889a93505/)
6. Spectral and Rhythm Features for Audio Classification with Deep Convolutional Neural Networks \- arXiv, дата последнего обращения: ноября 27, 2025, [https://arxiv.org/html/2410.06927v1](https://arxiv.org/html/2410.06927v1)
7. What is a convolutional neural network? \- Google Cloud, дата последнего обращения: ноября 27, 2025, [https://cloud.google.com/discover/what-are-convolutional-neural-networks](https://cloud.google.com/discover/what-are-convolutional-neural-networks)
8. What is a convolutional neural network? \- Milvus, дата последнего обращения: ноября 27, 2025, [https://milvus.io/ai-quick-reference/what-is-a-convolutional-neural-network](https://milvus.io/ai-quick-reference/what-is-a-convolutional-neural-network)
9. Librispeech: An ASR corpus based on public domain audio books \- IEEE Xplore, дата последнего обращения: ноября 27, 2025, [https://ieeexplore.ieee.org/document/7178964](https://ieeexplore.ieee.org/document/7178964)
10. README.md · openslr/librispeech_asr at main \- Hugging Face, дата последнего обращения: ноября 27, 2025, [https://huggingface.co/datasets/openslr/librispeech_asr/blob/main/README.md](https://huggingface.co/datasets/openslr/librispeech_asr/blob/main/README.md)
11. Identifying sounds in spectrograms, дата последнего обращения: ноября 27, 2025, [https://home.cc.umanitoba.ca/\~krussll/phonetics/acoustic/spectrogram-sounds.html](https://home.cc.umanitoba.ca/~krussll/phonetics/acoustic/spectrogram-sounds.html)
12. Harmonics Vs. Formants \- VoiceScienceWorks, дата последнего обращения: ноября 27, 2025, [http://www.voicescienceworks.org/harmonics-vs-formants.html](http://www.voicescienceworks.org/harmonics-vs-formants.html)
13. Understanding the shape of spectrograms and n_mels \- Stack Overflow, дата последнего обращения: ноября 27, 2025, [https://stackoverflow.com/questions/62584184/understanding-the-shape-of-spectrograms-and-n-mels](https://stackoverflow.com/questions/62584184/understanding-the-shape-of-spectrograms-and-n-mels)
14. librosa.feature.melspectrogram — librosa 0.11.0 documentation, дата последнего обращения: ноября 27, 2025, [https://librosa.org/doc/main/generated/librosa.feature.melspectrogram.html](https://librosa.org/doc/main/generated/librosa.feature.melspectrogram.html)
15. Presets — librosa 0.11.0 documentation, дата последнего обращения: ноября 27, 2025, [http://librosa.org/doc/0.11.0/auto_examples/plot_presets.html](http://librosa.org/doc/0.11.0/auto_examples/plot_presets.html)
16. Mel Spectrograms with Python and Librosa | Audio Feature Extraction \- YouTube, дата последнего обращения: ноября 27, 2025, [https://www.youtube.com/watch?v=g8Q452PEXwY](https://www.youtube.com/watch?v=g8Q452PEXwY)
17. Mel-frequency Cepstral Coefficients (MFCC) for Speech Recognition \- GeeksforGeeks, дата последнего обращения: ноября 27, 2025, [https://www.geeksforgeeks.org/nlp/mel-frequency-cepstral-coefficients-mfcc-for-speech-recognition/](https://www.geeksforgeeks.org/nlp/mel-frequency-cepstral-coefficients-mfcc-for-speech-recognition/)
18. Mel-frequency cepstrum \- Wikipedia, дата последнего обращения: ноября 27, 2025, [https://en.wikipedia.org/wiki/Mel-frequency_cepstrum](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum)
19. Understanding spectrograms \- iZotope, дата последнего обращения: ноября 27, 2025, [https://www.izotope.com/en/learn/understanding-spectrograms](https://www.izotope.com/en/learn/understanding-spectrograms)
20. What is a Spectrogram? A 101 Guide to Reading Spectrograms \- Blog \- Splice, дата последнего обращения: ноября 27, 2025, [https://splice.com/blog/what-is-a-spectrogram/](https://splice.com/blog/what-is-a-spectrogram/)
21. 45\. CNN Applications Beyond Vision: Images and Sound | by Kiran vutukuri \- Medium, дата последнего обращения: ноября 27, 2025, [https://medium.com/@kiranvutukuri/45-cnn-applications-beyond-vision-images-and-sound-0c2f038b3530](https://medium.com/@kiranvutukuri/45-cnn-applications-beyond-vision-images-and-sound-0c2f038b3530)
22. 27 Learned Features – Interpretable Machine Learning, дата последнего обращения: ноября 27, 2025, [https://christophm.github.io/interpretable-ml-book/cnn-features.html](https://christophm.github.io/interpretable-ml-book/cnn-features.html)
23. CNN filter shapes discussion for music spectrograms \- Jordi Pons, дата последнего обращения: ноября 27, 2025, [https://www.jordipons.me/cnn-filter-shapes-discussion/](https://www.jordipons.me/cnn-filter-shapes-discussion/)
24. Comparison of Pre-Trained CNNs for Audio Classification Using Transfer Learning \- MDPI, дата последнего обращения: ноября 27, 2025, [https://www.mdpi.com/2224-2708/10/4/72](https://www.mdpi.com/2224-2708/10/4/72)
25. Audio classification using transfer learning approach | by Krishna D N \- Medium, дата последнего обращения: ноября 27, 2025, [https://medium.com/@krishna_84429/audio-classification-using-transfer-learning-approach-912e6f7397bb](https://medium.com/@krishna_84429/audio-classification-using-transfer-learning-approach-912e6f7397bb)
26. A Deep Learning Method Using Gender-Specific Features for Emotion Recognition \- NIH, дата последнего обращения: ноября 27, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC9921859/](https://pmc.ncbi.nlm.nih.gov/articles/PMC9921859/)
27. Gender classification from speech using convolutional networks augmented with synthetic spectrograms | The Journal of the Acoustical Society of America | AIP Publishing, дата последнего обращения: ноября 27, 2025, [https://pubs.aip.org/asa/jasa/article/150/4_Supplement/A358/704937/Gender-classification-from-speech-using](https://pubs.aip.org/asa/jasa/article/150/4_Supplement/A358/704937/Gender-classification-from-speech-using)
28. Spectrograms of the average female (left panel) and male voice (right... \- ResearchGate, дата последнего обращения: ноября 27, 2025, [https://www.researchgate.net/figure/Spectrograms-of-the-average-female-left-panel-and-male-voice-right-panel-for-the-five_fig4_221689580](https://www.researchgate.net/figure/Spectrograms-of-the-average-female-left-panel-and-male-voice-right-panel-for-the-five_fig4_221689580)
29. Corner vowels in males and females ages 4 to 20 years: Fundamental and F1–F4 formant frequencies \- NIH, дата последнего обращения: ноября 27, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC6850954/](https://pmc.ncbi.nlm.nih.gov/articles/PMC6850954/)
30. Age and Gender Recognition Using a Convolutional Neural Network with a Specially Designed Multi-Attention Module through Speech Spectrograms \- PubMed, дата последнего обращения: ноября 27, 2025, [https://pubmed.ncbi.nlm.nih.gov/34502785/](https://pubmed.ncbi.nlm.nih.gov/34502785/)
31. Machine Hearing for Industrial Fault Diagnosis \- Loughborough University Research Repository, дата последнего обращения: ноября 27, 2025, [https://repository.lboro.ac.uk/articles/conference_contribution/Machine_hearing_for_industrial_fault_diagnosis/15073527/files/28984890.pdf](https://repository.lboro.ac.uk/articles/conference_contribution/Machine_hearing_for_industrial_fault_diagnosis/15073527/files/28984890.pdf)
32. Audio-based Anomaly Detection in Industrial Machines Using Deep One-Class Support Vector Data Description \- arXiv, дата последнего обращения: ноября 27, 2025, [https://arxiv.org/html/2412.10792v1](https://arxiv.org/html/2412.10792v1)
33. Comparison of Mel Frequency Cepstral Coefficient (MFCC) and Mel Spectrogram Techniques to Classify Industrial Machine Sound \- IEEE Xplore, дата последнего обращения: ноября 27, 2025, [https://ieeexplore.ieee.org/document/10387339/](https://ieeexplore.ieee.org/document/10387339/)
34. Capturing Bird Calls and Other Wildlife Sounds With Bioacoustics \- Noble Research Institute, дата последнего обращения: ноября 27, 2025, [https://www.noble.org/regenerative-agriculture/wildlife/capturing-bird-calls-and-other-wildlife-sounds-with-bioacoustics/](https://www.noble.org/regenerative-agriculture/wildlife/capturing-bird-calls-and-other-wildlife-sounds-with-bioacoustics/)
35. Deep transfer learning-based bird species classification using mel spectrogram images | PLOS One \- Research journals, дата последнего обращения: ноября 27, 2025, [https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0305708](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0305708)
36. Classifying Heart-Sound Signals Based on CNN Trained on MelSpectrum and Log-MelSpectrum Features \- PMC \- NIH, дата последнего обращения: ноября 27, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC10294824/](https://pmc.ncbi.nlm.nih.gov/articles/PMC10294824/)
37. Classification of Acoustic Tones and Cardiac Murmurs Based on Digital Signal Analysis Leveraging Machine Learning Methods \- MDPI, дата последнего обращения: ноября 27, 2025, [https://www.mdpi.com/2079-3197/12/10/208](https://www.mdpi.com/2079-3197/12/10/208)
38. Feature-Based Fusion Using CNN for Lung and Heart Sound Classification \- PMC \- NIH, дата последнего обращения: ноября 27, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC8875944/](https://pmc.ncbi.nlm.nih.gov/articles/PMC8875944/)
