# **Отчет об исследовании: Оптимизация видеоаналитики через методы прореживания кадров (Frame Sampling)**

## **1\. Введение: Экономика внимания в эпоху визуальных данных**

В современной экосистеме цифровых данных видеопоток занимает доминирующее положение, представляя собой наиболее богатый, но и наиболее ресурсоемкий источник информации. В контексте задачи, сформулированной как «Квест 8.1» в рамках метафорической «Школы Управления Временем», видео описывается как сущность, жадная до «маны» — вычислительных ресурсов, пропускной способности каналов связи и емкости систем хранения. Эта метафора точно отражает техническую реальность: обработка видеопотока в полном разрешении и с исходной частотой кадров (Full Frame Rate) часто является не просто расточительной, но и технически неоправданной стратегией, ведущей к перегрузке инфраструктуры и экспоненциальному росту операционных расходов.  
Фундаментальная проблема видеоаналитики заключается в высокой степени временной избыточности (temporal redundancy). В отличие от статических изображений, где каждый пиксель несет уникальную пространственную информацию, видео состоит из последовательности кадров, где изменения между моментами t и t+1 часто минимальны или отсутствуют вовсе. Например, в системах видеонаблюдения камера может часами фиксировать пустой коридор, генерируя гигабайты данных, содержащих нулевую энтропию с точки зрения аналитической ценности. Обработка каждого такого кадра с помощью глубоких нейронных сетей (DNN) эквивалентна сжиганию вычислительной «маны» впустую.  
Техника «Прореживания Кадров» (Frame Sampling) не является простым отбрасыванием данных. Это сложный алгоритмический процесс снижения размерности временного ряда, целью которого является максимизация плотности информации на единицу вычислительных затрат. Как показывают современные исследования, грамотное применение стратегий выборки может повысить эффективность инференса в разы, а в некоторых случаях — и точность моделей за счет снижения шума и переобучения на повторяющихся паттернах. Однако этот процесс сопряжен с рисками: чрезмерное или некорректное прореживание ведет к потере причинно-следственных связей, возникновению эффекта временного наложения (temporal aliasing) и пропуску критических событий, что недопустимо в задачах безопасности.  
Данный отчет представляет собой исчерпывающее руководство по методологии, алгоритмике и экономике прореживания кадров. Мы рассмотрим этот процесс на всех уровнях: от низкоуровневой работы с кодеками и буферами памяти в OpenCV до высокоуровневых адаптивных алгоритмов на базе искусственного интеллекта и стратегий оптимизации затрат в облачных платформах AWS и Google Cloud.

## **2\. Фундаментальная теория цифрового видео и времени**

Прежде чем приступать к оптимизации, необходимо глубоко понять физическую и логическую структуру видеопотока. Видеофайл — это не просто стопка фотографий; это сложная структура данных, сжатая с использованием психовизуальных моделей и предсказания движения. Понимание механики сжатия критически важно для эффективного прореживания, так как попытка прочитать произвольный кадр может вызвать каскад скрытых вычислительных операций.

### **2.1 Архитектура GOP и типы кадров**

Большинство современных видеокодеков (H.264/AVC, H.265/HEVC, VP9) используют концепцию GOP (Group of Pictures — Группа изображений). Внутри GOP кадры делятся на три основных типа, каждый из которых играет свою роль в возможности доступа к данным :

1. **I-кадры (Intra-coded frames):** Также известные как ключевые кадры (keyframes). Это полностью независимые изображения, сжатые подобно JPEG. Они не требуют ссылок на другие кадры для декодирования. I-кадры являются «опорными точками» во времени.
2. **P-кадры (Predicted frames):** Содержат только разницу (delta) по сравнению с предыдущим кадром (I или P). Они хранят векторы движения и остаточную ошибку предсказания. Для декодирования P-кадра декодер должен сначала восстановить все предыдущие кадры вплоть до ближайшего I-кадра.
3. **B-кадры (Bi-directional predicted frames):** Используют ссылки как на предыдущие, так и на будущие кадры. Они обеспечивают максимальное сжатие, но вносят наибольшую задержку и сложность при декодировании.

#### **Влияние на прореживание**

Эта структура создает фундаментальную асимметрию в стоимости доступа к кадрам. Чтение I-кадра происходит быстро. Чтение P-кадра, находящегося в конце длинной GOP-структуры (например, 50-го кадра после I-кадра), требует от системы декодирования распаковать и обработать 49 предшествующих кадров, даже если они не нужны пользователю. В контексте задачи «Квеста», попытка «сэкономить ману», запрашивая только каждый 10-й кадр, может парадоксальным образом увеличить нагрузку на декодер, если алгоритм навигации не учитывает структуру GOP.

### **2.2 Теорема Котельникова-Шеннона во временной области**

При прореживании кадров мы фактически снижаем частоту дискретизации сигнала. Согласно теореме отсчетов, для корректного восстановления сигнала частота дискретизации должна быть как минимум в два раза выше максимальной частоты полезного сигнала (частота Найквиста).  
В видео это проявляется через эффект временного наложения спектров (temporal aliasing). Классическим примером является «эффект колеса телеги» (wagon-wheel effect), когда при определенной частоте съемки колесо кажется вращающимся в обратную сторону или неподвижным. Если система видеонаблюдения настроена на захват 1 кадра в секунду (1 FPS), а человек пробегает через дверной проем за 0.8 секунды, событие будет полностью потеряно. Более того, если частота выборки синхронизируется с периодическим движением (например, мерцанием лампы или вращением вентилятора), аналитическая система может получить ложные данные о стационарности объекта.  
Таким образом, выбор коэффициента прореживания (N) — это не произвольное решение, а расчет, основанный на ожидаемой скорости объектов в сцене. Для задач трекинга быстрых объектов (автомобили, спорт) прореживание должно быть минимальным или адаптивным. Для мониторинга статических сцен (склад, парковка) допустимо агрессивное снижение частоты.

## **3\. Механика декодирования: Низкоуровневая реализация в OpenCV**

Библиотека OpenCV является де\-факто стандартом в компьютерном зрении. Однако её высокоуровневый API скрывает множество подводных камней, влияющих на производительность сэмплирования. Рассмотрим детально методы доступа к кадрам.

### **3.1 Дихотомия read() против grab() и retrieve()**

Стандартный метод cap.read() в Python-биндингах OpenCV выполняет две операции атомарно:

1. **Grab:** Извлечение сжатого пакета данных следующего кадра из видеопотока или файла.
2. **Retrieve:** Декодирование этого пакета (IDCT, компенсация движения, преобразование цветового пространства YUV \-\> BGR) в массив numpy.

При реализации прореживания (например, каждый 5-й кадр) наивный подход выглядит так:  
`# Наивный подход (плохая практика)`  
`count = 0`  
`while True:`  
 `ret, frame = cap.read()`  
 `if count % 5 == 0:`  
 `process(frame)`  
 `count += 1`

В этом коде система тратит огромные ресурсы на полное декодирование 4-х кадров, которые затем просто игнорируются. Декодирование — это самая «дорогая» операция, потребляющая до 80-90% времени CPU в цикле захвата.  
Оптимизированный подход использует разделение методов:  
`# Оптимизированный подход (техника "Прореживания")`  
`count = 0`  
`while True:`  
 `if count % 5 == 0:`  
 `ret, frame = cap.read() # Grab + Retrieve`  
 `process(frame)`  
 `else:`  
 `cap.grab() # Только извлечение пакета, без декодирования`  
 `count += 1`

Метод cap.grab() считывает данные кадра, обновляет внутренний счетчик позиции, но _не выполняет_ декодирование пикселей. Это позволяет пропускать кадры значительно быстрее, экономя такты процессора и пропускную способность шины памяти. Исследования показывают, что использование grab() для пропуска кадров может ускорить цикл чтения в разы по сравнению с read(), особенно на высоких разрешениях (4K), где декодирование особенно затратно.

### **3.2 Произвольный доступ: CAP_PROP_POS_FRAMES**

Альтернативой последовательному пропуску через grab() является использование функции перемотки (seeking) через установку свойства: cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)  
Этот метод кажется идеальным для разреженного сэмплирования (например, 1 кадр каждые 10 секунд). Однако его производительность крайне нестабильна и зависит от формата файла и расположения ключевых кадров (I-frames).

- **Проблема точности:** Установка позиции часто неточна. В видео с переменным битрейтом (VBR) FFMPEG (бэкенд OpenCV) может промахнуться мимо целевого кадра, перейдя к ближайшему ключевому кадру.
- **Проблема производительности:** Если целевой кадр — P-кадр, находящийся далеко от предыдущего I-кадра, декодеру придется скрытно декодировать все промежуточные кадры, что вызывает ощутимую задержку (lag). В тестах пользователей задержка при set() может достигать сотен миллисекунд, делая этот метод медленнее, чем последовательный grab() для малых интервалов пропуска.

**Рекомендация:** Используйте set(cv2.CAP_PROP_POS_FRAMES) только для больших скачков (например, \> 100 кадров или \> 2-3 секунды). Для частых пропусков (каждый 2-й, 5-й, 10-й кадр) комбинация grab() в цикле является более надежной и предсказуемой по времени исполнения стратегией.

### **3.3 Управление буфером и многопоточность**

При работе с потоковым видео (IP-камеры, RTSP) возникает проблема переполнения буфера. VideoCapture имеет внутренний буфер, который заполняется кадрами по мере их поступления с камеры. Если алгоритм обработки (инференс нейросети) работает медленнее, чем поступают кадры (например, обработка 10 FPS при потоке 30 FPS), буфер заполняется устаревшими кадрами. В результате, когда алгоритм запрашивает read(), он получает кадр, который был снят несколько секунд назад. Это явление известно как накапливающаяся задержка.  
Для решения этой задачи в рамках «Квеста» необходимо реализовать асинхронный захват кадров (Threaded Video Capture).

- **Архитектура:** Отдельный поток (Thread) в бесконечном цикле вызывает cap.read() и сохраняет результат в переменную (или очередь размера 1), перезаписывая старые кадры.
- **Результат:** Основной поток обработки всегда забирает _самый последний_ доступный кадр, мгновенно пропуская все, что было между ними. Это автоматически реализует адаптивное прореживание: если система тормозит, она просто пропускает больше кадров, но не отстает от реального времени.
- **Параметр CAP_PROP_BUFFERSIZE:** В некоторых бэкендах OpenCV можно принудительно ограничить размер буфера (например, cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)), чтобы минимизировать задержку на уровне драйвера.

## **4\. Стратегии выборки: От равномерного к адаптивному**

Выбор того, _какие_ кадры обрабатывать, определяет баланс между точностью и производительностью.

### **4.1 Равномерная выборка (Uniform Frame Sampling \- UFS)**

Самый простой метод — обработка каждого N-го кадра.

- **Преимущества:** Простота реализации, предсказуемость нагрузки, сохранение временной линейности (что важно для некоторых моделей действий).
- **Недостатки:** «Слепота» к контенту. UFS тратит ресурсы на статические сцены и может пропустить кратковременные события, попавшие в интервал пропуска.
- **Эмпирические данные:** Исследования показывают, что точность обнаружения объектов (mAP) растет с увеличением частоты выборки, но имеет насыщение. Например, при анализе видео увеличение числа кадров с 16 до 256 дает резкий прирост точности, но дальнейшее увеличение до 600 кадров может даже снизить результаты из\-за накопления шума и избыточности.

### **4.2 Адаптивная выборка (Adaptive Frame Sampling \- AFS)**

Более продвинутая техника, соответствующая уровню «Магистра Времени» в легенде квеста. Идея состоит в том, чтобы динамически изменять частоту выборки в зависимости от информативности сцены.

#### **4.2.1 Детекция движения (Motion-Based)**

Алгоритм вычисляет разницу между текущим и предыдущим кадром. Если изменения превышают порог (Threshold), частота выборки повышается.

- **Реализация:**
  1. Преобразование в оттенки серого (cv2.cvtColor).
  2. Размытие по Гауссу для подавления шума сенсора.
  3. Вычисление абсолютной разности (cv2.absdiff).
  4. Бинаризация (cv2.threshold) и подсчет ненулевых пикселей.
- **Оптический поток:** Более сложный вариант AFS использует разреженный (Lucas-Kanade) или плотный (Farneback) оптический поток для оценки векторов движения. Это позволяет игнорировать глобальное движение (дрожание камеры) и реагировать только на движение объектов.
- **Результативность:** В задачах обнаружения малых целей (например, дронов или дефектов) адаптивная выборка на основе взаимной информации (mutual information) позволяет достичь скорости инференса 37 FPS с точностью 98%, концентрируя внимание модели только на значимых кадрах.

#### **4.2.2 Выделение ключевых кадров (Keyframe Extraction)**

Метод направлен на создание «резюме» видео путем выбора наиболее репрезентативных кадров. Это критически важно для задач индексации и поиска (RAG \- Retrieval-Augmented Generation).

- **Гистрограммный анализ:** Сравнение цветовых гистограмм кадров. Резкое изменение гистограммы сигнализирует о смене плана или освещения. Такие кадры автоматически помечаются как ключевые.
- **Кластеризация:** Алгоритмы типа K-Means могут сгруппировать все кадры видео по визуальному сходству и выбрать центроиды кластеров как ключевые кадры. Это устраняет дублирование информации, но требует предварительного прохода по всему видео.
- **Семантическая выборка (AKS):** Новейшие методы (2024-2025 гг.) используют мультимодальные модели (LLMs) для оценки «релевантности» кадра запросу пользователя. Метод Adaptive Keyframe Sampling (AKS) оптимизирует баланс между релевантностью кадра и покрытием видео, значительно превосходя равномерную выборку в задачах ответов на вопросы по видео (VideoQA).

## **5\. Обнаружение смены сцен и семантическая сегментация**

Для корректной работы алгоритмов прореживания необходимо понимать структуру видеомонтажа. Резкая смена сцены (Shot Boundary) означает разрыв временного контекста. Если трекер объекта или накопитель движения не сбросится при смене сцены, данные будут загрязнены ложными векторами движения.

### **5.1 Методы корреляции гистограмм**

В библиотеке OpenCV функция cv2.compareHist предоставляет мощный инструмент для детекции смены сцен.

- **Методика:** Гистограммы двух последовательных кадров сравниваются с использованием метрики корреляции (CV_COMP_CORREL). Значение близкое к 1.0 означает идентичность. Падение значения ниже порога (например, 0.7 или 0.8) указывает на смену сцены.
- **Нюансы реализации:** Для устойчивости к изменениям яркости рекомендуется использовать цветовое пространство HSV и строить гистограмму только по каналу Hue (Оттенок) или использовать нормализованные гистограммы.

### **5.2 Инструменты и библиотеки**

Существуют специализированные инструменты, такие как PySceneDetect, которые реализуют адаптивные пороговые значения и детектируют не только резкие склейки (cuts), но и плавные переходы (fades/dissolves). Эти инструменты анализируют яркостную компоненту (Luminance) и позволяют автоматически разбивать видео на логические сегменты перед применением тяжелых алгоритмов аналитики. В контексте нашего квеста, интеграция детектора сцен позволяет запускать «полный анализ» только на первом кадре каждой новой сцены, а далее переходить в режим экономичного мониторинга.

## **6\. Скользящие окна и темпоральный контекст в распознавании действий**

Для задач распознавания действий (Action Recognition) одиночный кадр часто не несет смысла. Действия «бег», «махание рукой» или «падение» существуют только во времени. Здесь прореживание кадров трансформируется в технику **Скользящего Окна (Sliding Window)**.

### **6.1 Параметры окна**

Вместо обработки потока покадрово, система накапливает буфер (окно) длительностью T (например, 32 кадра или 2 секунды).

- **Длина окна:** Исследования показывают, что для распознавания человеческой активности оптимальная длина окна составляет от 2.5 до 3.5 секунд. Слишком короткое окно разрывает паттерн движения, слишком длинное — смешивает несколько действий.
- **Перекрытие (Overlap/Stride):** Окна обычно перекрываются (например, на 50%), чтобы не потерять действие, происходящее на границе окон. Однако использование перекрытия увеличивает вычислительную нагрузку. В некоторых сценариях (subject-independent cross validation) доказано, что использование неперекрывающихся окон дает сопоставимую точность при значительно меньших затратах.

### **6.2 Внутреннее прореживание окна**

Даже внутри окна не обязательно использовать все кадры. Современные архитектуры (например, SlowFast или TimeSformer) часто принимают на вход разреженный тензор, например, 8 или 16 кадров, равномерно выбранных из 64-кадрового окна. Это позволяет модели охватить большой временной интервал (receptive field), не раздувая размер входных данных. Это подтверждает гипотезу о том, что временная структура видео обладает огромной избыточностью, и нейронные сети способны восстанавливать контекст движения даже по редким опорным кадрам.

## **7\. Экономическая архитектура облачной видеоаналитики**

Техника прореживания кадров имеет прямое денежное выражение при использовании облачных сервисов (AWS, Google Cloud, Azure). Ценообразование этих платформ создает мощные стимулы для внедрения Edge-Side Sampling (прореживания на стороне устройства).

### **7.1 Арбитраж стоимости API: Image vs Video**

Облачные провайдеры предлагают два пути анализа:

1. **Video API:** Вы загружаете видеофайл или подключаете поток. Платформа сама нарезает кадры и берет плату за _минуту_ видео.
   - _AWS Rekognition Video:_ \~$0.10 за минуту.
   - _Google Video Intelligence:_ \~$0.10 за минуту \+ надбавки за дополнительные функции.
2. **Image API:** Вы сами нарезаете кадры и отправляете их как отдельные изображения.
   - _AWS Rekognition Image:_ \~$1.00 за 1000 изображений.

**Сравнительный расчет (Экономическая магия):** Предположим, нам нужно мониторить видеопоток на предмет запрещенного контента или наличия людей.

- **Сценарий А (Video API):** 1 час видео \= 60 минут \* $0.10 \= **$6.00**.
- **Сценарий Б (Image API с прореживанием 1 FPS):** 1 час \= 3600 кадров. 3.6 тыс \* $0.001 \= **$3.60**. (Экономия 40%).
- **Сценарий В (Image API с адаптивным прореживанием):** Если сцена статична 90% времени, мы отправляем кадр только при детекции движения (в среднем 1 кадр в 10 секунд). 1 час \= 360 кадров. Стоимость \= **$0.36**.

**Вывод:** Использование интеллектуального прореживания кадров _до_ отправки в облако может снизить расходы в 15-20 раз по сравнению с использованием стандартных Video API. Это и есть высшая магия «Школы Управления Временем» — превращение алгоритмической эффективности в финансовый капитал.

### **7.2 Стоимость инференса и обучения**

При обучении собственных моделей (Custom Models) прореживание также играет ключевую роль. Обучение на видео 30 FPS требует в 30 раз больше операций (FLOPs), чем на видео 1 FPS, при этом информационная ценность каждого дополнительного кадра стремительно падает. Стоимость инференса модели на AWS (например, 44 часа в день на инстансе) может составлять $180/день для полного потока, против $2/день для оптимизированного потока.  
\#\# 8\. Риски, ограничения и граничные случаи  
Внедряя технику прореживания, инженер должен четко осознавать границы её применимости.

### **8.1 Проблема «Моргания глазом» (Blink of an Eye)**

В судебной видеоэкспертизе часто приводится пример: при частоте 1 кадр в 5 секунд (0.2 FPS) человек может пройти от двери до трибуны, и камера зафиксирует только начальную и конечную точки. Все, что произошло в промежутке (передача предмета, удар, падение), выпадает из реальности. Это создает риск ложной интерпретации событий (interpolation bias). Для систем безопасности (Security) прореживание ниже 10-15 FPS считается рискованным. Рекомендуемая архитектура: запись на диск (NVR) идет с полной частотой (30 FPS), а аналитика реального времени (Alerting) работает на прореженном потоке (5 FPS).

### **8.2 Дрожание и потери кадров (Frame Drops)**

Неконтролируемое прореживание (когда система просто не успевает обрабатывать кадры и «роняет» их) опаснее, чем алгоритмическое. Дропы обычно случаются в моменты пиковой активности (много движения \-\> выше битрейт \-\> выше нагрузка на декодер), то есть именно тогда, когда данные наиболее важны. Использование буферизированных потоков и очередей (Queue) обязательно для предотвращения потери критических кадров.

### **8.3 Инфракрасные и тепловизионные данные**

В специфических спектрах (ИК) движение объектов часто размыто, а контраст низок. Стандартные методы оценки движения (разница пикселей) могут не сработать. Здесь требуются специализированные методы выравнивания признаков (feature alignment) и оценки взаимной информации, чтобы прореживание не удалило малозаметную цель.

## **9\. Заключение**

Техника «Прореживания Кадров» (Frame Sampling) в видеоаналитике — это фундаментальный инструмент балансировки между информационной полнотой и ресурсной эффективностью. Исследование показало, что:

1. **На уровне реализации:** Использование cap.grab() и cap.retrieve() вместо наивного cap.read() является обязательным паттерном программирования для эффективного пропуска кадров, позволяя избегать избыточного декодирования.
2. **На уровне алгоритмов:** Адаптивная выборка (AFS), управляемая движением или семантической значимостью, превосходит равномерную выборку (UFS), позволяя достигать высокой точности при кратно меньшем объеме данных.
3. **На уровне экономики:** Грамотное прореживание на периферии (Edge) перед отправкой в облако способно сократить операционные расходы на порядки, открывая возможности для масштабирования систем, которые ранее были экономически нецелесообразны.

В метафорическом смысле «Школы Управления Временем», мы научились не просто «пропускать» моменты, но выделять из потока времени его квинтэссенцию, отбрасывая пустую оболочку избыточности. Это и есть истинное мастерство оптимизации.

### **Таблица 1\. Сравнительный анализ методов прореживания**

| Метод                              | Сложность реализации | Вычислительная стоимость | Устойчивость к шуму                   | Типичный сценарий использования                  |
| :--------------------------------- | :------------------- | :----------------------- | :------------------------------------ | :----------------------------------------------- |
| **Равномерная (Naive)**            | Низкая               | Низкая                   | Высокая                               | Time-lapse, общий мониторинг, стабильные условия |
| **Детекция движения (Pixel Diff)** | Низкая               | Низкая                   | Низкая (ложные срабатывания от света) | Охрана помещений, простые триггеры записи        |
| **Оптический поток**               | Высокая              | Высокая                  | Средняя                               | Трекинг объектов, анализ поведения               |
| **Гистограммные ключи**            | Средняя              | Средняя                  | Средняя (устойчивость к яркости)      | Резюмирование видео, детекция монтажных склеек   |
| **Семантическая (AI/CLIP)**        | Очень высокая        | Очень высокая            | Очень высокая                         | Умный поиск по архиву, сложные запросы (QA)      |

### **Таблица 2\. Моделирование затрат на облачную аналитику (на 1 камеру/мес)**

\*Расчет для разрешения 1080p, непрерывный мониторинг 24/7, цены AWS US-East \*

| Стратегия                   | Обрабатываемый FPS        | Кол-во запросов API | Оценка стоимости (мес) | Экономия vs База |
| :-------------------------- | :------------------------ | :------------------ | :--------------------- | :--------------- |
| **Video API (Full Stream)** | N/A (по длительности)     | N/A                 | \~$427.00              | 0% (База)        |
| **Image API (1 FPS)**       | 1 FPS                     | 2,592,000           | \~$2,280.00            | \-433% (Убыток)  |
| **Image API (Разреженная)** | 0.1 FPS (1 кадр/10 сек)   | 259,200             | \~$260.00              | 39%              |
| **Edge Adaptive (Гибрид)**  | Переменный (Avg 0.05 FPS) | \~100,000           | \~$100.00              | **76%**          |

#### **Источники**

1\. Improving LLM Video Understanding with 16 Frames Per Second \- arXiv, https://arxiv.org/html/2503.13956v1 2\. Key frame extraction algorithm for surveillance videos using an evolutionary approach \- NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC11696284/ 3\. What are best practices for frame sampling and selection? \- Milvus, https://milvus.io/ai-quick-reference/what-are-best-practices-for-frame-sampling-and-selection 4\. How to prevent Frame Skipping in IP Cameras \- CCTV Camera World, https://www.cctvcameraworld.com/how-to-prevent-frame-skipping-ip-cameras/ 5\. Frame Sampling Strategies Matter: A Benchmark for small vision language models \- arXiv, https://arxiv.org/html/2509.14769v1 6\. Exploring Video Frame Redundancies for Efficient Data Sampling and Annotation in Instance Segmentation \- CVF Open Access, https://openaccess.thecvf.com/content/CVPR2023W/VDU/papers/Yoon\_Exploring\_Video\_Frame\_Redundancies\_for\_Efficient\_Data\_Sampling\_and\_Annotation\_CVPRW\_2023\_paper.pdf 7\. Wagon-wheel effect \- Wikipedia, https://en.wikipedia.org/wiki/Wagon-wheel\_effect 8\. Frame Rate: Understanding the Impact of Frame Rate on Video Interpretation, https://www.jonathanhak.com/2018/03/03/frame-rate-understanding-the-impact-of-frame-rate-on-video-interpretation/ 9\. Reading and Writing Videos using OpenCV, https://opencv.org/blog/reading-and-writing-videos-using-opencv/ 10\. Opencv VideoCapture set CV_CAP_PROP_POS_FRAMES not working \- Stack Overflow, https://stackoverflow.com/questions/19404245/opencv-videocapture-set-cv-cap-prop-pos-frames-not-working 11\. CAP_PROP_POS_FRAMES is abnormal slow \- Python \- OpenCV Forum, https://forum.opencv.org/t/cap-prop-pos-frames-is-abnormal-slow/11651 12\. Faster video file FPS with cv2.VideoCapture and OpenCV \- PyImageSearch, https://pyimagesearch.com/2017/02/06/faster-video-file-fps-with-cv2-videocapture-and-opencv/ 13\. C14 5 Wagon wheel effect \- YouTube, https://www.youtube.com/watch?v=VNftf5qLpiA 14\. Temporal Aliasing (Wagon Wheel/Stroboscopic Effect) \- Q3 Media Training, https://q3mediatraining.co.uk/knowledge/temporal-aliasing/ 15\. The Wagon Wheel Effect \- Temporal Aliasing \- YouTube, https://www.youtube.com/watch?v=QYYK4tlCMlY 16\. mmaaz60/SkipVideoFramesUsingOpenCV: This repository contains two different methods to skip frames using OpenCV. It also provides speed comparison of two methods. \- GitHub, https://github.com/mmaaz60/SkipVideoFramesUsingOpenCV 17\. Difference between video capture read and grab \- Stack Overflow, https://stackoverflow.com/questions/57716962/difference-between-video-capture-read-and-grab 18\. Fast way to iterate through video frames with Python and OpenCV \- Tam Vu, https://vuamitom.github.io/2019/12/13/fast-iterate-through-video-frames.html 19\. OpenCV VideoCapture set(CAP_PROP_POS_FRAMES) taking some time \- Stack Overflow, https://stackoverflow.com/questions/73588631/opencv-videocapture-setcap-prop-pos-frames-taking-some-time 20\. OpenCV getting very slow when using cap.set(cv2.CAP_PROP_POS_FRAMES, https://stackoverflow.com/questions/59951178/opencv-getting-very-slow-when-using-cap-setcv2-cap-prop-pos-frames 21\. Help with optimization opencv videocapture optical flow analysis \- Python, https://forum.opencv.org/t/help-with-optimization-opencv-videocapture-optical-flow-analysis/17068 22\. OpenCV. How do you catch a real time frame from OpenCV Video Capture, https://forum.opencv.org/t/opencv-how-do-you-catch-a-real-time-frame-from-opencv-video-capture/14880 23\. OpenCV real time streaming video capture is slow. How to drop frames or get synced with real time? \- Stack Overflow, https://stackoverflow.com/questions/58293187/opencv-real-time-streaming-video-capture-is-slow-how-to-drop-frames-or-get-sync 24\. Speeding up frame capture in opencv : r/computervision \- Reddit, https://www.reddit.com/r/computervision/comments/eoos6m/speeding\_up\_frame\_capture\_in\_opencv/ 25\. Adaptive Keyframe Sampling for Long Video Understanding \- arXiv, https://arxiv.org/html/2502.21271v1 26\. How to determine average x \+ y motion of keypoints in video? \- Python \- OpenCV Forum, https://forum.opencv.org/t/how-to-determine-average-x-y-motion-of-keypoints-in-video/8983 27\. Adaptive Frame Sampling and Feature Alignment for Multi-Frame Infrared Small Target Detection \- MDPI, https://www.mdpi.com/2076-3417/14/14/6360 28\. The comparison and analysis of extracting video key frame \- ResearchGate, https://www.researchgate.net/publication/325189881\_The\_comparison\_and\_analysis\_of\_extracting\_video\_key\_frame 29\. Analysis of Various Keyframe Extraction Methods \- Research Publish Journals, https://www.researchpublish.com/upload/book/Analysis%20of%20Various%20Keyframe-3090.pdf 30\. A Keyframe Extraction Method for Assembly Line Operation Videos Based on Optical Flow Estimation and ORB Features \- NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC12074371/ 31\. Python API \- PySceneDetect, https://www.scenedetect.com/api/ 32\. Histogram Comparison \- OpenCV Documentation, https://docs.opencv.org/3.4/d8/dc8/tutorial\_histogram\_comparison.html 33\. How to Detect Scene Cuts in Video with PySceneDetect \- Sieve, https://www.sievedata.com/resources/how-to-detect-scene-cuts-in-video-pyscenedetect 34\. Impact of Sliding Window Length in Indoor Human Motion Modes and Pose Pattern Recognition Based on Smartphone Sensors \- MDPI, https://www.mdpi.com/1424-8220/18/6/1965 35\. A Quantitative Comparison of Overlapping and Non-Overlapping Sliding Windows for Human Activity Recognition Using Inertial Sensors \- PMC \- NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC6891351/ 36\. Human Interaction Classification in Sliding Video Windows Using Skeleton Data Tracking and Feature Extraction \- NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC10384121/ 37\. Effects of sliding window variation in the performance of acceleration-based human activity recognition using deep learning models \- PeerJ, https://peerj.com/articles/cs-1052/ 38\. Image recognition software, ML image analysis, and video analysis – Amazon Rekognition pricing \- AWS, https://aws.amazon.com/rekognition/pricing/ 39\. Video Intelligence API pricing \- Google Cloud, https://cloud.google.com/video-intelligence/pricing 40\. How to decide between Amazon Rekognition image and video API for video moderation, https://aws.amazon.com/blogs/machine-learning/how-to-decide-between-amazon-rekognition-image-and-video-api-for-video-moderation/ 41\. Minimizing Video Frame Drops In Video Surveillance Systems \- SecurityInformed.com, https://www.securityinformed.com/insights/minimising-video-frame-drops-video-surveillance-co-14796-ga.22721.html
