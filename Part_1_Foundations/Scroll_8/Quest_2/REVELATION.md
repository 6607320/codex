# **Архитектурные парадигмы распознавания действий в видео: Сравнительный анализ стратегий временной агрегации с использованием Transfer Learning на разреженных кадрах**

## **1\. Введение: Эволюция восприятия от статики к динамике**

### **1.1 Фундаментальный вызов четвертого измерения**

В современной области компьютерного зрения переход от анализа статических изображений к пониманию видеопотока представляет собой один из наиболее значимых и сложных технологических скачков. Если классификация изображений, достигшая зрелости благодаря архитектурам сверточных нейронных сетей (CNN), оперирует пространственными данными, то распознавание действий в видео (Video Action Recognition) вводит критически важную переменную — время (T). Видеофайл не является просто набором картинок; это структурированная последовательность, где семантический смысл часто заключен не в пикселях отдельного кадра, а в трансформации этих пикселей между кадрами.  
Задача, поставленная в рамках Квеста 8.2, — создание классификатора действий с использованием Transfer Learning (переноса обучения) на разреженных кадрах и сравнение двух полярных подходов к агрегации временной информации: усреднения эссенций (Global Average Pooling) и рекуррентных нейронных сетей (RNN/LSTM) — затрагивает самую суть проблемы видеоанализа. Это выбор между восприятием видео как «мешка визуальных слов» (Bag of Frames) и восприятием видео как упорядоченной временной структуры.  
Основная сложность здесь заключается в так называемом «проклятии размерности». Стандартный видеоклип длительностью 10 секунд, снятый с частотой 30 кадров в секунду (fps), содержит 300 кадров. Если каждый кадр представляет собой тензор размерности 224 \\times 224 \\times 3 (высота, ширина, цветовые каналы RGB), то общий объем данных для одного экземпляра обучения превышает объем статического изображения в 300 раз. Попытка обработать такой объем данных с помощью глубоких нейронных сетей «в лоб» (dense sampling) требует колоссальных вычислительных ресурсов, недоступных для большинства прикладных задач, особенно работающих в реальном времени или на мобильных устройствах.

### **1.2 Концепция разреженного сэмплирования (Sparse Sampling)**

Ответом индустрии на проблему вычислительной сложности стало внедрение методологии разреженного сэмплирования (Sparse Sampling). Эта техника базируется на гипотезе о высокой временной избыточности видеосигнала. В видеопотоке, изображающем, например, идущего человека, различия между кадром t и кадром t+1 (через 33 миллисекунды) часто ничтожны и находятся на уровне шума сенсора или микроскопических смещений контуров. Семантическая информация — фаза шага — меняется гораздо медленнее.  
Следовательно, обработка каждого кадра является не только дорогой, но и избыточной операцией. Разреженное сэмплирование предлагает стратегию, при которой из всего видеопотока извлекается фиксированное, небольшое количество кадров K (например, K=8, K=16 или K=32), которые равномерно распределены по всей временной шкале клипа. Это позволяет модели охватить глобальную темпоральную структуру действия (начало, середину, конец), игнорируя высокочастотную избыточность.  
Однако, как только мы переходим от непрерывного видеопотока к набору из K дискретных кадров, возникает архитектурный вопрос: как объединить информацию из этих разрозненных "снимков" реальности в единое предсказание класса? Именно здесь происходит развилка, исследуемая в данном отчете: должны ли мы считать эти кадры независимыми элементами множества, чья совокупность определяет контекст (усреднение), или же мы обязаны рассматривать их как жестко упорядоченную последовательность, где перестановка слагаемых меняет сумму (LSTM)?

## **2\. Теоретические основы извлечения признаков: Transfer Learning и MobileNetV2**

### **2.1 Философия переноса обучения (Transfer Learning)**

Построение эффективного классификатора видеодействий "с нуля" (from scratch) является задачей, граничащей с невозможным для большинства исследовательских и прикладных проектов из\-за необходимости наличия миллионов размеченных видеопримеров. Нейронные сети требуют огромных объемов данных для того, чтобы научиться распознавать базовые примитивы: грани, углы, текстуры, простые геометрические формы.  
Методология Transfer Learning позволяет обойти это ограничение, используя знания, накопленные моделями при обучении на фундаментальных наборах данных статических изображений, таких как ImageNet (содержащем 1.2 миллиона изображений в 1000 классах). Гипотеза состоит в том, что визуальные признаки, необходимые для распознавания "собаки" или "автомобиля" на статичном фото, идентичны признакам, необходимым для распознавания этих объектов в видеопотоке. Таким образом, мы можем использовать мощную сверточную сеть (CNN) в качестве "экстрактора признаков" (feature extractor), заморозив её веса и обучая только механизмы временной агрегации.

### **2.2 Архитектура MobileNetV2: Оптимизация для инференса**

Для задач видеоанализа выбор базовой архитектуры (backbone) имеет критическое значение. Поскольку эта сеть должна быть запущена K раз для каждого видео (по одному прогону на каждый сэмплированный кадр), вычислительная сложность умножается на K. Использование тяжеловесных архитектур, таких как ResNet-152 или VGG-19, сделало бы процесс обучения и инференса неоправданно медленным.  
В данном исследовании в качестве опорной сети выбрана **MobileNetV2**. Эта архитектура была специально разработана инженерами Google для мобильных и встраиваемых устройств, где ресурсы процессора и батареи ограничены. Её эффективность базируется на двух ключевых инновациях:

1. **Разделимые свертки по глубине (Depthwise Separable Convolutions):** В стандартной свертке каждый фильтр одновременно обрабатывает пространственную информацию и смешивает каналы. MobileNetV2 факторизует этот процесс на два этапа:
   - _Depthwise Convolution:_ Применяет отдельный фильтр к каждому входному каналу (пространственная фильтрация).
   - _Pointwise Convolution:_ Применяет свертку 1 \\times 1 для объединения выходов каналов (линейная комбинация). Это снижает количество параметров и операций умножения-сложения (MAdd) в 8-9 раз по сравнению со стандартной сверткой при минимальной потере точности.
2. **Инвертированные остаточные блоки (Inverted Residuals):** В классическом ResNet блок сжимает размерность каналов перед сверткой (bottleneck), а затем восстанавливает её. MobileNetV2 делает наоборот: сначала расширяет количество каналов (expansion) с помощью 1 \\times 1 свертки, применяет нелинейность ReLU6 к этому многомерному представлению, делает depthwise свертку, а затем проецирует результат обратно в малоразмерное пространство с помощью линейной (без функции активации) 1 \\times 1 свертки. Линейность "бутылочного горлышка" (linear bottleneck) критически важна, так как применение ReLU в пространстве низкой размерности приводит к потере информации.

### **2.3 Извлечение "Эссенций" (Feature Extraction Process)**

В контексте Квеста 8.2 термин "эссенция" (essence) используется для описания высокоуровневого векторного представления кадра, полученного на выходе сверточной части сети. Это дистиллированная визуальная информация, очищенная от пиксельного шума.  
Процесс получения эссенций выглядит следующим образом:

1. **Входные данные:** Тензор размерности (B, T, 3, H, W), где B — размер батча, T — количество кадров, H, W — пространственное разрешение (обычно 224 \\times 224).
2. **Трансформация:** Тензор переформатируется (reshape) в (B \\cdot T, 3, H, W), чтобы подать каждый кадр как независимое изображение в CNN.
3. **Проход через MobileNetV2:** Данные проходят через все сверточные слои. Мы отсекаем последний классификационный слой (голову на 1000 классов ImageNet).
4. **Выход бэкбона:** На выходе последнего сверточного слоя (Conv2d_1x1) мы получаем тензор признаков (feature map) размерности (B \\cdot T, 1280, 7, 7). Здесь 1280 — количество каналов признаков, а 7 \\times 7 — пространственная сетка.
5. **Пространственный пулинг:** К этому тензору применяется операция Global Average Pooling (GAP) по пространственным измерениям (7 \\times 7), превращая его в вектор (B \\cdot T, 1280).
6. **Восстановление структуры:** Тензор переформатируется обратно в (B, T, 1280).

Теперь каждый видеофрагмент представлен как последовательность из T векторов, каждый длиной 1280 чисел. Именно с этими "эссенциями" будут работать сравниваемые методы агрегации.

## **3\. Методология А: Усреднение эссенций (Global Average Pooling)**

### **3.1 Концептуальная модель "Мешка кадров"**

Метод усреднения эссенций, часто упоминаемый в литературе как **Temporal Global Average Pooling (TGAP)**, реализует концепцию "Мешка кадров" (Bag of Frames). Эта аналогия заимствована из обработки естественного языка (Bag of Words), где текст классифицируется на основе частоты встречаемости слов, без учета грамматики и порядка их следования. В видеоанализе этот подход предполагает, что действие можно определить по совокупности визуальных признаков, присутствующих в кадрах, независимо от их хронологического порядка.  
Математически операция выражается предельно просто. Пусть v_t \\in \\mathbb{R}^{D} — вектор признаков (эссенция) кадра t, где D=1280. Результирующий вектор видео V\_{out} вычисляется как:  
Полученный вектор V\_{out} затем подается на полносвязный слой (Fully Connected Layer) для получения логитов классов:  
Где W \\in \\mathbb{R}^{N\_{class} \\times D} и b \\in \\mathbb{R}^{N\_{class}} — обучаемые веса и смещения классификатора.

### **3.2 Анализ преимуществ: Эффективность и Стабильность**

Популярность этого метода, несмотря на его теоретическую простоту, объясняется рядом прагматических преимуществ, выявленных в ходе исследований :

1. **Отсутствие дополнительных параметров:** Операция усреднения не имеет обучаемых весов. Вся "емкость" модели сосредоточена в бэкбоне (который часто заморожен или только тонко настраивается) и финальном слое. Это делает модель чрезвычайно компактной и снижает риск переобучения (overfitting), что критично при работе с небольшими датасетами.
2. **Вычислительная эффективность:** Усреднение — это линейная операция с мизерной вычислительной стоимостью (O(T) сложений). Она практически не добавляет задержки (latency) в процесс инференса.
3. **Скорость сходимости:** Поскольку нет сложной рекуррентной динамики, градиенты от функции потерь распространяются напрямую к выходам бэкбона. Обучение таких моделей происходит значительно быстрее и стабильнее, чем обучение RNN.
4. **Инвариантность к перестановкам:** Для многих классов действий порядок действительно не важен. Например, класс "Плавание" (Swimming) характеризуется наличием воды, пловца и брызг. Неважно, увидим ли мы сначала кадр с гребком левой рукой, а потом правой, или наоборот. Усреднение признаков отлично улавливает этот глобальный контекст.

### **3.3 Фундаментальные ограничения: Слепота к структуре**

Главный недостаток метода вытекает из его же главного свойства — инвариантности к порядку. Усреднение эссенций полностью уничтожает информацию о временной последовательности.

- **Проблема обратимых действий:** Рассмотрим два действия: "Открывание двери" и "Закрывание двери". Если проиграть видео открывания двери в обратном порядке, мы получим видео закрывания. Набор кадров (а значит, и набор векторов-эссенций \\{v*1,..., v_T\\}) в обоих случаях идентичен. Следовательно, их среднее арифметическое будет одинаковым: \\frac{1}{T}\\sum v_t. Классификатор, основанный на усреднении, *математически не способен\_ различить эти два класса, если они присутствуют в одном датасете.
- **Смешение состояний:** Если видео содержит последовательность состояний A \\to B \\to C, усреднение создает вектор, представляющий некое "среднее" состояние, которое может не соответствовать ничему реальному. Например, смесь признаков "целая чашка" и "разбитая чашка" может создать шумный вектор, затрудняющий классификацию.

## **4\. Методология Б: Рекуррентное моделирование (RNN/LSTM)**

### **4.1 Необходимость памяти в системе**

Второй подход рассматривает видео как временной ряд (Time Series). Здесь каждый кадр t является контекстом для кадра t+1. Для моделирования такой зависимости используются Рекуррентные Нейронные Сети (RNN). В отличие от сетей прямого распространения (Feedforward), RNN имеют внутреннее состояние (hidden state), которое обновляется на каждом шаге, сохраняя информацию о предыдущих входах.  
Однако классические ("ванильные") RNN страдают от проблемы затухающих градиентов (vanishing gradient problem). При обратном распространении ошибки через множество временных шагов градиенты, являющиеся произведением матриц весов, имеют тенденцию либо экспоненциально уменьшаться до нуля (сеть перестает учиться), либо взрываться. В контексте видео, где зависимости могут быть растянуты во времени, это делает обучение стандартных RNN крайне сложным.

### **4.2 Архитектура LSTM (Long Short-Term Memory)**

Для решения этой проблемы была разработана архитектура LSTM. Она вводит понятие "ячейки памяти" (cell state C_t), которая действует как информационная магистраль, проходящая сквозь всю цепочку с минимальными линейными взаимодействиями, что позволяет градиентам течь беспрепятственно.  
В рамках нашего классификатора, LSTM принимает на вход последовательность эссенций (v_1, v_2,..., v_T). На каждом шаге t происходят следующие процессы :

1. **Forget Gate (Вентиль забывания) f_t:** Решает, какую информацию из предыдущего состояния ячейки C\_{t-1} нужно удалить. Это сигмоидальный слой, смотрящий на h\_{t-1} и v_t.
2. **Input Gate (Вентиль ввода) i_t:** Решает, какую новую информацию записать в ячейку.
3. **Update Cell State:** Старое состояние умножается на забывание, добавляется новое.
4. **Output Gate (Вентиль выхода) o_t:** Определяет, что будет в скрытом состоянии h_t (которое используется для предсказания и передается на следующий шаг).

### **4.3 Стратегия агрегации в LSTM**

В задаче классификации видео (many-to-one) обычно используется только **последнее скрытое состояние** h_T. Это состояние теоретически содержит в себе сжатую информацию обо всей просмотренной последовательности кадров, учитывая их порядок и причинно-следственные связи. Именно этот вектор h_T подается на финальный полносвязный классификатор.

### **4.4 Анализ преимуществ: Понимание динамики**

1. **Чувствительность к порядку:** LSTM фундаментально различает последовательности A \\to B и B \\to A. В примере с дверью, сеть сформирует совершенно разные скрытые состояния для процессов открывания и закрывания, так как эволюция вектора состояния будет идти по разным траекториям в пространстве признаков.
2. **Моделирование причинности:** LSTM способна улавливать сложные зависимости, например: "действие считается ударом, только если сначала был замах, а затем резкое движение вперед". Усреднение увидело бы просто "замах" и "движение", не связывая их в единый паттерн.

### **4.5 Ограничения и сложности обучения**

1. **Вычислительная сложность:** LSTM добавляет значительное количество параметров (четыре матрицы весов для каждого слоя). Это увеличивает размер модели и требует больше памяти GPU.
2. **Сложность оптимизации:** Обучение LSTM требует использования алгоритма Backpropagation Through Time (BPTT). Это ресурсоемкая операция. Кроме того, несмотря на архитектуру ячейки, обучение на очень длинных последовательностях все еще может быть нестабильным (требуется gradient clipping).
3. **Риск переобучения:** Из-за большого числа параметров LSTM склонна к переобучению на малых датасетах, запоминая конкретные последовательности шума вместо общих паттернов движения.

## **5\. Сравнительный анализ и экспериментальные данные**

Сравнение этих двух подходов невозможно без контекста данных. Эффективность того или иного метода напрямую зависит от природы классифицируемых действий. В исследовательском сообществе принято разделять видео-датасеты на "Пространственные" (Spatial) и "Временные" (Temporal).

### **5.1 Пространственные датасеты: UCF101 и Sports-1M**

Датасет **UCF101** содержит видео различных спортивных активностей (Бейсбол, Теннис, Плавание). Исследования показывают, что разрыв в точности между простым усреднением (или даже анализом одного кадра) и сложными LSTM сетями на этом датасете минимален, а иногда LSTM показывает худшие результаты из\-за переобучения.

- **Инсайт:** Большинство классов в UCF101 можно определить по статическим объектам. "Стрельба из лука" (Archery) определяется наличием лука. "Гребля" определяется наличием лодки и воды. Движение (motion) здесь вторично. Для таких задач метод **Усреднения Эссенций** является предпочтительным выбором: он быстрее, легче обучается и дает сопоставимую точность (около 85-88% при использовании качественного бэкбона и оптического потока).

### **5.2 Временные датасеты: Something-Something V2**

Датасет **Something-Something V2 (SSv2)** был специально создан для того, чтобы бросить вызов моделям, полагающимся на статический контекст. Классы здесь определяются не объектами, а взаимодействиями: "Moving something from left to right" (Двигаем что-то слева направо), "Dropping something" (Бросаем что-то). Объекты могут быть любыми: чашка, телефон, камень.

- **Инсайт:** В SSv2 статический кадр (эссенция) несет мало информации о классе. Кадр с чашкой в руке может относиться к классу "Поднимаем чашку", "Опускаем чашку" или "Держим чашку". Только последовательность изменений координат и состояний позволяет сделать верный вывод.
- **Результат:** На таких данных метод **Усреднения Эссенций** терпит фиаско, показывая результаты, близкие к случайному угадыванию. **RNN/LSTM** (и более современные трансформеры) здесь демонстрируют подавляющее преимущество, так как способны отследить вектор движения.

### **5.3 Сводная таблица характеристик**

В таблице ниже представлено обобщенное сравнение методов в контексте задачи Transfer Learning на MobileNetV2.

| Характеристика                         | Усреднение Эссенций (GAP)            | RNN / LSTM                                                  |
| :------------------------------------- | :----------------------------------- | :---------------------------------------------------------- |
| **Количество параметров агрегатора**   | 0 (Parameter-free)                   | Высокое (4 \\times (N\_{in} \\cdot N\_{hid} \+ N\_{hid}^2)) |
| **Вычислительная сложность**           | O(T) (линейное сложение)             | O(T \\cdot N\_{params}) (матричные операции)                |
| **Затраты памяти при обучении**        | Низкие                               | Высокие (хранение графа для BPTT)                           |
| **Устойчивость к переобучению**        | Высокая                              | Низкая (требует Dropout/Augmentation)                       |
| **Чувствительность к порядку**         | Нет (Инвариантен)                    | Высокая                                                     |
| **Применимость (UCF101)**              | Отличная                             | Хорошая (риск overfitting)                                  |
| **Применимость (Something-Something)** | Плохая                               | Отличная / Необходимая                                      |
| **Интерпретируемость**                 | Высокая (можно увидеть вклад кадров) | Низкая ("черный ящик" состояний)                            |

## **6\. Практические аспекты реализации и обучения (Квест 8.2)**

### **6.1 Подготовка данных: Пайплайн загрузки**

Критическим этапом является правильная реализация разреженного сэмплирования в коде (например, PyTorch).

1. **Декодирование:** Использование библиотек типа decord или torchvision.io предпочтительнее OpenCV, так как они позволяют эффективно осуществлять произвольный доступ (random seek) к кадрам без декодирования всего видеофайла.
2. **Сэмплирование:**
   - Видео разбивается на T сегментов равной длины.
   - Во время **обучения** из каждого сегмента выбирается случайный индекс кадра. Это работает как аугментация данных, предотвращая запоминание конкретных кадров.
   - Во время **валидации/теста** выбирается центральный кадр сегмента для детерминированности результата.
3. **Пространственные трансформации:** Крайне важно применять одинаковые трансформации (например, RandomCrop) ко всем кадрам одного видео в батче. Если первый кадр обрезан слева, а второй — справа, пространственная когерентность нарушится, что собьет с толку LSTM (хотя усреднение это переживет легче).

### **6.2 Особенности обучения LSTM**

При обучении ветки с LSTM необходимо учитывать ряд технических нюансов:

- **Инициализация:** Скрытые состояния h_0, c_0 обычно инициализируются нулями.
- **Dropout:** Обязательное использование Dropout (например, p=0.5) перед финальным линейным слоем и, возможно, внутри LSTM (между слоями, если их несколько), чтобы бороться с переобучением на малом объеме данных.
- **Размерность признаков:** Вектор размером 1280 может быть избыточным для входа в LSTM. Часто применяют дополнительный линейный слой (проекцию) для снижения размерности до 256 или 512 перед подачей в LSTM, что снижает количество параметров рекуррентного слоя.

### **6.3 Проблема "Статического смещения" (Static Bias)**

Важно понимать, что современные CNN настолько мощны, что они склонны "жульничать", находя корреляции в фоне. Если в датасете все видео с классом "Игра на пианино" сняты в помещении с определенным освещением, модель может запомнить освещение, а не движение рук. Усреднение эссенций усугубляет эту проблему, создавая "супер-статический" контекст. LSTM, принуждаемая искать изменения во времени, теоретически более устойчива к такой ошибке, но требует тщательной настройки регуляризации.

## **7\. Заключение и Рекомендации к Квесту**

В ходе анализа Квеста 8.2 мы установили, что выбор между усреднением эссенций и LSTM не является выбором между "плохим" и "хорошим" методом. Это выбор инструмента под конкретную задачу.  
**Для учебных целей и построения бейзлайна (Baseline):** Всегда следует начинать с **Усреднения Эссенций (GAP)**.

1. Это позволяет быстро отладить пайплайн данных (загрузку, препроцессинг).
2. Это дает нижнюю границу точности. Если ваша LSTM модель показывает результат хуже, чем GAP, значит, она не обучается (проблема с градиентами, гиперпараметрами) или переобучается.

**Для решения сложных задач (Production):** Использование **LSTM** (или GRU) необходимо, если классы действий зависят от временной структуры. Однако в современном State-of-the-Art (SOTA) на смену простым LSTM приходят более сложные механизмы:

- **Трансформеры (TimeSformer, ViT):** Используют механизм Self-Attention для анализа всех кадров одновременно, превосходя LSTM в способности захватывать долгосрочные зависимости.
- **VideoMAE:** Использует маскированное автокодирование для обучения мощных представлений, заставляя модель реконструировать пропущенные куски видео во времени.

Тем не менее, комбинация **MobileNetV2 \+ LSTM** остается "золотым стандартом" для понимания основ видеоанализа и создания эффективных решений для мобильных платформ, где развертывание гигантских Трансформеров невозможно. Для успешного прохождения квеста рекомендуется реализовать оба метода и провести сравнительный анализ на выбранном датасете, обращая внимание на ошибки классификации "обратимых" действий.

#### **Источники**

1\. TransNet: A Transfer Learning-Based Network for Human Action Recognition \- arXiv, https://arxiv.org/pdf/2309.06951 2\. Deep Learning for Videos: A 2018 Guide to Action Recognition \- Qure AI, https://www.qure.ai/blog/deep-learning-for-videos-a-2018-guide-to-action-recognition 3\. Temporal-Spatial Redundancy Reduction in Video Sequences: A Motion-Based Entropy-Driven Attention Approach \- PubMed Central, https://pmc.ncbi.nlm.nih.gov/articles/PMC12025262/ 4\. Human Interaction Classification in Sliding Video Windows Using Skeleton Data Tracking and Feature Extraction \- NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC10384121/ 5\. MobileNet for human activity recognition in smart surveillance using transfer learning, https://www.researchgate.net/publication/387139197\_MobileNet\_for\_human\_activity\_recognition\_in\_smart\_surveillance\_using\_transfer\_learning 6\. Transfer learning and fine-tuning | TensorFlow Core, https://www.tensorflow.org/tutorials/images/transfer\_learning 7\. Human Action Recognition Based on Transfer Learning Approach \- IEEE Xplore, https://ieeexplore.ieee.org/iel7/6287639/9312710/09447028.pdf 8\. A Deep Bidirectional LSTM Model Enhanced by Transfer-Learning-Based Feature Extraction for Dynamic Human Activity Recognition \- MDPI, https://www.mdpi.com/2076-3417/14/2/603 9\. MobileNet V2 \- Hugging Face, https://huggingface.co/docs/transformers/v4.38.0/model\_doc/mobilenet\_v2 10\. Pretrained CNN (MobileNet V2) output shape \- python \- Stack Overflow, https://stackoverflow.com/questions/69812600/pretrained-cnn-mobilenet-v2-output-shape 11\. The bag-of-frame approach to audio pattern recognition: A sufficient model for urban soundscapes but not for polyphonic music \- François Pachet, https://www.francoispachet.fr/wp-content/uploads/2021/01/aucouturier-07b.pdf 12\. The bag-of-frames approach to audio pattern recognition: A sufficient model for urban soundscapes but not for polyphonic music \- ResearchGate, https://www.researchgate.net/publication/6165924\_The\_bag-of-frames\_approach\_to\_audio\_pattern\_recognition\_A\_sufficient\_model\_for\_urban\_soundscapes\_but\_not\_for\_polyphonic\_music 13\. CAST: Cross-Attention in Space and Time for Video Action Recognition \- arXiv, https://arxiv.org/html/2311.18825v2 14\. A Critical Analysis on Machine Learning Techniques for Video-based Human Activity Recognition of Surveillance Systems: A Review \- arXiv, https://arxiv.org/html/2409.00731v1 15\. 7 PyTorch Pooling Methods You Should Be Using | by Benjamin Bodner | Medium, https://medium.com/@benjybo7/7-pytorch-pool-methods-you-should-be-using-495eb00325d6 16\. Temporal Relational Reasoning in Videos \- CVF Open Access, https://openaccess.thecvf.com/content\_ECCV\_2018/papers/Bolei\_Zhou\_Temporal\_Relational\_Reasoning\_ECCV\_2018\_paper.pdf 17\. What Happens When: Learning Temporal Orders of Events in Videos \- arXiv, https://arxiv.org/html/2512.08979v1 18\. Video Action Differencing \- arXiv, https://arxiv.org/html/2503.07860v1 19\. Retro-Actions: Learning 'Close' by Time-Reversing 'Open' Videos, https://openaccess.thecvf.com/content\_ICCVW\_2019/papers/MDALC/Price\_Retro-Actions\_Learning\_Close\_by\_Time-Reversing\_Open\_Videos\_ICCVW\_2019\_paper.pdf 20\. Comparative Analysis of Image, Video, and Audio Classifiers for Automated News Video Segmentation This work is part of the project 'Data-Informed Media Analysis Suite' (DIMAS). It is financed by Xjenza Malta and The Malta Digital Innovation Authority for and on behalf of the Foundation for Science and Technology through the FUSION: R\&I Thematic \- arXiv, https://arxiv.org/html/2503.21848v1 21\. Beyond Short Snippets: Deep Networks for Video Classification \- Google Research, https://research.google.com/pubs/archive/43793.pdf 22\. Video Prediction using ConvLSTM Autoencoder (PyTorch) | Andreas Holm Nielsen, https://holmdk.github.io/2020/04/02/video\_prediction.html 23\. Video Frame Prediction using ConvLSTM Network in PyTorch | by Rohit Panda | Medium, https://sladewinter.medium.com/video-frame-prediction-using-convlstm-network-in-pytorch-b5210a6ce582 24\. Human Activity Recognition using UCF101 based on LSTM and CNN \- Medium, https://medium.com/@Visal296/human-activity-recognition-using-ucf101-based-on-lstm-and-cnn-cnn-lstm-c339b06773f8 25\. Modeling Video Evolution for Action Recognition \- CVF Open Access, https://openaccess.thecvf.com/content\_cvpr\_2015/papers/Fernando\_Modeling\_Video\_Evolution\_2015\_CVPR\_paper.pdf 26\. Learning End-to-end Video Classification with Rank-Pooling \- Proceedings of Machine Learning Research, http://proceedings.mlr.press/v48/fernando16.pdf 27\. STUPD: A Synthetic Dataset for Spatial and Temporal Relation Reasoning \- arXiv, https://arxiv.org/html/2309.06680v3 28\. Brief Review — The “Something Something” Video Database for Learning and Evaluating Visual Common Sense \- Sik-Ho Tsang, https://sh-tsang.medium.com/brief-review-the-something-something-video-database-for-learning-and-evaluating-visual-common-e851dd3015a6 29\. MIT Open Access Articles Temporal Relational Reasoning in Videos, https://dspace.mit.edu/bitstream/handle/1721.1/125123/1711.08496.pdf?sequence=2\&isAllowed=y 30\. A Spatio-Temporal Motion Network for Action Recognition Based on Spatial Attention \- PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC8947561/ 31\. torchcodec: Easy and Efficient Video Decoding for PyTorch, https://pytorch.org/blog/torchcodec/ 32\. Action Recognition Models — MMAction2 1.2.0 documentation, https://mmaction2.readthedocs.io/en/latest/model\_zoo/recognition.html 33\. What is the usual way to sample frames for video classification? \- vision \- PyTorch Forums, https://discuss.pytorch.org/t/what-is-the-usual-way-to-sample-frames-for-video-classification/18235 34\. Preparing Video Data for Training: A PyTorch Guide | by Naneet Tyagi | Medium, https://medium.com/@naneettyagi2004/preparing-video-data-for-training-a-pytorch-guide-fc644ee9e64c 35\. Deep Learning with PyTorch: Build, Train and Deploy an Image Classifier \- YouTube, https://www.youtube.com/watch?v=Ne25VujHRLA 36\. A Short Video Classification Framework Based on Cross-Modal Fusion \- MDPI, https://www.mdpi.com/1424-8220/23/20/8425 37\. \[2102.05095\] Is Space-Time Attention All You Need for Video Understanding? \- arXiv, https://arxiv.org/abs/2102.05095 38\. VideoMAE: Scaling Self-Supervised Video Transformers Beyond Labeled Data \- Medium, https://medium.com/@kdk199604/videomae-scaling-self-supervised-video-transformers-beyond-labeled-data-1048a780522d 39\. VideoMAE \- Hugging Face, https://huggingface.co/docs/transformers/en/model\_doc/videomae
