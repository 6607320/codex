# **Исследовательский отчет: Оценка качества дообученных моделей LLM и архитектурные особенности инференса с использованием PEFT-адаптеров**

## **1\. Введение: Смена парадигмы в развертывании больших языковых моделей**

Современный ландшафт искусственного интеллекта переживает фундаментальную трансформацию, переходя от эпохи создания монолитных, универсальных моделей к эре специализированных адаптаций. В центре этой эволюции находится концепция эффективного дообучения параметров (Parameter-Efficient Fine-Tuning, PEFT), которая позволяет радикально снизить вычислительные барьеры для настройки моделей масштаба миллиардов параметров под конкретные прикладные задачи. В рамках образовательного и практического контекста, обозначенного как «Квест 5.3», перед исследователями и инженерами ставится задача не просто технически реализовать процесс дообучения, но и глубоко осмыслить последующие этапы жизненного цикла модели: валидацию полученных результатов и организацию эффективного инференса.  
Актуальность данной проблематики продиктована тем, что успешное завершение процесса обучения (training convergence) не является гарантией работоспособности модели в реальных условиях эксплуатации. Существует значительный разрыв между снижением функции потерь (loss function) на графиках мониторинга и получением когерентного, фактологически точного ответа от модели, интегрированной в продуктовый контур. Особенно остро этот разрыв ощущается при использовании методов квантования, таких как QLoRA, где взаимодействие между сжатыми весами базовой модели и полнопрецизионными адаптерами создает нетривиальные инженерные вызовы.  
Данный отчет призван дать исчерпывающий анализ методологии оценки качества дообученных моделей и детализировать архитектурные паттерны инференса с использованием PEFT-адаптеров. Мы рассмотрим теоретические основы метода Low-Rank Adaptation (LoRA), проанализируем влияние квантования на процессы слияния весов, выявим причины возникновения дегенеративных паттернов генерации (таких как бесконечные циклы) и предложим архитектурные решения для высоконагруженного мульти-модельного обслуживания.

## **2\. Теоретический базис PEFT и механика Low-Rank Adaptation (LoRA)**

Для того чтобы корректно оценивать качество и проектировать системы инференса, необходимо обладать глубоким пониманием математической природы адаптеров, используемых в современных архитектурах трансформеров. Метод LoRA, ставший де\-факто стандартом индустрии, базируется на строгих теоретических предпосылках, понимание которых критически важно для диагностики проблем.

### **2.1. Гипотеза низкого внутреннего ранга**

В основе метода LoRA лежит гипотеза о низком внутреннем ранге (low intrinsic dimension), сформулированная в ряде фундаментальных работ по теории глубокого обучения. Суть гипотезы заключается в том, что при адаптации перепараметризованной модели (каковой является любой современный LLM с миллиардами параметров) к новой задаче, значимые изменения весов происходят не во всем пространстве параметров, а в подпространстве существенно меньшей размерности.  
Если представить матрицу весов слоя предобученной модели как W_0 \\in \\mathbb{R}^{d \\times k}, то традиционное полное дообучение (full fine-tuning) предполагает обновление всех элементов этой матрицы, что требует хранения оптимизатором состояний для каждого из d \\times k параметров. LoRA предлагает моделировать обновление весов \\Delta W как произведение двух матриц низкого ранга:  
Где:

- B \\in \\mathbb{R}^{d \\times r} — матрица проекции в пространство высокой размерности.
- A \\in \\mathbb{R}^{r \\times k} — матрица проекции в пространство низкой размерности.
- r — ранг адаптации (обычно r \\ll \\min(d, k)), являющийся гиперпараметром, определяющим емкость адаптера.

В процессе обучения матрица W_0 остается замороженной ("frozen"), что означает отсутствие вычисления градиентов для нее. Обучаются только матрицы A и B. Это позволяет сократить количество обучаемых параметров на несколько порядков (до 0.1-1% от общего числа), что, в свою очередь, снижает требования к видеопамяти (VRAM) и объему сохраняемых чекпоинтов.

### **2.2. Инициализация и динамика обучения**

Критическим аспектом, влияющим на стабильность начала обучения, является стратегия инициализации матриц адаптера. В стандартной реализации LoRA используется следующий подход:

- Матрица A инициализируется случайными значениями из нормального распределения (Gaussian initialization).
- Матрица B инициализируется нулями.

Такая конфигурация гарантирует, что в самом начале процесса обучения произведение BA будет равно нулевой матрице. Следовательно, на первом шаге W' \= W_0 \+ 0 \= W_0. Это обеспечивает плавный старт: модель начинает обучение с поведения, идентичного предобученной модели, и постепенно адаптируется под новые данные. Если бы обе матрицы инициализировались случайным шумом, это внесло бы значительное возмущение в работу сети на первых итерациях, что могло бы привести к расхождению градиентов (gradient divergence).

### **2.3. Масштабирование (Scaling Factor) и инференс**

Важной, но часто упускаемой из виду деталью является коэффициент масштабирования \\alpha (alpha). Итоговая формула прямого прохода (forward pass) для слоя с LoRA выглядит следующим образом:  
Где x — входной вектор активаций. Коэффициент \\frac{\\alpha}{r} играет роль регулятора влияния адаптера на поведение модели.

- **Смысл \\alpha:** Он позволяет настраивать амплитуду обновлений весов. Традиционно \\alpha фиксируется как константа (например, равная r или 2r), что упрощает подбор гиперпараметров, сводя его к настройке только learning rate.
- **Роль при инференсе:** Наличие этого коэффициента позволяет проводить тонкую настройку модели уже после обучения. Изменяя \\alpha во время инференса (без переобучения), можно усиливать или ослаблять влияние новых знаний, что используется в некоторых продвинутых техниках смешивания адаптеров.

### **2.4. QLoRA: Квантование как необходимость**

С ростом размеров моделей даже методы PEFT столкнулись с ограничениями памяти GPU. QLoRA (Quantized LoRA) решает эту проблему путем агрессивного сжатия весов базовой модели W_0 до 4-битного представления. Ключевые инновации QLoRA включают:

1. **Тип данных 4-bit NormalFloat (NF4):** Этот тип данных теоретически оптимален для хранения весов, распределенных по нормальному закону, что характерно для нейронных сетей.
2. **Двойное квантование (Double Quantization):** Квантование самих констант квантования для дополнительной экономии памяти.
3. **Paged Optimizers:** Использование страничной памяти CPU для выгрузки состояний оптимизатора при пиковых нагрузках.

В контексте QLoRA уравнение инференса усложняется, так как базовые веса должны быть деквантованы перед умножением:  
Здесь веса адаптеров (A и B) остаются в формате высокого разрешения (например, BFloat16), тогда как базовые веса хранятся в сжатом виде и распаковываются "на лету" во время вычислений. Это создает вычислительный оверхед, но позволяет запускать модели размером 65B параметров на одной карте с 48GB VRAM. Именно этот механизм деквантования становится камнем преткновения при попытке слияния весов, что будет детально рассмотрено в разделе, посвященном архитектуре инференса.

## **3\. Методология оценки качества дообученной модели**

Оценка генеративных моделей представляет собой многогранную задачу, где ни одна метрика не может дать полной картины. Валидация качества адаптера должна проводиться комплексно, сочетая автоматические количественные метрики с качественным анализом поведения модели.

### **3.1. Количественные метрики и их интерпретация**

#### **3.1.1. Перплексия (Perplexity) и Loss**

Перплексия, являющаяся экспонентой от функции потерь кросс-энтропии, традиционно используется как первичный индикатор качества языковой модели. Она отражает степень "удивления" модели при виде следующего токена в последовательности.

- **Динамика Loss:** При обучении адаптеров критически важно отслеживать расхождение между Training Loss и Validation Loss. Поскольку LoRA оперирует малым числом параметров, риск переобучения (overfitting) на специфику обучающей выборки достаточно высок. Ситуация, когда Training Loss продолжает падать, а Validation Loss начинает расти, свидетельствует о том, что адаптер "зазубривает" примеры, теряя способность к обобщению.
- **Ограничения:** Низкая перплексия является необходимым, но не достаточным условием качества. Модель может иметь низкую перплексию, но при этом генерировать репетитивные (повторяющиеся) тексты или страдать от логических противоречий.

#### **3.1.2. Метрики генерации: ROUGE, BLEU, METEOR, BERTScore**

Для задач, подразумевающих генерацию текста с определенной структурой (суммаризация, перевод, ответы на вопросы), применяются метрики сравнения с эталонными ответами.

- **ROUGE (Recall-Oriented Understudy for Gisting Evaluation):** Семейство метрик (ROUGE-1, ROUGE-2, ROUGE-L), измеряющих перекрытие n-грамм между сгенерированным текстом и эталоном. ROUGE особенно полезна для оценки того, насколько хорошо модель улавливает ключевые фразы и структуру ответа. В отчетах AWS отмечается, что качественные адаптеры могут демонстрировать прирост ROUGE score на 150% по сравнению с базовой моделью на специфических доменах.
- **BERTScore:** В отличие от ROUGE и BLEU, которые опираются на точное совпадение слов, BERTScore использует контекстуальные эмбеддинги для оценки смысловой близости. Это позволяет более справедливо оценивать ответы, где модель использует синонимы или перефразирует эталон, сохраняя смысл.

### **3.2. Качественный анализ и феноменология ошибок**

Автоматические метрики часто не способны уловить тонкие нарушения логики или специфические артефакты генерации. Поэтому "Золотым стандартом" современной оценки является использование парадигмы "LLM-as-a-Judge" (использование более мощной модели, например GPT-4, для оценки ответов) и ручной анализ типичных ошибок.

#### **3.2.1. Проблема бесконечной генерации и EOS токенов**

Одной из наиболее распространенных и раздражающих проблем при инференсе дообученных моделей является неспособность модели вовремя остановиться. Модель генерирует правильный ответ, но затем продолжает выдавать бессвязный текст, повторять последние предложения или галлюцинировать до тех пор, пока не сработает лимит max_new_tokens.  
**Анатомия проблемы:** Эта проблема часто коренится в некорректной настройке токенизатора, особенно в моделях семейства GPT-2, DistilGPT2 и ранних версиях Llama, где отсутствует выделенный токен паддинга (pad_token).

- **Конфликт PAD и EOS:** Для обеспечения возможности батчинга (обработки нескольких примеров разной длины одновременно) разработчики часто вынуждены назначать pad_token равным eos_token (tokenizer.pad_token \= tokenizer.eos_token).
- **Механизм сбоя при обучении:** При использовании стандартного DataCollatorForLanguageModeling, токены, помеченные как паддинг, игнорируются при расчете функции потерь (loss mask \= 0). Если EOS токен используется как PAD, то во время обучения модель может не получать штрафа или поощрения за предсказание конца последовательности. Фактически, модель учится на примерах, где конец предложения "не важен" или замаскирован.
- **Последствия:** В результате адаптер выучивает контент, но не выучивает сигнал остановки. Вероятность генерации токена EOS становится исчезающе малой, и модель переходит в режим бесконечного продолжения текста.

**Стратегии миграции и исправления:**

1. **На этапе обучения:** Использование специального токена для паддинга (например, добавление нового токена \`\` в словарь и изменение размера эмбеддингов модели) является наиболее надежным решением. Если это невозможно, необходимо использовать DataCollator, который корректно обрабатывает маски внимания, не исключая истинный EOS токен из расчета лосса.
2. **На этапе инференса:** Явное указание pad_token_id в конфигурации генерации может помочь в некоторых случаях. Однако, если веса модели уже "испорчены" неправильным обучением, программные "костыли" при генерации (например, принудительная остановка по стоп-словам) будут лишь маскировать проблему. В документации Hugging Face и обсуждениях сообщества подчеркивается, что установка pad_token_id \= eos_token_id при инференсе допустима только для "open-ended" генерации, где остановка не так критична, но для задач с четким ответом это плохая практика.

## **4\. Архитектура и процесс инференса с PEFT-адаптерами**

Понимание архитектуры инференса является ключом к эффективному внедрению моделей в продакшн. В зависимости от требований к задержке (latency), пропускной способности (throughput) и доступным ресурсам, инженеры могут выбирать между динамическим подключением адаптеров и статическим слиянием весов.

### **4.1. Динамический инференс (Adapter Loading)**

Этот подход предполагает, что веса базовой модели загружаются в память GPU один раз и остаются неизменными. Веса адаптеров (матрицы A и B) загружаются по требованию и подключаются к вычислительному графу модели "на лету".  
**Процесс прохождения данных (Forward Pass):**

1. Входной токен преобразуется в эмбеддинг и проходит через слои трансформера.
2. В каждом слое, где сконфигурирован LoRA-модуль (обычно слои внимания q_proj, v_proj), происходит разветвление потока данных.
3. **Основная ветка:** Данные проходят через замороженные веса W_0, выполняя операцию W_0 x.
4. **Адаптерная ветка:** Параллельно данные проходят через последовательность матричных умножений низкой размерности: сначала понижение размерности (A x), затем нелинейность (опционально), затем повышение размерности (B (Ax)).
5. **Агрегация:** Результаты обеих веток суммируются с учетом коэффициента масштабирования: y \= W_0 x \+ \\frac{\\alpha}{r} B A x.

**Преимущества:**

- **Мультиарендность (Multi-tenancy):** Возможность обслуживать тысячи различных задач на одной инсталляции базовой модели. Адаптеры занимают ничтожно мало памяти (10-100 МБ) по сравнению с базовой моделью (10-100 ГБ), что делает переключение между ними практически мгновенным.
- **Изоляция:** Ошибки или дообучение одного адаптера никак не влияют на другие, так как базовая модель неизменна.

**Недостатки:**

- **Вычислительные накладные расходы:** Необходимость выполнения дополнительных матричных операций увеличивает время инференса. Хотя матрицы A и B малы, сам факт наличия дополнительных ядер CUDA (kernels) и операций суммирования памяти создает задержку порядка 10-30% по сравнению с чистой моделью.

### **4.2. Статическое слияние весов (Weight Merging)**

Для систем, где критична минимальная задержка, применяется метод слияния весов адаптера с весами базовой модели. Метод merge_and_unload(), предоставляемый библиотекой PEFT, выполняет это преобразование необратимо для текущего инстанса модели в памяти.  
**Математическая суть слияния:** Поскольку операция LoRA линейна (относительно входных данных в слое без нелинейности между A и B), мы можем раскрыть скобки в уравнении прямого прохода:  
Новая матрица весов W\_{merged} \= W_0 \+ \\frac{\\alpha}{r} B A вычисляется один раз перед началом обслуживания запросов. После этого ветки адаптера удаляются из архитектуры.  
**Преимущества:**

- **Нулевой оверхед:** Инференс выполняется со скоростью исходной базовой модели.
- **Упрощение деплоя:** Слитая модель представляет собой стандартный артефакт (например, в формате Hugging Face или ONNX), который можно запускать инструментами, не поддерживающими LoRA нативно (например, старые версии TGI или специализированные инференс-движки).

**Ограничения:**

- **Потеря гибкости:** Невозможность динамического переключения задач. Для каждой задачи требуется загружать отдельную копию полной модели, что кратно увеличивает требования к VRAM.
- **Проблемы с квантованием:** Это наиболее критичный аспект, требующий отдельного рассмотрения.

## **5\. Проблема слияния в QLoRA и протокол безопасной интеграции**

Ключевым техническим инсайтом, необходимым для успешного выполнения Квеста 5.3, является понимание несовместимости наивного слияния весов с квантованием QLoRA. Это классическая "ловушка" для начинающих исследователей, приводящая к резкой деградации качества модели.

### **5.1. Феномен "Garbage Output" при слиянии 4-битных моделей**

При использовании QLoRA базовая модель загружается в формате 4-bit (NF4). Адаптеры обучаются в формате BF16 или FP16. Если пользователь попытается выполнить model.merge_and_unload() на такой гибридной конфигурации, возникают фундаментальные проблемы арифметики типов данных.  
**Почему это происходит?**

1. **Разная природа данных:** Сложение целочисленных квантованных значений (или упакованных NF4) с числами с плавающей точкой (FP16) напрямую невозможно. Библиотеки (bitsandbytes, PEFT) пытаются выполнить приведение типов "под капотом".
2. **Потеря точности (Precision Loss):** При попытке деквантовать вес W\_{NF4} в FP16, прибавить к нему \\Delta W\_{LoRA}, и затем (часто неявно) снова использовать этот вес, накапливаются ошибки округления. Эти ошибки, будучи малыми для одного веса, суммируются по слоям глубокой сети, приводя к полному разрушению семантических связей.
3. **Симптоматика:** Пользователи наблюдают, что до вызова merge_and_unload модель работает корректно (используя динамический путь инференса), но сразу после слияния начинает выдавать бессвязный набор токенов ("garbage output") или случайный шум.

### **5.2. Протокол корректного слияния (Workaround)**

Сообщество разработчиков и исследователей выработало строгий протокол действий для решения этой проблемы. Суть решения заключается в отказе от слияния в квантованном пространстве в пользу слияния в пространстве высокой точности.  
**Пошаговый алгоритм (Best Practice 2024-2025):**

| Этап                        | Действие                                                                                                            | Техническое обоснование                                                                                                          |
| :-------------------------- | :------------------------------------------------------------------------------------------------------------------ | :------------------------------------------------------------------------------------------------------------------------------- |
| **1\. Сохранение адаптера** | Сохранить обученные веса LoRA на диск (локально или в Hub).                                                         | Разделение процесса обучения и процесса слияния.                                                                                 |
| **2\. Перезагрузка базы**   | Загрузить **базовую модель** заново в формате FP16 или BF16. **Запрещено** использовать аргумент load_in_4bit=True. | Слияние должно происходить между матрицами одной точности (FP16 \+ FP16), чтобы избежать артефактов деквантования/реквантования. |
| **3\. Загрузка адаптера**   | Загрузить сохраненный адаптер поверх FP16-базы, используя PeftModel.from_pretrained.                                | Подготовка к арифметической операции сложения матриц.                                                                            |
| **4\. Слияние**             | Выполнить model.merge_and_unload().                                                                                 | Теперь операция W\_{new} \= W_0 \+ BA выполняется корректно в пространстве Floating Point.                                       |
| **5\. Финализация**         | Сохранить полученную слитую модель или квантовать её заново (например, в GGUF/AWQ).                                 | Получение готового к деплою артефакта.                                                                                           |

**Важное примечание по ресурсам:** Этап 2 (загрузка базовой модели в FP16) требует значительно больше видеопамяти (VRAM), чем обучение в QLoRA. Например, для модели 7B требуется около 14-16 ГБ VRAM для загрузки в FP16, тогда как для 4-битного обучения достаточно 6-8 ГБ. Если VRAM недостаточно для слияния, рекомендуется использовать параметр device_map="cpu" или low_cpu_mem_usage=True для выполнения операции слияния в системной оперативной памяти (RAM), что медленнее, но позволяет обойти ограничение видеопамяти.  
**Иллюстрация кода для корректного слияния:**  
`import torch`  
`from transformers import AutoModelForCausalLM`  
`from peft import PeftModel`

`# Очистка памяти после обучения`  
`del model, trainer`  
`torch.cuda.empty_cache()`

`# 1. Загрузка базовой модели в полной точности (FP16)`  
`# Критически важно НЕ использовать quantization_config здесь`  
`base_model = AutoModelForCausalLM.from_pretrained(`  
 `"base_model_path",`  
 `torch_dtype=torch.float16,`  
 `device_map="auto" # Или "cpu", если мало VRAM`  
`)`

`# 2. Подключение адаптера`  
`model_to_merge = PeftModel.from_pretrained(`  
 `base_model,`  
 `"path_to_saved_adapter"`  
`)`

`# 3. Безопасное слияние`  
`merged_model = model_to_merge.merge_and_unload()`

`# 4. Сохранение результата`  
`merged_model.save_pretrained("merged_model_output")`  
`tokenizer.save_pretrained("merged_model_output")`

Данный подход, описанный в многочисленных обсуждениях на GitHub и форумах Hugging Face, является единственно надежным способом конвертации QLoRA-моделей в автономные артефакты.

## **6\. Продвинутые архитектуры: LoRAX и масштабируемый инференс**

В условиях промышленной эксплуатации часто возникает потребность обслуживать множество различных адаптеров одновременно. Например, SaaS-платформа может предоставлять услуги генерации текста тысячам клиентов, каждый из которых имеет свой кастомизированный fine-tune. Запуск тысяч отдельных инстансов моделей экономически невозможен.

### **6.1. Архитектура Multi-LoRA Serving**

Для решения этой задачи были разработаны специализированные системы инференса, такие как **LoRAX (LoRA Exchange)**, **S-LoRA** и **Punica**. Эти системы реализуют парадигму мультиарендности на уровне ядра GPU.  
**Ключевые технологические решения:**

1. **Гетерогенный батчинг (Heterogeneous Batching):** Традиционный батчинг предполагает, что все запросы в пачке обрабатываются одной моделью. В системах Multi-LoRA в одном батче могут находиться запросы, требующие разных адаптеров (A_1, B_1 для запроса 1; A_2, B_2 для запроса 2). Специализированные CUDA-ядра (например, переписанные ядра Attention) позволяют применять уникальные смещения весов для каждого элемента в батче без потери производительности.
2. **Унифицированная память и кэширование (Unified Paged Memory):** По аналогии с управлением виртуальной памятью в операционных системах, LoRAX использует пул памяти для хранения весов адаптеров. Популярные адаптеры хранятся в быстрой памяти GPU, редко используемые вытесняются в CPU RAM. Это позволяет обслуживать теоретически неограниченное число адаптеров на одной видеокарте, загружая их по требованию с минимальной задержкой.
3. **Экономическая эффективность:** Использование Multi-LoRA архитектур позволяет сократить расходы на инфраструктуру в десятки раз. Вместо поддержки парка серверов под каждую модель, достаточно одного мощного инстанса, который динамически переключает контекст. Это делает бизнес-модели, основанные на персонализированном ИИ, рентабельными.

### **6.2. Сравнение подходов к инференсу**

В таблице ниже приведено сравнение трех основных стратегий инференса, обсуждаемых в данном отчете.

| Характеристика         | Динамический LoRA (Standard PEFT)          | Статическое слияние (Merged FP16)             | Multi-LoRA Serving (LoRAX/S-LoRA)   |
| :--------------------- | :----------------------------------------- | :-------------------------------------------- | :---------------------------------- |
| **Скорость (Latency)** | Средняя (10-20% оверхед)                   | Высокая (Нативная скорость)                   | Высокая (Оптимизированные ядра)     |
| **Потребление VRAM**   | База \+ Активный адаптер                   | Полная модель FP16                            | База \+ Кэш адаптеров               |
| **Сложность деплоя**   | Низкая                                     | Средняя (требует конвертации)                 | Высокая (специализированный сервер) |
| **Гибкость**           | Высокая (горячая замена)                   | Низкая (одна задача на модель)                | Максимальная (тысячи адаптеров)     |
| **Применимость**       | Разработка, тестирование, локальный запуск | Продакшн с одной задачей, экспорт в GGUF/ONNX | Высоконагруженные SaaS платформы    |

## **7\. Заключение**

Выполнение Квеста 5.3 требует от специалиста не только навыков написания кода обучения, но и глубокого системного понимания архитектуры больших языковых моделей. В ходе данного исследования мы установили, что качество дообученной модели нельзя оценивать исключительно по метрикам потерь (Loss); необходим комплексный анализ, включающий семантические метрики и проверку на наличие структурных дефектов генерации, таких как проблема EOS-токена.  
В части инференса критически важно различать сценарии динамического использования адаптеров и статического слияния. Особое внимание следует уделять нюансам работы с квантованными моделями (QLoRA): попытка прямого слияния 4-битных весов с 16-битными адаптерами является гарантированным путем к получению неработоспособной модели. Единственно верный путь — это процедура деквантования, слияния в высоком разрешении и, при необходимости, повторного квантования.  
Понимание этих процессов, а также знание современных архитектур масштабирования (LoRAX), отличает квалифицированного инженера, способного доводить ML-решения до продуктивной эксплуатации, от начинающего исследователя. Применение описанных практик позволит обеспечить надежность, эффективность и экономическую целесообразность внедряемых ИИ-систем.

#### **Источники**

1\. What is LoRA (Low-Rank Adaption)? | IBM, https://www.ibm.com/think/topics/lora 2\. Understanding LoRA \- Low Rank Adaptation For Finetuning Large Models, https://towardsdatascience.com/understanding-lora-low-rank-adaptation-for-finetuning-large-models-936bce1a07c6/ 3\. huggingface/peft: PEFT: State-of-the-art Parameter-Efficient Fine-Tuning. \- GitHub, https://github.com/huggingface/peft 4\. How LoRA Makes AI Fine-Tuning Faster, Cheaper, and More Practical \- Exxact Corporation, https://www.exxactcorp.com/blog/deep-learning/ai-fine-tuning-with-lora 5\. LoRA \- Hugging Face, https://huggingface.co/docs/peft/developer\_guides/lora 6\. Understanding LoRA: Low Rank Adaptation | by Vikram Pande | Medium, https://medium.com/@vikrampande783/understanding-lora-low-rank-adaptation-563978253d6e 7\. A Rank Stabilization Scaling Factor for Fine-Tuning with LoRA \- arXiv, https://arxiv.org/pdf/2312.03732 8\. Understanding alpha parameter tuning in LORA paper \- Data Science Stack Exchange, https://datascience.stackexchange.com/questions/123229/understanding-alpha-parameter-tuning-in-lora-paper 9\. Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA, https://huggingface.co/blog/4bit-transformers-bitsandbytes 10\. Bitsandbytes \- Hugging Face, https://huggingface.co/docs/transformers/quantization/bitsandbytes 11\. Easily deploy and manage hundreds of LoRA adapters with SageMaker efficient multi-adapter inference | Artificial Intelligence \- AWS, https://aws.amazon.com/blogs/machine-learning/easily-deploy-and-manage-hundreds-of-lora-adapters-with-sagemaker-efficient-multi-adapter-inference/ 12\. Understanding eos_token and pad_token in model generate config : Huggingface Transformers | by khalandar m | Oct, 2025 | Medium, https://medium.com/@khalandar.mokula/understanding-eos-token-and-pad-token-in-model-generate-config-huggingface-transformers-fef6845ed92a 13\. Why does the falcon QLoRA tutorial code use eos_token as pad_token? \- Models, https://discuss.huggingface.co/t/why-does-the-falcon-qlora-tutorial-code-use-eos-token-as-pad-token/45954 14\. GPT2 finetuned with eos token will never yield eos token during generation \- Beginners, https://discuss.huggingface.co/t/gpt2-finetuned-with-eos-token-will-never-yield-eos-token-during-generation/15437 15\. How does one set the pad token correctly (not to eos) during fine-tuning to avoid model not predicting EOS? \- PyTorch Forums, https://discuss.pytorch.org/t/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoid-model-not-predicting-eos/213619 16\. GPT2 doesn't generate new tokens if pad_token is added · Issue \#29899 · huggingface/transformers \- GitHub, https://github.com/huggingface/transformers/issues/29899 17\. Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation \- Beginners, https://discuss.huggingface.co/t/setting-pad-token-id-to-eos-token-id-50256-for-open-end-generation/22247 18\. Fine-Tuning GPT2 \- attention mask and pad token id errors \- Stack Overflow, https://stackoverflow.com/questions/74682597/fine-tuning-gpt2-attention-mask-and-pad-token-id-errors 19\. How to fix "the attention mask and the pad token id were not set..." ? : r/KoboldAI \- Reddit, https://www.reddit.com/r/KoboldAI/comments/yz26ol/how\_to\_fix\_the\_attention\_mask\_and\_the\_pad\_token/ 20\. Parameter-Efficient Fine-Tuning — NVIDIA NIM for Large Language Models (LLMs), https://docs.nvidia.com/nim/large-language-models/1.1.0/peft.html 21\. Host concurrent LLMs with LoRAX | Artificial Intelligence \- AWS, https://aws.amazon.com/blogs/machine-learning/host-concurrent-llms-with-lorax/ 22\. QLoRA and 4-bit Quantization \- Chris McCormick, https://mccormickml.com/2024/09/14/qlora-and-4bit-quantization/ 23\. Difficulty merging qlora adapter : r/LocalLLaMA \- Reddit, https://www.reddit.com/r/LocalLLaMA/comments/1ftco3t/difficulty\_merging\_qlora\_adapter/ 24\. Dequantize 4bit B\&B model to prepare for merging \- Transformers \- Hugging Face Forums, https://discuss.huggingface.co/t/dequantize-4bit-b-b-model-to-prepare-for-merging/167905 25\. Merge and unload changes inference a lot on quantized llama 2 · Issue \#1043 · huggingface/peft \- GitHub, https://github.com/huggingface/peft/issues/1043 26\. Fine-Tune Gemma using Hugging Face Transformers and QloRA \- Google AI for Developers, https://ai.google.dev/gemma/docs/core/huggingface\_text\_finetune\_qlora 27\. Merge LoRA Adapters with LLMs the right way \- Colab, https://colab.research.google.com/drive/12c\_sx8pIwiStqKr\_7CF5BVwyyJpXmMTf?usp=sharing 28\. Support merge_and_unload for IA3 Adapters with 4-bit and 8bit Quantization models · Issue \#1704 · huggingface/peft \- GitHub, https://github.com/huggingface/peft/issues/1704 29\. How to load a fine-tuned peft/lora model based on llama with Huggingface transformers?, https://stackoverflow.com/questions/76459034/how-to-load-a-fine-tuned-peft-lora-model-based-on-llama-with-huggingface-transfo 30\. TGI Multi-LoRA: Deploy Once, Serve 30 Models \- Hugging Face, https://huggingface.co/blog/multi-lora-serving 31\. Efficient and cost-effective multi-tenant LoRA serving with Amazon SageMaker \- AWS, https://aws.amazon.com/blogs/machine-learning/efficient-and-cost-effective-multi-tenant-lora-serving-with-amazon-sagemaker/ 32\. Efficiently Deploying LoRA Adapters: Optimizing LLM Fine-Tuning for Multi-Task AI, https://www.inferless.com/learn/how-to-serve-multi-lora-adapters 33\. Creating cost effective specialized AI solutions with LoRA adapters on Red Hat OpenShift AI, https://www.redhat.com/en/blog/creating-cost-effective-specialized-ai-solutions-lora-adapters-red-hat-openshift-ai 34\. LoRAX Docs, https://loraexchange.ai/
