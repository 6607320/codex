# quest_5_1.py Specification

1. Meta Information

- Domain: Scripting
- Complexity: Medium
- Language: Python
- Frameworks: datasets
- Context: Independent Artifact

2. Goal & Purpose (Цель и Назначение)
   Легенда: Этот модуль демонстрирует экономию ресурсов при работе с большими датасетами через потоковую загрузку: подключается к Великой Библиотеке через портал и извлекает лишь небольшой срез Эфира, чтобы обучить навыку работы с большими данными в условиях ограниченных ресурсов. Зачем нужен файл: показать как получать часть данных без полной загрузки архива и вывести первую страницу инструкции, контекст и идеальный ответ для наставления Голема.

3. Interface Contract (Интерфейсный Контракт)

3.1. Inputs (Входы)

- Source: CLI Args | STDIN | API Request | Kafka Topic | Smart Contract Call
- Format: JSON | Text | Binary | Stream
- Schema: InputData
  - archiveName?: string
  - split?: string
  - streaming?: boolean
  - trustRemoteCode?: boolean

    3.2. Outputs (Выходы)

- Destination: STDOUT | File | Database | API Response | Event Log
- Format: JSON | CSV | Text
- Success Criteria: Exit Code 0 | 200 OK | File Created
- Schema: OutputResult
  - status?: string
  - extractedCount?: number
  - firstPage?: {
    instruction?: string
    context?: string
    response?: string
    }

4. Implementation Details (The Source DNA / Исходный Код)

4.1. Algorithmic Logic (Для исполняемого кода)

1. Пролог заклинается: импортируется ритуал load_dataset из Великой Библиотеки заклятий под названием datasets.
2. Объявляется имя архива и связанные параметры: archive_name задаёт имя архива в Великой Библиотеке, split указывает часть данных, streaming активирован для потоковой загрузки, trust_remote_code активизирует доверие к удалённому коду.
3. Проводится торжественный портал: выводится на кристалл сообщение об открытии портала к архиву с именем archive_name.
4. Открывается портал Архива: через ритуал load_dataset создаётся full_dataset как потоковый IterableDataset с параметрами архив_name, split="train", streaming=True и trust_remote_code=True.
5. Пролог извлечения: объявляется намерение извлечь 100 страниц для урока.
6. Подготовка свитка: создаётся пустой свиток small_dataset для хранения извлечённых записей.
7. Выполняется цикл ритуала по порталу: для каждого элемента sample из full_dataset счётчик i инкрементируется.
   - if i достигает 100, прерывается цикл.
   - текущая запись sample добавляется в small_dataset.
8. Гимн успешного извлечения: выводится на кристалл количество извлечённых записей.
9. Финальный акт — просмотр страницы: выбирается первая страница first_page из small_dataset (индекс 0) и на кристалле выводятся три блока — Инструкция (instruction), Контекст (context) и Ответ (response) из этой страницы.
10. Эпилог: процесс завершается после демонстрации первой страницы учебного свитка.

4.2. Declarative Content (Для конфигураций и данных)
Образы и атрибуты артефакта:

- Архив великий архив: databricks/databricks-dolly-15k (портал архива)
- Портал Архива (full_dataset): потоковый канал, который подаёт данные по мере запроса
- Свиток (small_dataset): локальный контейнер, где аккумулируются первые 100 страниц
- Письмо Инструкция (first_page.instruction): текстовая инструкция для Голема
- Письмо Контекст (first_page.context): дополнительные сведения
- Письмо Ответ (first_page.response): идеальный ответ, чему мы учим Голема
- Сообщения на кристалле: системные статусы и уведомления ритуала, такие как открытие портала и извлечённое количество страниц

5. Structural Decomposition (Декомпозиция структуры)

- Ритуалы и порталы
  - Архивный портал: archive_name (строка имени архива) и full_dataset (IterableDataset, поток)
  - Портал архива: архивName, split, streaming, trustRemoteCode
- Свитки и_pages
  - small_dataset: список страниц (страницы каждая — структура с полями instruction, context, иاید response)
  - first_page: выборка из small_dataset по индексу 0
- Контент на кристалле
  - print-операторы для вывода статусов и данных инструкции/контекста/ответа
- Элементы управления потоком
  - for i, sample in enumerate(full_dataset): цикл, ограничение i >= 100
- Внешние зависимости
  - datasets.load_dataset: источник данных и способ доступа к архиву
  - архив_name: databricks/databricks-dolly-15k

6. System Context & Constraints (Системный контекст и Ограничения)

6.1. Technical Constraints

- Performance: Оптимизирован под экономию памяти за счёт потоковой загрузки (streaming=True) — характерно для работы с большими данными.
- Concurrency: Синхронный режим выполнения в рамках одного потока.
- Dependencies: зависимость к библиотеке datasets и доступ к Hugging Face Hub.

  6.2. Prohibited Actions (Negative Constraints)

- DO NOT хранить секреты в открытом виде (используйте переменные окружения/.env для чувствительных данных).
- DO NOT выводить сырые данные в консоль в продакшн-режиме без фильтрации.
- DO NOT выполнять синхронные сетевые вызовы в критических циклaх, если можно использовать асинхронность (в рамках данного кода — отсутствует).
- DO NOT упаковывать конфигурационные файлы (.yaml, .json) в исполняемые скрипты.
- DO NOT менять версии библиотек или пути к артефактам во время реконструкции.

7. Verification & Testing (Верификация)

Геркин-сценарии

Функциональный сценарий 1: Успешная работа
Контекст: доступен архив и сеть позволяет потоковую загрузку.
Дано: окружение настроено, архив(databricks/databricks-dolly-15k) доступен, сеть функционирует.
Когда: выполняется ритуал загрузки и потоковая передача данных.
Тогда: успешно извлечено 100 страниц; первая страница shows инструкции, контекст и ответ; процесс завершается успешно.

Функциональный сценарий 2: Ошибка потока данных
Контекст: сеть временно недоступна или архив недоступен.
Дано: окружение содержит неверный адрес портала или проблемный архив.
Когда: запускается ритуал, но поток данных прерывается до извлечения хотя бы одной страницы.
Тогда: выводится сообщение об ошибке и процесс завершается с кодом неудачи; предусмотрено безопасное завершение.

ИССЛЕДУЕМЫЙ АРТЕФАКТ: quest_5_1.py
