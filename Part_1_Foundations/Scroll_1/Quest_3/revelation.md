# **От «Всемогущего» pipeline к Мастерству над Големами: Руководство Исследователя по Глубинам Трансформеров**

## **Введение: Очарование Бесшовного pipeline**

Наблюдение, сделанное подмастерьем, совершенно справедливо и свидетельствует о проницательности. Функция pipeline из библиотеки Hugging Face Transformers действительно представляет собой вершину инженерной мысли в области дизайна API. Она намеренно абстрагирует колоссальную сложность, предлагая простой, интуитивно понятный и чрезвычайно мощный интерфейс для доступа к передовым моделям искусственного интеллекта.1 Эта функция является результатом значительных усилий, направленных на демократизацию ИИ, делая его доступным для широкого круга разработчиков и исследователей.  
Однако, чтобы постичь истинную природу этой магии, полезно рассматривать pipeline не как единую, монолитную сущность, а скорее как дирижера, управляющего сложным оркестром. Каждый музыкант в этом оркестре — это высокоспециализированный «Голем», выполняющий свою уникальную и незаменимую партию. Дирижер (pipeline) знает партитуру — задачу, которую нужно выполнить, будь то «анализ тональности» или «ответы на вопросы». Но истинное звучание, глубина и виртуозность исполнения рождаются из мастерства отдельных исполнителей. Чтобы сочинять новую музыку, адаптировать существующие произведения для необычных инструментов или добиваться исполнения мирового класса, необходимо не только понимать дирижера, но и знать каждого музыканта лично, понимать его сильные и слабые стороны, и уметь говорить с ним на его языке.  
Цель данного отчета — провести подмастерье за кулисы этого грандиозного представления. Мы встретимся с этими Големами, разберем их ремесло и поймем, почему прямое взаимодействие с ними — это не просто альтернативный путь, а единственно верная дорога к подлинному мастерству. Этот путь позволит перейти от роли пользователя мощных инструментов к роли архитектора интеллектуальных систем. В последующих главах будет раскрыто не только то, _что_ скрывается под капотом pipeline, но и _почему_ знание этих механизмов является ключом к раскрытию полного потенциала трансформерных моделей.

## **Глава 1: Анатомия Абстракции: Раскрывая Големов Внутри**

Функция pipeline элегантно скрывает сложный технологический процесс, который можно разложить на три фундаментальных этапа. Каждый вызов pipeline инициирует строго определенную последовательность действий, выполняемую тремя метафорическими Големами, каждый из которых отвечает за свой участок работы: предварительная обработка (препроцессинг), инференс (вывод) модели и постобработка.1 Понимание ролей этих Големов — первый шаг к осознанию как мощи, так и ограничений этой абстракции.

### **Голем-Писец: Препроцессинг и Искусство Токенизации**

Первый и, возможно, самый недооцененный Голем — это Писец. Его задача — перевести данные из мира людей в мир машин. Модели не понимают текст, изображения или аудио в их исходном виде; они оперируют исключительно числами. Работа Писца заключается в этом критически важном переводе. Для текстовых данных этот процесс называется токенизацией. Для других модальностей, таких как изображения или аудио, эту роль выполняют компоненты, известные как image_processor или feature_extractor.1

#### **Глубокое Погружение в Токенизацию**

Токенизация — это процесс разбиения текста на более мелкие единицы, называемые токенами, с последующим сопоставлением каждого токена с уникальным числовым идентификатором (ID) из заранее определенного словаря (vocabulary).7 Этот процесс является фундаментом, на котором строится все дальнейшее «понимание» текста моделью.  
Существует несколько стратегий токенизации. Простейшие подходы, такие как разбиение по словам или символам, имеют серьезные недостатки. Разбиение по словам создает огромные словари и не способно обрабатывать незнакомые или редкие слова. Разбиение по символам, наоборот, теряет семантическую целостность слов. Поэтому современные трансформерные модели используют более совершенные методы субсловной токенизации, такие как Byte-Pair Encoding (BPE), WordPiece или SentencePiece.9 Эти алгоритмы находят оптимальный баланс между размером словаря и информационной плотностью токенов. Они разбивают редкие слова на осмысленные части (например, слово «annoyingly» может быть разбито на «annoying» и «ly»), сохраняя при этом часто встречающиеся слова целиком.9  
Ключевым моментом является то, что каждая предварительно обученная модель неразрывно связана с конкретным токенизатором и его словарем.8 Эта связь не является взаимозаменяемой. Модель обучалась на числовых последовательностях, сгенерированных определенным токенизатором, и ожидает на входе данные, обработанные по тем же правилам. Использование другого токенизатора приведет к полному непониманию и некорректной работе модели.

#### **За Пределами Токенизации: Пэддинг и Маски Внимания**

Когда на обработку подается несколько текстов одновременно (пакетная обработка или батчинг), возникает проблема: модели требуют, чтобы все входные последовательности в одном батче имели одинаковую длину. Однако в реальности предложения и документы редко совпадают по количеству токенов. Для решения этой проблемы Писец использует две техники: пэддинг (padding) и создание маски внимания (attention mask).14  
Пэддинг — это процесс добавления специальных токенов к более коротким последовательностям до тех пор, пока их длина не сравняется с длиной самой длинной последовательности в батче. Однако эти добавленные токены не несут никакой смысловой нагрузки и не должны влиять на результат работы модели. Здесь в игру вступает маска внимания. Это бинарная последовательность (состоящая из 0 и 1\) той же длины, что и токенизированный ввод. Она служит инструкцией для следующего Голема (Оракула), указывая, на какие токены следует обращать внимание (значение 1), а какие следует игнорировать (значение 0, соответствующие токенам). pipeline выполняет эти операции автоматически, но для ручного управления и оптимизации понимание этих механизмов абсолютно необходимо.

### **Голем-Оракул: Ядро Модели и Ритуал Инференса**

Второй Голем, Оракул, — это сердце всей системы. Это сама трансформерная модель, которая принимает числовые последовательности от Писца и генерирует предсказания.

#### **Архитектура Трансформера**

В основе архитектуры Трансформера лежит механизм self-attention (само-внимания).13 В отличие от рекуррентных нейронных сетей (RNN), которые обрабатывают текст последовательно, токен за токеном, Трансформер обрабатывает все токены одновременно. Механизм внимания позволяет модели для каждого токена в последовательности взвешивать важность всех остальных токенов. Это дает возможность улавливать сложные контекстуальные зависимости между словами, даже если они находятся далеко друг от друга в предложении, что было серьезным ограничением для предыдущих архитектур.

#### **Две Фазы Инференса**

Для авторегрессионных моделей, таких как GPT, которые генерируют текст токен за токеном, процесс инференса (вывода) делится на две четко различимые фазы: **prefill** (предзаполнение) и **decode** (декодирование).

1. **Фаза Prefill:** На этом этапе модель обрабатывает всю входную последовательность (промпт) параллельно. Эта фаза является вычислительно-интенсивной (compute-bound) и характеризуется большими операциями умножения матриц на матрицы. Современные графические процессоры (GPU) отлично справляются с такими задачами, поэтому эта фаза выполняется очень эффективно.15
2. **Фаза Decode:** После обработки промпта модель начинает генерировать выходную последовательность, по одному токену за раз. Каждый новый токен зависит от всех предыдущих (как изначальных, так и уже сгенерированных). Этот процесс является последовательным и ограничен пропускной способностью памяти (memory-bandwidth-bound). Он состоит из множества операций умножения матрицы на вектор, которые менее эффективно утилизируют вычислительные мощности GPU. Именно эта фаза является узким местом в скорости генерации текста.15

#### **Роль KV-кэширования**

Наивная реализация фазы декодирования была бы катастрофически медленной. При генерации каждого нового токена модели пришлось бы заново вычислять связи со всеми предыдущими токенами. Это привело бы к квадратичной зависимости времени вычислений от длины последовательности. Чтобы сделать этот процесс осуществимым, используется фундаментальная оптимизация — **Key-Value (KV) кэширование**.  
В процессе работы механизма внимания для каждого токена вычисляются три вектора: Query (Запрос), Key (Ключ) и Value (Значение). KV-кэширование заключается в сохранении векторов Key и Value для всех уже обработанных токенов. При генерации следующего токена модели нужно вычислить только его собственный Query-вектор и сравнить его с уже сохраненными в кэше парами Key-Value. Это позволяет избежать повторных вычислений и превращает процесс с квадратичной сложностью в процесс с линейной сложностью.15 Любая ручная реализация генерации текста, не использующая KV-кэш, будет на порядки медленнее встроенного метода .generate(), который активно его применяет.17

### **Голем-Интерпретатор: Постобработка и Расшифровка Ответов Оракула**

Третий Голем, Интерпретатор, получает сырой числовой вывод от Оракула и преобразует его в понятный для человека формат. Его работа сильно зависит от конкретной задачи.

#### **От Логитов к Вероятностям**

Для задач классификации (например, анализ тональности) сырой вывод модели представляет собой вектор чисел, называемых «логитами». Это не вероятности, а скорее «сырые оценки» уверенности модели в принадлежности к каждому классу. Чтобы преобразовать их в вероятности, которые суммируются в единицу, Интерпретатор применяет математическую функцию, чаще всего softmax. Для задач с несколькими возможными метками (multi-label classification) может использоваться функция sigmoid.1

#### **Декодирование Текста**

Для задач генерации текста работа Интерпретатора заключается в обратном процессе токенизации. Он получает последовательность числовых ID токенов, сгенерированных Оракулом, и, используя тот же словарь, что и Писец, преобразует их обратно в человекочитаемый текст.6

#### **Специфическая Логика Задач**

Для многих задач постобработка включает в себя сложную, нестандартную логику. Например, в задаче «ответов на вопросы» модель выдает логиты для каждого токена во входном контексте, указывая вероятность того, что данный токен является началом ответа, и вероятность того, что он является концом ответа. Задача Интерпретатора — проанализировать эти два набора вероятностей и найти наиболее правдоподобный отрезок текста (span), который и будет являться ответом.5 В задаче «распознавания именованных сущностей» (NER) Интерпретатор должен сгруппировать отдельные токены, помеченные как части одной сущности (например, "Нью" и "Йорк"), в единое целое. pipeline скрывает всю эту сложную, специфичную для каждой задачи логику, предоставляя пользователю только конечный, аккуратно отформатированный результат.2  
Следовательно, pipeline — это не просто последовательное выполнение трех шагов. Это система с жесткими взаимосвязями, где выбор одного Голема накладывает строгие ограничения на всех остальных. Токенизатор (Писец) определяет словарь и формат входных данных для модели. Архитектура модели (Оракул) определяет природу ее сырого вывода. А сама задача определяет логику, которую должен реализовать Интерпретатор для осмысленной интерпретации этого вывода. Таким образом, pipeline выступает не только как удобная обертка, но и как **гарант совместимости**. Он обеспечивает, чтобы все три Голема были из одной «школы» и могли безупречно взаимодействовать. Для новичка это огромное преимущество, избавляющее от множества потенциальных ошибок. Однако для эксперта, желающего модифицировать один из компонентов или комбинировать их нестандартным образом, эта жесткость становится серьезным ограничением.

## **Глава 2: Пределы Автоматизации: Когда pipeline Недостаточно**

Несмотря на свою мощь и удобство, pipeline не является универсальным решением для всех задач. Его простота достигается за счет компромиссов в производительности, гибкости и контроле. Существуют целые классы проблем, где опора исключительно на эту высокоуровневую абстракцию становится не просто неоптимальной, а невозможной.

### **Производительность и Эффективность: «Налог на Удобство»**

pipeline спроектирован так, чтобы быть надежным и работать в самых разных сценариях «из коробки». Однако эта универсальность сопряжена с дополнительными накладными расходами на производительность. Можно говорить о своего рода «налоге на удобство», когда за простоту использования приходится платить сниженной скоростью или неоптимальным использованием ресурсов.3

#### **Иллюзия Пакетной Обработки**

Пакетная обработка (batching) — передача нескольких образцов данных модели за один проход — может значительно увеличить пропускную способность на GPU. Однако это не панацея. Сама документация pipeline предостерегает от использования батчинга в определенных условиях: на CPU, при работе с последовательностями переменной длины или в приложениях, критичных к задержке (latency).1 Достижение оптимальной производительности с помощью батчинга требует тщательных измерений, экспериментов и тонкой настройки, что выходит за рамки простого указания параметра batch_size в вызове pipeline.

#### **Ускоренные Среды Выполнения**

Тот факт, что существует библиотека optimum и среда выполнения ONNX Runtime, является прямым доказательством того, что стандартное исполнение pipeline в PyTorch не является самым быстрым из возможных. Преобразование модели в статический граф с помощью ONNX может обеспечить значительное ускорение, особенно на центральных процессорах (CPU), за счет оптимизации операций и уменьшения накладных расходов интерпретатора Python.3 Этот шаг оптимизации происходит за пределами стандартного рабочего процесса pipeline и требует более глубокого понимания того, как модели могут быть скомпилированы и развернуты.

### **Бремя Гигантов: Укрощение Големов в Условиях Ограниченной Памяти**

Современные модели, особенно в области генерации текста, содержат миллиарды параметров. Это приводит к колоссальным требованиям к объему видеопамяти (VRAM).20 Например, модель с 7 миллиардами параметров в полной 32-битной точности (float32) требует около 28 ГБ VRAM только для хранения своих весов, не считая памяти, необходимой для промежуточных вычислений (активаций) и состояний оптимизатора.

#### **Почему pipeline по Умолчанию Терпит Неудачу**

Простая попытка запустить большую и мощную модель с помощью стандартного вызова, например, pipeline("text-generation", model="meta-llama/Llama-3-8B"), немедленно приведет к ошибке нехватки памяти (Out-of-Memory, OOM) на подавляющем большинстве потребительских GPU. Это жесткое ограничение, с которым подмастерье столкнется сразу же, как только попытается перейти от небольших демонстрационных моделей к современным, более производительным.

#### **Продвинутые Техники Управления Памятью**

Для работы с большими моделями требуется арсенал продвинутых техник, и все они требуют выхода за рамки базового вызова pipeline:

- **Квантование (Quantization):** Использование типов данных с более низкой точностью, таких как float16 (половинная точность) или int8 (8-битные целые числа), позволяет радикально сократить объем памяти, занимаемый весами модели. Параметр load_in_8bit=True или torch_dtype=torch.float16 при загрузке модели — это первый шаг к укрощению гигантов.3
- **Распределение Модели (Model Offloading):** Использование библиотеки accelerate с параметром device_map="auto" позволяет интеллектуально распределить слои модели по доступным ресурсам: сначала по GPU, затем, если место закончилось, по оперативной памяти CPU и даже по жесткому диску. Это делает возможным запуск моделей, которые физически не помещаются в VRAM ни одного из доступных GPU.21
- **Специализированные Методы Распределения:** Существуют и более сложные техники, такие как последовательная выгрузка на CPU (sequential_cpu_offload), которые позволяют экономить еще больше памяти, но ценой значительного замедления инференса. Выбор между этими методами — это всегда компромисс, требующий понимания внутренних процессов.20

### **Барьер Кастомизации и «Дырявая Абстракция»**

pipeline отлично справляется со стандартизированными задачами, для которых он был разработан. Но что, если задача выходит за эти рамки?

#### **Работа с Нестандартными Задачами**

Предположим, требуется не просто классифицировать текст, а извлечь из него структурированные данные в формате JSON, или реализовать многошаговый процесс рассуждений, где вывод одной модели становится входом для другой. Жесткая трехэтапная структура pipeline (препроцессинг, инференс, постобработка) делает реализацию такой сложной логики крайне затруднительной или вовсе невозможной. Для этого необходим прямой доступ к каждому Голему и возможность управлять потоком данных между ними.

#### **Проблема «Черного Ящика»**

Когда pipeline выдает неожиданный или неверный результат, отладка становится настоящим кошмаром. В чем причина ошибки? В особенностях токенизации? В артефактах работы модели? В неверных предположениях, заложенных в логику постобработки? Не имея возможности заглянуть внутрь и проверить промежуточные состояния каждого этапа, подмастерье остается лишь строить догадки.

#### **Воспроизводимость и Скрытые Параметры**

pipeline может применять по умолчанию различные параметры генерации (например, стратегии сэмплирования, штрафы за повторения), которые не всегда очевидны из его вызова.18 Это может приводить к тому, что результаты работы pipeline будут отличаться от результатов ручного вызова model.generate() с теми же входными данными, что вызывает путаницу и проблемы с воспроизводимостью, особенно в исследовательском контексте.18 Эта «дырявая абстракция» означает, что pipeline не так прозрачен, как кажется на первый взгляд, и скрывает важные детали, влияющие на конечный результат.  
Вся работа по развертыванию высокопроизводительных моделей — это не поиск одной «идеальной» настройки, а навигация в трехмерном пространстве компромиссов между **Скоростью**, **Использованием Памяти** и **Точностью/Возможностями Модели**. Использование более компактных моделей, таких как DistilBERT, или применение квантования повышает скорость и снижает требования к памяти, но почти всегда сопряжено с некоторым «приемлемым снижением точности».3 Пакетная обработка может увеличить пропускную способность (скорость для больших объемов данных), но при этом увеличивает использование памяти и задержку для одиночных запросов.19 Выбор более крупной и мощной модели повышает точность и возможности, но резко увеличивает требования к памяти.22 Эти три фактора неразрывно связаны. pipeline делает один, универсальный выбор в этом пространстве. Эксперт же должен уметь свободно перемещаться по этой «границе производительности», осознанно жертвуя одним параметром ради другого в зависимости от требований конкретного приложения: например, пожертвовать точностью ради скорости в реальном времени на мобильном устройстве или смириться с высоким потреблением памяти ради максимальной точности в научном исследовании. Этот уровень стратегического принятия решений и есть суть MLOps, и он невозможен без полного ручного контроля над всеми компонентами.

### **Таблица 1: pipeline против Ручного Контроля: Сравнительный Анализ**

| Критерий                   | Абстракция pipeline                                                                                                        | Ручной Контроль (AutoClass)                                                                                                              |
| :------------------------- | :------------------------------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------- |
| **Простота использования** | **Высокая.** Одна строка кода для выполнения сложных задач. Идеально для быстрого прототипирования и обучения.             | **Низкая.** Требует явного выполнения всех шагов: загрузка, токенизация, инференс, декодирование.                                        |
| **Оптимизация скорости**   | **Ограниченная.** Базовая поддержка батчинга. Не поддерживает ускоренные среды выполнения (ONNX) напрямую.                 | **Полная.** Детальный контроль над параметрами генерации, батчингом, возможность использования ONNX/TensorRT через optimum.              |
| **Управление памятью**     | **Очень ограниченное.** Базовое размещение на устройстве (device). Не работает с моделями, превышающими VRAM GPU.          | **Полная.** Поддержка квантования (8-бит, 4-бит), распределения модели (device_map), выгрузки на CPU, работы с разделенными чекпоинтами. |
| **Кастомизация**           | **Низкая.** Ограничена предопределенными задачами. Сложно реализовать нестандартную логику пре- и постобработки.           | **Неограниченная.** Полная свобода в реализации любой логики обработки данных до и после модели, создание сложных многоэтапных систем.   |
| **Отладка**                | **Сложная.** Работает как «черный ящик». Трудно диагностировать источник ошибок (токенизация, модель, постобработка).      | **Прозрачная.** Возможность инспектировать данные на каждом шаге: сырые токены, логиты, промежуточные состояния.                         |
| **Воспроизводимость**      | **Средняя.** Скрытые параметры по умолчанию могут приводить к неожиданным результатам и затруднять точное воспроизведение. | **Высокая.** Все параметры инференса задаются явно, что обеспечивает полный контроль и гарантированную воспроизводимость результатов.    |

## **Глава 3: Путь к Мастерству: Прямое Управление Големами**

Переход от теории к практике — ключевой этап в обучении. В этой главе мы оставим теоретические рассуждения и перейдем к конкретным действиям, демонстрируя, как взять под прямой контроль каждого из Големов. Это путь от использования готовых заклинаний к их самостоятельному созданию.

### **Ритуал Ручного Инференса: Пошаговое Руководство**

Следующий код демонстрирует полный цикл ручного инференса, который pipeline выполняет под капотом. Мы будем использовать AutoClass, который автоматически подбирает правильную архитектуру модели и токенизатора на основе имени модели.27

#### **Шаг 1: Загрузка Големов**

Первый шаг — загрузить модель (Оракула) и ее токенизатор (Писца). Для работы с большими моделями на ограниченном оборудовании мы используем параметры, которые обсуждались в предыдущей главе.

Python

import torch  
from transformers import AutoTokenizer, AutoModelForCausalLM

\# Имя модели на Hugging Face Hub  
model_name \= "google/gemma-2-2b"

\# Загрузка токенизатора  
tokenizer \= AutoTokenizer.from_pretrained(model_name)

\# Загрузка модели с параметрами для оптимизации памяти  
\# device_map="auto" \- автоматически распределит слои модели по GPU и CPU  
\# torch_dtype=torch.bfloat16 \- использует тип данных с пониженной точностью для экономии VRAM  
model \= AutoModelForCausalLM.from_pretrained(  
 model_name,  
 device_map="auto",  
 torch_dtype=torch.bfloat16  
)

#### **Шаг 2: Приказ Писцу (Токенизация)**

Теперь мы вручную токенизируем наш входной текст. Мы увидим, что tokenizer возвращает словарь с input_ids (числовые ID токенов) и attention_mask.

Python

\# Входной текст  
prompt \= "The secret to baking a really good cake is"

\# Ручная токенизация  
\# return_tensors="pt" \- возвращает тензоры PyTorch  
\#.to(model.device) \- перемещает тензоры на то же устройство, где находится модель  
inputs \= tokenizer(prompt, return_tensors="pt").to(model.device)

\# Выведем результат токенизации, чтобы увидеть, что получает модель  
print(inputs)  
\# Ожидаемый вывод:  
\# {'input_ids': tensor(\[\[ 2, 21101, 1115, 267, 23783, 263, 1134, 1587, 12221, 322\]\], device='cuda:0'),  
\# 'attention_mask': tensor(\[\], device='cuda:0')}

#### **Шаг 3: Обращение к Оракулу (Инференс)**

Мы вызываем метод model.generate(), передавая ему токенизированный ввод. Здесь мы можем явно указать множество параметров, управляющих процессом генерации, таких как max_new_tokens (максимальное количество новых токенов для генерации), do_sample (использовать ли сэмплирование) и temperature (контролирует случайность генерации).17

Python

\# Вызов генерации с явными параметрами  
outputs \= model.generate(\*\*inputs, max_new_tokens=20)

print(outputs)  
\# Ожидаемый вывод (последовательность ID токенов):  
\# tensor(\[\[ 2, 21101, 1115, 267, 23783, 263, 1134, 1587, 12221, 322,  
\# 263, 1134, 2673, 16271, 1115, 267, 1134, 12221, 322, 263,  
\# 1134, 1587, 4191, 1115, 267, 1134, 12221, 322, 263, 1134\]\], device='cuda:0')

#### **Шаг 4: Интерпретация Ответа**

Наконец, мы используем токенизатор для декодирования числовой последовательности обратно в текст.

Python

\# Декодирование результата  
result_text \= tokenizer.batch_decode(outputs, skip_special_tokens=True)  
print(result_text)  
\# Ожидаемый вывод:  
\# 'The secret to baking a really good cake is a good recipe. A good cake is a good cake is a good cake is a good'

Этот четырехэтапный ритуал дает полный контроль над процессом, который pipeline выполняет как единую, непрозрачную операцию.

### **Суть Адаптации: Токенизация и Вызовы Новых Доменов**

Возможно, самый важный аспект, который становится очевидным при переходе к ручному управлению, — это критическая роль токенизации при адаптации моделей к новым, специализированным областям.

#### **Золотое Правило Файн-тюнинга**

Существует одно незыблемое правило тонкой настройки (fine-tuning): **всегда используйте токенизатор базовой модели**.29 Модель была предварительно обучена на миллиардах токенов, и ее «понимание» языка, вся ее внутренняя структура знаний, основана на конкретном сопоставлении субслов с числовыми ID из ее словаря. Использование другого токенизатора во время файн-тюнинга равносильно попытке обучить модель на совершенно чужом языке. Векторные представления (embeddings), связанные с ID из старого словаря, будут применяться к токенам из нового, что приведет к семантическому хаосу и полной деградации модели.

#### **Несоответствие Словарей и OOV-токены**

Проблема возникает, когда мы начинаем тонкую настройку на данных из узкоспециализированной области, будь то медицина, юриспруденция или финансы.30 Эти тексты изобилуют специфической терминологией, которая отсутствовала в общецелевых данных, на которых обучалась базовая модель. Такие термины становятся **Out-of-Vocabulary (OOV)** токенами — словами, которых нет в словаре токенизатора.29  
Стандартный токенизатор, столкнувшись с OOV-словом, разбивает его на известные ему субсловесные части. Например, критически важный медицинский термин «миокардиальный инфаркт» может быть разбит на бессмысленные с точки зрения медицины части: \['ми', '\#\#окар', '\#\#диаль', '\#\#ный', 'ин', '\#\#фар', '\#\#кт'\]. В результате модель теряет способность воспринимать этот термин как единую семантическую единицу. Ей приходится заново учить значение целого понятия из его фрагментов, что крайне неэффективно и снижает качество обучения.12

#### **Узкое Место Контекстного Окна**

Неэффективная токенизация (когда на одно слово приходится много токенов) напрямую влияет на то, сколько реальной информации поместится в ограниченное **контекстное окно** модели.29 Если юридический документ из\-за обилия OOV-терминов токенизируется неэффективно, его длина в токенах может превысить лимит контекстного окна. В результате модель просто не увидит важные положения в конце документа, что сделает невозможным правильный анализ.

#### **Решение Эксперта: Специализация Писца**

Для серьезной доменной адаптации простого следования «Золотому правилу» может быть недостаточно. Настоящий экспертный подход заключается в модификации самого токенизатора. Исследования показывают, что специализация токенизатора для целевого домена — например, путем добавления в его словарь новых, специфичных для домена терминов, или даже путем обучения нового токенизатора с нуля на целевом корпусе текстов — может привести к значительному выигрышу в эффективности. Это сокращает количество токенов, необходимых для представления текста, что, в свою очередь, увеличивает скорость генерации, эффективный размер контекста и снижает затраты на обучение и инференс.32 Это уровень мастерства, который находится далеко за пределами возможностей pipeline.

### **Создание Улучшенного Голема: Практическое Руководство по Оптимизации**

Владея ручным контролем, подмастерье может применять широкий спектр техник для создания более быстрых, компактных и эффективных Големов.

#### **Реализация Экономии Памяти**

Как было показано в Шаге 1 ручного ритуала, параметры device_map="auto" и torch_dtype являются первым шагом. Для еще большей экономии можно использовать 8-битное или даже 4-битное квантование с помощью библиотеки bitsandbytes.

Python

\# Пример загрузки модели с 8-битным квантованием  
\# Требуется установка \`pip install bitsandbytes\`  
model_8bit \= AutoModelForCausalLM.from_pretrained(  
 model_name,  
 device_map="auto",  
 load_in_8bit=True  
)

#### **Реализация Ускорения**

Для ускорения инференса можно использовать библиотеку optimum для преобразования модели в формат ONNX.

Python

\# Пример экспорта и использования модели в формате ONNX  
\# Требуется установка \`pip install optimum\[onnxruntime\]\`  
from optimum.onnxruntime import ORTModelForCausalLM

\# 1\. Экспортируем модель в ONNX  
ort_model \= ORTModelForCausalLM.from_pretrained(model_name, export=True)  
\# 2\. Сохраняем ее  
ort_model.save_pretrained("./gemma-onnx")  
tokenizer.save_pretrained("./gemma-onnx")

\# 3\. Загружаем и используем ускоренную модель  
ort_model_loaded \= ORTModelForCausalLM.from_pretrained("./gemma-onnx", device_map="auto")  
\# Дальнейший инференс аналогичен стандартному  
inputs \= tokenizer(prompt, return_tensors="pt").to(ort_model_loaded.device)  
outputs \= ort_model_loaded.generate(\*\*inputs, max_new_tokens=20)  
print(tokenizer.batch_decode(outputs, skip_special_tokens=True))

#### **Выбор Правильного Голема**

Оптимизация — это не только код, но и правильный выбор инструмента. Часто для задачи не требуется самая большая и мощная модель. Использование дистиллированных моделей, таких как DistilBERT, которые сохраняют большую часть производительности своих более крупных «учителей» при значительно меньшем количестве параметров, может быть гораздо более эффективным решением с точки зрения баланса скорости, памяти и точности.3  
Экосистема Hugging Face спроектирована как путь с **постепенно возрастающей сложностью**. pipeline — это широкий и гостеприимный вход. Но по мере роста потребностей пользователя, он естественным образом знакомится с более специализированными инструментами: datasets для работы с большими наборами данных 33, accelerate для распределенного обучения и запуска больших моделей 21, optimum для ускорения инференса 3, peft для эффективного файн-тюнинга 34 и bitsandbytes для квантования. Эта структура не случайна. Это продуманный педагогический и инженерный дизайн, который позволяет пользователям осваивать сложность по мере необходимости. «Квесты» подмастерья по изучению Големов — это не отклонение от пути, а следующий логический шаг по маршруту, проложенному создателями платформы.

### **Таблица 2: Продвинутые Техники Оптимизации Инференса Трансформеров**

| Техника                     | Основная Цель                                                                              | Ключевая Библиотека/Инструмент | Основной Компромисс                                                       |
| :-------------------------- | :----------------------------------------------------------------------------------------- | :----------------------------- | :------------------------------------------------------------------------ |
| **Квантование (8/4-бит)**   | Сокращение занимаемой памяти, ускорение на совместимом оборудовании.                       | bitsandbytes, auto-gptq        | Небольшая потенциальная потеря точности.                                  |
| **ONNX Runtime**            | Ускорение инференса на CPU/GPU за счет компиляции в статический граф.                      | optimum                        | Потеря гибкости динамического графа PyTorch, требует этапа экспорта.      |
| **KV-кэширование**          | Критическое ускорение авторегрессионной генерации текста.                                  | torch, transformers            | Увеличивает потребление памяти с ростом длины последовательности.         |
| **Распределение Модели**    | Запуск моделей, превышающих VRAM одного устройства.                                        | accelerate                     | Замедление инференса из\-за передачи данных между устройствами (GPU/CPU). |
| **Дистиллированные Модели** | Достижение баланса между производительностью и ресурсами.                                  | Model Hub (e.g., DistilBERT)   | Более низкая точность по сравнению с полноразмерными моделями-учителями.  |
| **Flash Attention**         | Ускорение механизма внимания и снижение потребления памяти за счет аппаратных оптимизаций. | flash-attn                     | Требует специфического оборудования (современные GPU NVIDIA) и установки. |

## **Заключение: Восхождение от Пользователя к Архитектору**

Мы завершаем наше путешествие от обманчивой простоты pipeline к детальному и мощному ручному контролю над Големами. Вывод однозначен: pipeline — это превосходный инструмент для быстрого прототипирования, изучения основ и решения стандартных, хорошо определенных задач. Это первая книга заклинаний для начинающего мага, позволяющая творить чудеса без глубокого понимания их природы.  
Однако истинное мастерство заключается не в чтении готовых заклинаний, а в способности их создавать. Оно проявляется в глубоком знании, позволяющем диагностировать узкие места в производительности, делать осознанные компромиссы между скоростью, памятью и точностью, создавать решения для совершенно новых, нестандартных проблем и, в конечном счете, раздвигать границы возможного. В этом и заключается фундаментальное различие между пользователем инструментов и архитектором систем.  
Путь к пониманию Големов сложен и требует усилий. Но именно эти трудности и формируют эксперта. Задав вопрос «Зачем?», подмастерье уже сделал самый важный и правильный шаг на этом пути. Это любопытство, стремление заглянуть за завесу абстракции и понять фундаментальные принципы, — вот что отличает будущего мастера.

#### **Источники**

1. Pipelines \- Hugging Face, дата последнего обращения: октября 23, 2025, [https://huggingface.co/docs/transformers/main_classes/pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines)
2. Pipelines — transformers 3.0.2 documentation \- Hugging Face, дата последнего обращения: октября 23, 2025, [https://huggingface.co/transformers/v3.0.2/main_classes/pipelines.html](https://huggingface.co/transformers/v3.0.2/main_classes/pipelines.html)
3. 5 Tips for Building Optimized Hugging Face Transformer Pipelines ..., дата последнего обращения: октября 23, 2025, [https://www.kdnuggets.com/5-tips-for-building-optimized-hugging-face-transformer-pipelines](https://www.kdnuggets.com/5-tips-for-building-optimized-hugging-face-transformer-pipelines)
4. Pipelines \- Hugging Face, дата последнего обращения: октября 23, 2025, [https://huggingface.co/docs/transformers/v4.31.0/main_classes/pipelines](https://huggingface.co/docs/transformers/v4.31.0/main_classes/pipelines)
5. Pipelines — transformers 3.5.0 documentation \- Hugging Face, дата последнего обращения: октября 23, 2025, [https://huggingface.co/transformers/v3.5.1/main_classes/pipelines.html](https://huggingface.co/transformers/v3.5.1/main_classes/pipelines.html)
6. Getting Started with Hugging Face Transformers: A Practical Guide | by anirudh Bhatnagar, дата последнего обращения: октября 23, 2025, [https://medium.com/@anirudh.bh/getting-started-with-hugging-face-transformers-a-practical-guide-d0ea5ae592da](https://medium.com/@anirudh.bh/getting-started-with-hugging-face-transformers-a-practical-guide-d0ea5ae592da)
7. What is Tokenization? Types, Use Cases, Implementation | DataCamp, дата последнего обращения: октября 23, 2025, [https://www.datacamp.com/blog/what-is-tokenization](https://www.datacamp.com/blog/what-is-tokenization)
8. Breaking Language into Tokens: How Transformers Process Information? \- Hugging Face, дата последнего обращения: октября 23, 2025, [https://huggingface.co/blog/royswastik/transformer-tokenization-vocabulary-creation](https://huggingface.co/blog/royswastik/transformer-tokenization-vocabulary-creation)
9. Summary of the tokenizers \- Hugging Face, дата последнего обращения: октября 23, 2025, [https://huggingface.co/docs/transformers/tokenizer_summary](https://huggingface.co/docs/transformers/tokenizer_summary)
10. Understanding Tokenizers, Embeddings, and Transformers in NLP | by Software Chasers, дата последнего обращения: октября 23, 2025, [https://medium.com/@softwarechasers/understanding-tokenizers-embeddings-and-transformers-in-nlp-879cfeb6a63d](https://medium.com/@softwarechasers/understanding-tokenizers-embeddings-and-transformers-in-nlp-879cfeb6a63d)
11. Tokenization in Transformers. The recent AI research and development… | by Abhijith Prasad M | Medium, дата последнего обращения: октября 23, 2025, [https://medium.com/@abhijithprasadmkp/tokenization-in-transformers-9e8b0a5fd5f4](https://medium.com/@abhijithprasadmkp/tokenization-in-transformers-9e8b0a5fd5f4)
12. Impact of Tokenization, Pretraining Task, and Transformer Depth on Text Ranking, дата последнего обращения: октября 23, 2025, [https://trec.nist.gov/pubs/trec29/papers/UAmsterdam.DL.pdf](https://trec.nist.gov/pubs/trec29/papers/UAmsterdam.DL.pdf)
13. Transformer (deep learning architecture) \- Wikipedia, дата последнего обращения: октября 23, 2025, [https://en.wikipedia.org/wiki/Transformer\_(deep_learning_architecture)](<https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)>)
14. Chapter 10: Training, Fine-tuning, and Evaluation of Transformer Models | 10.1 Preprocessing Data for Transformers \- Cuantum Technologies, дата последнего обращения: октября 23, 2025, [https://www.cuantum.tech/app/section/101-preprocessing-data-for-transformers-2baa214fce2d40169cd83337de75573c](https://www.cuantum.tech/app/section/101-preprocessing-data-for-transformers-2baa214fce2d40169cd83337de75573c)
15. Transformer Inference: Techniques for Faster AI Models \- Prem AI, дата последнего обращения: октября 23, 2025, [https://blog.premai.io/transformer-inference-techniques-for-faster-ai-models/](https://blog.premai.io/transformer-inference-techniques-for-faster-ai-models/)
16. How Inference is done in Transformer? | by Sachinsoni \- Medium, дата последнего обращения: октября 23, 2025, [https://medium.com/@sachinsoni600517/how-inference-is-done-in-transformer-3a1fd1a8bfea](https://medium.com/@sachinsoni600517/how-inference-is-done-in-transformer-3a1fd1a8bfea)
17. What makes the built-in generate method faster than a crude manual implementation?, дата последнего обращения: октября 23, 2025, [https://discuss.huggingface.co/t/what-makes-the-built-in-generate-method-faster-than-a-crude-manual-implementation/69629](https://discuss.huggingface.co/t/what-makes-the-built-in-generate-method-faster-than-a-crude-manual-implementation/69629)
18. Pipeline vs model.generate() \- Beginners \- Hugging Face Forums, дата последнего обращения: октября 23, 2025, [https://discuss.huggingface.co/t/pipeline-vs-model-generate/26203](https://discuss.huggingface.co/t/pipeline-vs-model-generate/26203)
19. Pipeline \- Hugging Face, дата последнего обращения: октября 23, 2025, [https://huggingface.co/docs/transformers/pipeline_tutorial](https://huggingface.co/docs/transformers/pipeline_tutorial)
20. Reduce memory usage \- Hugging Face, дата последнего обращения: октября 23, 2025, [https://huggingface.co/docs/diffusers/main/optimization/memory](https://huggingface.co/docs/diffusers/main/optimization/memory)
21. Loading big models into memory \- Hugging Face, дата последнего обращения: октября 23, 2025, [https://huggingface.co/docs/accelerate/concept_guides/big_model_inference](https://huggingface.co/docs/accelerate/concept_guides/big_model_inference)
22. Model memory estimator \- Hugging Face, дата последнего обращения: октября 23, 2025, [https://huggingface.co/docs/accelerate/usage_guides/model_size_estimator](https://huggingface.co/docs/accelerate/usage_guides/model_size_estimator)
23. Model training anatomy \- Hugging Face, дата последнего обращения: октября 23, 2025, [https://huggingface.co/docs/transformers/v4.32.1/model_memory_anatomy](https://huggingface.co/docs/transformers/v4.32.1/model_memory_anatomy)
24. Memory-Efficient Fine-Tuning of Transformers via Token Selection \- arXiv, дата последнего обращения: октября 23, 2025, [https://arxiv.org/html/2501.18824v1](https://arxiv.org/html/2501.18824v1)
25. Pipeline \- Hugging Face, дата последнего обращения: октября 23, 2025, [https://huggingface.co/docs/transformers/main/pipeline_tutorial](https://huggingface.co/docs/transformers/main/pipeline_tutorial)
26. Hugging Face pipeline vs manual processing produces different embeddings for Vision Transformers \- Stack Overflow, дата последнего обращения: октября 23, 2025, [https://stackoverflow.com/questions/78784330/hugging-face-pipeline-vs-manual-processing-produces-different-embeddings-for-vis](https://stackoverflow.com/questions/78784330/hugging-face-pipeline-vs-manual-processing-produces-different-embeddings-for-vis)
27. Quickstart \- Hugging Face, дата последнего обращения: октября 23, 2025, [https://huggingface.co/docs/transformers/quicktour](https://huggingface.co/docs/transformers/quicktour)
28. Using transformers at Hugging Face, дата последнего обращения: октября 23, 2025, [https://huggingface.co/docs/hub/transformers](https://huggingface.co/docs/hub/transformers)
29. tokenization-fine-tuning-accuracy | Article, дата последнего обращения: октября 23, 2025, [https://www.swiftorial.com/articles/llm-fine-tuning/tokenization-fine-tuning-accuracy](https://www.swiftorial.com/articles/llm-fine-tuning/tokenization-fine-tuning-accuracy)
30. A Survey on Challenges and Advances in Natural Language ... \- MDPI, дата последнего обращения: октября 23, 2025, [https://www.mdpi.com/2079-9292/13/3/648](https://www.mdpi.com/2079-9292/13/3/648)
31. Challenges and barriers of using large language models (LLM) such ..., дата последнего обращения: октября 23, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC10898121/](https://pmc.ncbi.nlm.nih.gov/articles/PMC10898121/)
32. Getting the most out of your tokenizer for pre-training and domain adaptation \- arXiv, дата последнего обращения: октября 23, 2025, [https://arxiv.org/html/2402.01035v2](https://arxiv.org/html/2402.01035v2)
33. Getting Started with Sentiment Analysis using Python \- Hugging Face, дата последнего обращения: октября 23, 2025, [https://huggingface.co/blog/sentiment-analysis-python](https://huggingface.co/blog/sentiment-analysis-python)
34. Documentation \- Hugging Face, дата последнего обращения: октября 23, 2025, [https://huggingface.co/docs](https://huggingface.co/docs)
