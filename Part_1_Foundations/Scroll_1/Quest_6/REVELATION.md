# **Архитектурный анализ и практическая реализация алгоритмов генерации текста: Протокол Quest 1.6 и модель DistilGPT-2**

## **1\. Введение: Эволюция генеративного ИИ и императив эффективности**

Современный ландшафт обработки естественного языка (NLP) характеризуется фундаментальным противоречием между стремлением к масштабируемости и необходимостью доступности. В то время как передовой край исследований смещается в сторону моделей с триллионами параметров, требующих кластеров GPU промышленного масштаба, существует параллельный и не менее критичный вектор развития, направленный на демократизацию технологий искусственного интеллекта. Этот вектор представлен малыми языковыми моделями (Small Language Models, SLM) и методами компрессии нейронных сетей, такими как дистилляция знаний.

Данный аналитический отчет посвящен всестороннему рассмотрению задачи, обозначенной как «Quest 1.6: Призыв Духа-Сказителя». С технической точки зрения, данный квест представляет собой задачу реализации конвейера (pipeline) генерации текста с использованием модели distilgpt2 в экосистеме Hugging Face. Выбор данной архитектуры неслучаен: distilgpt2 является каноническим примером успешного применения методов дистилляции для создания легковесной, производительной модели, способной функционировать в условиях ограниченных вычислительных ресурсов, включая стандартные центральные процессоры (CPU).

Целью данного документа является не просто предоставление кода для решения задачи, но и глубокий декомпозиционный анализ всех компонентов системы: от теоретических основ архитектуры Transformer и математики дистилляции знаний до низкоуровневых нюансов токенизации кириллических текстов и стратегий стохастического декодирования. Мы также проведем сравнительный анализ distilgpt2 с современными моделями 2024–2025 годов, такими как Phi-2 и TinyLlama, чтобы определить место рассматриваемой технологии в актуальном стеке инструментов разработчика ИИ.

---

## **2\. Теоретический фундамент: Архитектура Generative Pre-trained Transformer (GPT)**

Для того чтобы понять, как функционирует «Дух-Сказитель» (агент генерации текста), необходимо сначала разобрать архитектурные принципы, лежащие в основе семейства моделей GPT. В отличие от оригинальной архитектуры Transformer, предложенной Vaswani et al. в 2017 году, которая использовала структуру «Энкодер-Декодер» для задач перевода (sequence-to-sequence), семейство GPT опирается исключительно на декодерную часть (Decoder-only architecture).

### **2.1. Задача каузального языкового моделирования (Causal Language Modeling)**

Фундаментальной задачей, которую решает архитектура GPT, является каузальное языковое моделирование (CLM). Формально, если мы имеем последовательность токенов $U \= \\{u\_1, u\_2,..., u\_n\\}$, цель модели — оценить вероятность появления следующего токена $u\_{n+1}$, основываясь исключительно на контексте предыдущих токенов. Совместная вероятность всей последовательности факторизуется как произведение условных вероятностей:

$$P(U) \= \\prod\_{i=1}^{n} P(u\_i | u\_1,..., u\_{i-1})$$  
Эта авторегрессионная природа является критически важной для функции «рассказчика».1 Модель не «видит» будущее; она галлюцинирует следующий шаг, основываясь на статистических закономерностях, усвоенных в процессе предварительного обучения (pre-training). В контексте distilgpt2 это обучение происходило под наблюдением (supervision) более крупной модели GPT-2, что позволило перенести понимание синтаксиса, семантики и нарративной структуры английского языка в сжатое пространство весов.3

### **2.2. Механизм маскированного самовнимания (Masked Self-Attention)**

Двигателем, осуществляющим вычисление этих вероятностей, является механизм маскированного самовнимания (Masked Self-Attention). В стандартном слое внимания токен на позиции $i$ теоретически может взаимодействовать с любым другим токеном в последовательности. Однако для каузального генератора позволить позиции $i$ «смотреть» на позицию $i+1$ означало бы нарушение принципа причинности — модель увидела бы правильный ответ до того, как его предсказала.

Поэтому модели GPT используют треугольную матрицу маски $M$, где $M\_{ij} \= \-\\infty$, если $j \> i$. Эта маска применяется к масштабированным результатам скалярного произведения (scaled dot-product attention) перед операцией softmax:

$$\\text{Attention}(Q, K, V) \= \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d\_k}} \+ M\\right)V$$  
Где $Q$ (Запрос), $K$ (Ключ) и $V$ (Значение) — это линейные проекции входных эмбеддингов. Этот механизм гарантирует, что «Дух-Сказитель» генерирует повествовательную непрерывность, строго придерживаясь временного потока текста и предотвращая утечку информации из будущих токенов.5

---

## **3\. Дистилляция знаний: От GPT-2 к DistilGPT-2**

Ключевым аспектом данного исследования является использование модели distilgpt2. Это не просто «маленькая» модель, обученная с нуля; это продукт сложного процесса компрессии, известного как дистилляция знаний (Knowledge Distillation, KD). Этот процесс позволяет перенести интеллектуальные способности массивной модели-учителя (Teacher) в более компактную модель-ученика (Student).

### **3.1. Парадигма «Учитель-Ученик»**

В процессе стандартного обучения нейронных сетей модель минимизирует функцию потерь (обычно перекрестную энтропию) между своими предсказаниями и истинными метками (hard targets). Истинные метки представляют собой one-hot векторы, где правильному слову присвоена вероятность 1, а всем остальным — 0\. Однако такие метки содержат ограниченное количество информации. Они говорят модели, что слово «кот» является правильным продолжением фразы «Пушистый...», но они не сообщают, что слово «зверь» было бы приемлемой альтернативой, а слово «трактор» — полной бессмыслицей.

Дистилляция знаний, опираясь на работы Sanh et al. 3, использует «мягкие метки» (soft targets) — распределение вероятностей, генерируемое моделью-учителем (в данном случае GPT-2 Small, 124 млн параметров). Эти мягкие метки содержат богатую семантическую информацию о взаимосвязях между словами, так называемое «темное знание» (dark knowledge).

### **3.2. Функция потерь и инициализация**

distilgpt2 обучается путем минимизации комбинированной функции потерь, состоящей из двух основных компонентов:

1. Потеря дистилляции (Distillation Loss): Вычисляется как дивергенция Кульбака-Лейблера (KL Divergence) между мягкими логитами ученика и мягкими логитами учителя. Это заставляет ученика имитировать неуверенность и структуру вероятностей учителя.

   $$L\_{KD} \= \\sum p\_{teacher}(x) \\log \\left( \\frac{p\_{teacher}(x)}{p\_{student}(x)} \\right)$$

2. **Потеря косинусного расстояния (Cosine Embedding Loss):** Этот компонент выравнивает скрытые состояния (hidden states) ученика и учителя, гарантируя, что внутренние представления данных (эмбеддинги) остаются согласованными и направлены в одном векторном пространстве.3

### **3.3. Архитектурное прунирование (Layer Truncation)**

Основным методом уменьшения размера модели в distilgpt2 стало усечение количества слоев трансформера. В то время как размерность эмбеддинга осталась идентичной учителю (768 измерений), количество слоев было сокращено вдвое.

| Характеристика                | GPT-2 Small (Teacher) | DistilGPT-2 (Student) |
| :---------------------------- | :-------------------- | :-------------------- |
| **Количество слоев**          | 12                    | 6                     |
| **Параметры**                 | 124 Миллиона          | 82 Миллиона           |
| **Размер на диске**           | \~523 MB              | \~336 MB              |
| **Перплексия (WikiText-103)** | 16.3                  | 21.1                  |

Это сокращение не было случайным. Инициализация весов ученика производилась путем взятия каждого второго слоя учителя. Такой подход обеспечивает надежную стартовую точку, так как веса уже содержат значимые лингвистические признаки.4

Результатом этого процесса стала модель, которая на **33% легче** и **в 2 раза быстрее** при инференсе, сохраняя при этом около 97-98% производительности учителя на задачах понимания языка (например, GLUE), хотя перплексия (мера неуверенности модели) на генеративных задачах несколько возросла (с 16.3 до 21.1).4 Это означает, что для «Духа-Сказителя» мы жертвуем некоторой долей нарративной связности ради возможности запуска модели на потребительском оборудовании.

---

## **4\. Абстракция pipeline в библиотеке Transformers**

Вторая часть задачи квеста требует использования абстракции pipeline. В экосистеме Hugging Face transformers это высокоуровневый API, который инкапсулирует сложность трех последовательных этапов жизненного цикла инференса: препроцессинга, прогона модели (forward pass) и постпроцессинга.

### **4.1. Этап 1: Препроцессинг и Токенизация**

Прежде чем модель сможет обработать текст «Призыв Духа», этот текст должен быть преобразован в числовые тензоры. pipeline автоматически загружает и использует токенизатор, ассоциированный с чекпоинтом модели.

Для семейства GPT-2 используется **Byte-Level Byte Pair Encoding (BPE)**.8 Это критически важное архитектурное решение, отличающее его от других подходов (например, WordPiece в BERT). Стандартный BPE объединяет часто встречающиеся пары символов. Byte-Level BPE работает непосредственно с последовательностью байтов UTF-8. Это позволяет модели обрабатывать любую строку текста, независимо от языка или наличия спецсимволов, избегая появления токена \<UNK\> (неизвестный токен), который является бичом многих NLP-систем.10

Токенизатор оперирует словарем размером **50,257 токенов**.9 Распространенные английские слова отображаются в единичные токены. Редкие слова и слова на других языках (включая русский) декомпозируются на последовательности байтов или суб-слов. На выходе препроцессинга формируется тензор размерности (Batch_Size, Sequence_Length).

### **4.2. Этап 2: Инференс (Forward Pass)**

На этом этапе pipeline управляет передачей тензоров на вычислительное устройство. Аргумент device играет здесь ключевую роль:

- device=-1: Принудительное использование CPU. Благодаря малому размеру distilgpt2 (82M параметров), инференс на CPU происходит с приемлемой задержкой, что делает модель идеальной для локальных приложений.11
- device=0: Использование первого доступного GPU (CUDA).

Модель пропускает входные ID через 6 слоев трансформера. На каждом шаге модель вычисляет **логиты** (logits) — сырые, ненормализованные предсказания для следующего токена. Для словаря размером 50,257 выход последнего слоя представляет собой вектор размерностью 50,257 для каждой позиции в последовательности.

Важно отметить, что pipeline скрывает сложность управления масками внимания (attention_mask) и токенами заполнения (pad_token_id). Модели GPT-2 не имеют встроенного токена паддинга, что часто требует от разработчика ручного назначения tokenizer.pad_token \= tokenizer.eos_token во избежание ошибок при пакетной обработке (batch processing), о чем свидетельствуют дискуссии в сообществе разработчиков.13

### **4.3. Этап 3: Постпроцессинг и стратегии декодирования**

Сырые логиты должны быть преобразованы обратно в текст. Именно здесь вступают в силу параметры конфигурации генерации, которые определяют «характер» Духа-Сказителя.

По умолчанию метод generate() в старых версиях библиотеки использовал жадное декодирование (greedy decoding), выбирая токен с наивысшей вероятностью. Однако для творческих задач это губительно: модель быстро начинает повторяться («зацикливаться»), выдавая текст вроде «I went to the the the...».1

Для реализации «творческого духа» необходимо активировать стохастические методы через параметры, передаваемые в pipeline:

- **do_sample=True**: Этот флаг переключает режим с детерминированного выбора максимума на вероятностное сэмплирование. Модель выбирает следующее слово случайно, но с вероятностью, пропорциональной весу, предсказанному нейросетью.14
- **max_new_tokens**: Определяет горизонт планирования модели. В отличие от max_length (который включает длину промпта), этот параметр задает чистый объем генерируемого контента.14
- **temperature**: Гиперпараметр $T$, который масштабирует логиты $x\_i$ перед функцией softmax: $P\_i \= \\frac{\\exp(x\_i / T)}{\\sum \\exp(x\_j / T)}$.
  - При $T \> 1.0$ распределение «сплющивается», увеличивая вероятность выбора редких слов (хаос, креативность).
  - При $T \< 1.0$ распределение заостряется, усиливая доминирование наиболее вероятных слов (консерватизм, логика).
- **top_k и top_p (Nucleus Sampling)**: Стратегии усечения «хвоста» распределения. Они запрещают модели выбирать слова с ничтожно малой вероятностью, даже при высокой температуре, тем самым поддерживая грамматическую связность текста.14

---

## **5\. Реализация квеста: Скрипт «Call of the Storyteller»**

На основе проанализированных данных 11 мы можем реконструировать оптимальный код для выполнения квеста. Ключевым требованием является использование pipeline и грамотная конфигурация параметров генерации для избежания типичных проблем GPT-2 (повторы, потеря контекста).

### **5.1. Инициализация и настройка окружения**

Первым шагом является импорт необходимых модулей и создание экземпляра пайплайна. Важно явно указать фреймворк (pt для PyTorch), чтобы избежать неоднозначности, если в среде установлены и TensorFlow, и PyTorch.11

Python

from transformers import pipeline, set_seed

\# Фиксация зерна генератора для воспроизводимости результатов (опционально)  
set_seed(42)

\# Инициализация пайплайна генерации текста  
\# Модель distilgpt2 будет автоматически загружена (\~336MB)  
\# device=-1 указывает на использование CPU  
storyteller \= pipeline(  
 task="text-generation",  
 model="distilgpt2",  
 framework="pt",  
 device=-1  
)

### **5.2. Конфигурация генерации**

Для получения связного и интересного текста мы должны отойти от дефолтных настроек. Квест подразумевает «рассказчика», поэтому мы используем сэмплирование.

Python

\# Промпт \- начальная фраза для духа  
prompt \= "In the ancient ruins of the digital realm, the spirit awoke and said,"

\# Запуск генерации с явным указанием параметров стратегии  
story \= storyteller(  
 prompt,  
 max_new_tokens=150, \# Генерируем 150 новых токенов  
 do_sample=True, \# Включаем стохастичность  
 temperature=0.8, \# Баланс между креативностью и логикой  
 top_k=50, \# Ограничиваем выбор топ-50 вероятными токенами  
 top_p=0.95, \# Nucleus sampling для динамического словаря  
 repetition_penalty=1.2, \# Штраф за повторы слов  
 num_return_sequences=1, \# Генерируем одну историю  
 pad_token_id=storyteller.tokenizer.eos_token_id \# Фикс предупреждения об отсутствии pad_token  
)

\# Вывод результата  
print(f"Generated Story:\\n{story\['generated_text'\]}")

Использование pad_token_id здесь критично, так как без него библиотека может выдать предупреждение или ошибку при внутренней обработке тензоров, поскольку distilgpt2 не имеет выделенного токена для заполнения пустот.13 Параметр repetition_penalty (не упомянутый в базовом списке, но присутствующий в документации GenerationConfig 15) крайне важен для distilgpt2, так как малые модели особенно склонны к зацикливанию.

---

## **6\. Проблема мультиязычности: «Кириллический налог»**

Особого внимания заслуживает тот факт, что в запросе фигурирует русский текст («Призыв Духа-Сказителя»), в то время как distilgpt2 — это модель, обученная преимущественно на английском корпусе OpenWebText.3 Анализ токенизации раскрывает серьезные ограничения при работе с не-английскими языками.

### **6.1. Неэффективность Byte-Level BPE для кириллицы**

Токенизатор GPT-2 имеет словарь, оптимизированный под английский язык. Когда он встречает английское слово, например «forest», оно часто кодируется одним токеном. Однако слова на кириллице отсутствуют в явном виде в словаре слияний (merge rules) BPE.

В результате токенизатор вынужден откатываться на байтовый уровень. Русские символы в кодировке UTF-8 занимают 2 байта. Поскольку модель не «знает» русских букв как единых сущностей, она часто токенизирует их побайтово или посимвольно.

Исследования и эксперименты сообщества 16 показывают драматическую разницу в эффективности кодирования:

- **Английский язык:** В среднем \~1.3 токена на слово.
- **Русский язык:** В среднем \~3.3 токена на слово (иногда доходит до 1 токена на букву).

### **6.2. Последствия для инференса и качества**

Этот феномен, который можно назвать «кириллическим налогом» (Cyrillic Tax), имеет три фундаментальных последствия для нашего квеста:

1. **Сокращение контекстного окна:** Максимальная длина последовательности для distilgpt2 составляет 1024 токена.5 Если на английском это позволяет вместить около 750 слов, то на русском эффективное окно сжимается до \~300 слов. «Дух-Сказитель» будет иметь крайне короткую память при общении на русском.
2. **Вычислительная сложность:** Сложность внимания в трансформере растет квадратично от длины последовательности ($O(n^2)$). Поскольку для выражения той же мысли на русском требуется в 3 раза больше токенов, вычислительная нагрузка возрастает непропорционально, замедляя генерацию.
3. **Деградация качества:** Поскольку модель воспринимает русский текст как поток байтов, а не семантических единиц (слов), ей сложнее улавливать грамматические конструкции (падежи, склонения). distilgpt2 не была обучена на большом корпусе русского текста, поэтому, даже если она технически сможет сгенерировать кириллицу, результат будет напоминать транслитерацию или бессвязный набор слов, если не применить fine-tuning.20

**Вывод:** Для выполнения квеста на русском языке использование ванильной distilgpt2 является субоптимальным. Рекомендуется либо использовать английский язык для промптов, либо переключиться на многоязычные модели (например, mGPT или ruGPT-3), если условия квеста это позволяют.

---

## **7\. Сравнительный анализ: DistilGPT-2 в ландшафте 2025 года**

Для полноты исследования необходимо контекстуализировать distilgpt2 среди современных аналогов. С момента релиза модели прошло несколько лет, и ландшафт SLM (Small Language Models) кардинально изменился.

### **7.1. DistilGPT-2 против TinyLlama и Phi-2**

Современные модели, такие как **Phi-2** (Microsoft) и **TinyLlama** (проект с открытым кодом), демонстрируют квантовый скачок в производительности при сопоставимых или чуть больших размерах.22

| Характеристика              | DistilGPT-2                | TinyLlama-1.1B      | Phi-2                                   |
| :-------------------------- | :------------------------- | :------------------ | :-------------------------------------- |
| **Параметры**               | 82 Миллиона                | 1.1 Миллиарда       | 2.7 Миллиарда                           |
| **Архитектура**             | GPT-2 (2019)               | Llama-2 (2023)      | Transformer++ (2023)                    |
| **Обучающие данные**        | OpenWebText (Web scrape)   | 3 Триллиона токенов | Textbooks (синтетика высокого качества) |
| **Рассуждения (Reasoning)** | Низкое (Text completion)   | Среднее             | Высокое (Math/Coding)                   |
| **Требования к VRAM**       | \< 500 MB                  | \~2.5 GB            | \~6 GB                                  |
| **Инференс**                | Экстремально быстрый (CPU) | Быстрый (CPU/GPU)   | Умеренный (GPU)                         |

**Инсайт второго порядка:** Разрыв в качестве между distilgpt2 и Phi-2 объясняется не только размером, но и качеством данных. Phi-2 была обучена на «учебниках» (Textbook quality data) — тщательно отобранных и синтетических данных, направленных на развитие логики.22 distilgpt2 же обучалась на сыром интернете. Это подтверждает гипотезу «Textbooks Are All You Need»: качество данных важнее архитектурных трюков.

Однако distilgpt2 сохраняет уникальную нишу: **суб-100М параметры**. Это позволяет запускать модель в средах с жесткими ограничениями памяти (например, старые IoT устройства, микроконтроллеры с поддержкой Linux, или браузерный инференс через WebAssembly), где даже 1 ГБ RAM, требуемый для TinyLlama, является недостижимой роскошью.12

### **7.2. Экологический аспект и углеродный след**

Важным аспектом, отмеченным в документации модели 3, является ее экологичность. Углеродный след обучения distilgpt2 составил всего около 149 кг CO2-эквивалента. Для сравнения, обучение GPT-3 или Llama-2-70B генерирует сотни тонн CO2. В эпоху, когда «Green AI» становится трендом, использование дистиллированных моделей для простых задач (например, автодополнение текста в IDE) является ответственным инженерным выбором.

---

## **8\. Аппаратные требования и оптимизация**

Заключительный аспект анализа касается практического развертывания («железа»). Эффективность distilgpt2 позволяет использовать крайне скромные конфигурации.

### **8.1. Расчет потребления памяти (VRAM/RAM)**

Объем памяти для хранения весов модели рассчитывается по формуле:

$$\\text{Memory} \\approx \\text{Params} \\times \\text{Size of Datatype}$$

- **FP32 (Full Precision):** 4 байта на параметр. $82 \\times 10^6 \\times 4 \\approx 328 \\text{ MB}$.
- **FP16 (Half Precision):** 2 байта на параметр. $82 \\times 10^6 \\times 2 \\approx 164 \\text{ MB}$.25

Это означает, что модель целиком помещается в кэш L3 современных серверных процессоров или занимает ничтожную долю оперативной памяти бюджетного смартфона. Для сравнения, полная модель GPT-2 XL (1.5B) требует более 6 ГБ VRAM только для загрузки весов 26, что делает ее непригодной для большинства потребительских GPU без квантования.

### **8.2. Инференс на CPU и оптимизации**

Благодаря малому размеру, distilgpt2 не упирается в пропускную способность памяти (Memory Bandwidth Bound) так сильно, как большие модели. Это позволяет эффективно использовать векторизацию (AVX-512) на современных CPU. Исследования показывают, что дистиллированные модели на CPU могут быть на 50% быстрее и в 2 раза эффективнее по памяти, чем их учителя.7 Использование флага torch_dtype=torch.float16 при инициализации пайплайна может дать дополнительный прирост скорости на поддерживаемом оборудовании, хотя на чистых CPU выигрыш может быть неочевиден из\-за особенностей обработки плавающей запятой.28

---

## **9\. Заключение**

Выполнение квеста «Призыв Духа-Сказителя» с использованием distilgpt2 — это больше, чем просто упражнение в кодинге. Это погружение в фундаментальную архитектуру современного ИИ.

Проведенный анализ позволяет сделать следующие ключевые выводы:

1. **Архитектурная эффективность:** distilgpt2 доказывает, что 6 слоев трансформера достаточно для моделирования базовой структуры языка, благодаря эффективному переносу знаний от учителя через функцию потерь дистилляции.
2. **Инструментарий:** Библиотека transformers и абстракция pipeline предоставляют мощный, но требующий тонкой настройки инструмент. Понимание параметров сэмплирования (temperature, top_p) является обязательным условием для получения качественного результата.
3. **Ограничения:** Модель является продуктом своей эпохи (2019) и проигрывает современным SLM (Phi-2) в задачах логики и рассуждений. Также она демонстрирует значительную неэффективность при работе с кириллицей из\-за особенностей BPE-токенизации.
4. **Ниша:** Несмотря на возраст, distilgpt2 остается «золотым стандартом» для задач сверхлегкой генерации текста на CPU, где задержка (latency) и потребление памяти являются критическими факторами.

Таким образом, «Дух-Сказитель», призванный в рамках квеста, хоть и не обладает мудростью современных оракулов вроде GPT-4, является быстрым, легким и доступным инструментом, идеально подходящим для изучения основ генеративного искусственного интеллекта.

#### **Источники**

1. GPT-2 \- Wikipedia, дата последнего обращения: ноября 24, 2025, [https://en.wikipedia.org/wiki/GPT-2](https://en.wikipedia.org/wiki/GPT-2)
2. Better language models and their implications \- OpenAI, дата последнего обращения: ноября 24, 2025, [https://openai.com/index/better-language-models/](https://openai.com/index/better-language-models/)
3. raw \- Hugging Face, дата последнего обращения: ноября 24, 2025, [https://huggingface.co/distilbert/distilgpt2/raw/main/README.md](https://huggingface.co/distilbert/distilgpt2/raw/main/README.md)
4. distilbert/distilgpt2 \- Hugging Face, дата последнего обращения: ноября 24, 2025, [https://huggingface.co/distilbert/distilgpt2](https://huggingface.co/distilbert/distilgpt2)
5. GPT-2 \- Hugging Face, дата последнего обращения: ноября 24, 2025, [https://huggingface.co/docs/transformers/en/model_doc/gpt2](https://huggingface.co/docs/transformers/en/model_doc/gpt2)
6. Everything You Need to Know about Knowledge Distillation \- Hugging Face, дата последнего обращения: ноября 24, 2025, [https://huggingface.co/blog/Kseniase/kd](https://huggingface.co/blog/Kseniase/kd)
7. \[N\] Test a Distilled GPT-2's generative capabilities : r/MachineLearning \- Reddit, дата последнего обращения: ноября 24, 2025, [https://www.reddit.com/r/MachineLearning/comments/df55ij/n_test_a_distilled_gpt2s_generative_capabilities/](https://www.reddit.com/r/MachineLearning/comments/df55ij/n_test_a_distilled_gpt2s_generative_capabilities/)
8. karpathy/minbpe: Minimal, clean code for the Byte Pair Encoding (BPE) algorithm commonly used in LLM tokenization. \- GitHub, дата последнего обращения: ноября 24, 2025, [https://github.com/karpathy/minbpe](https://github.com/karpathy/minbpe)
9. openai-community/gpt2-large \- Hugging Face, дата последнего обращения: ноября 24, 2025, [https://huggingface.co/openai-community/gpt2-large](https://huggingface.co/openai-community/gpt2-large)
10. \[D\] Why are Byte Pair Encoding tokenizers preferred over character level ones in LLMs?, дата последнего обращения: ноября 24, 2025, [https://www.reddit.com/r/MachineLearning/comments/1ax6xuh/d_why_are_byte_pair_encoding_tokenizers_preferred/](https://www.reddit.com/r/MachineLearning/comments/1ax6xuh/d_why_are_byte_pair_encoding_tokenizers_preferred/)
11. Pipelines \- Hugging Face, дата последнего обращения: ноября 24, 2025, [https://huggingface.co/docs/transformers/v4.32.0/main_classes/pipelines](https://huggingface.co/docs/transformers/v4.32.0/main_classes/pipelines)
12. distilbert/distilgpt2 Free Chat Online \- Skywork.ai, дата последнего обращения: ноября 24, 2025, [https://skywork.ai/blog/models/distilbert-distilgpt2-free-chat-online-skywork-ai/](https://skywork.ai/blog/models/distilbert-distilgpt2-free-chat-online-skywork-ai/)
13. Xirider/finetune-gpt2xl: Guide: Finetune GPT2-XL (1.5 Billion Parameters) and finetune GPT-NEO (2.7 B) on a single GPU with Huggingface Transformers using DeepSpeed \- GitHub, дата последнего обращения: ноября 24, 2025, [https://github.com/Xirider/finetune-gpt2xl](https://github.com/Xirider/finetune-gpt2xl)
14. Generation \- Hugging Face, дата последнего обращения: ноября 24, 2025, [https://huggingface.co/docs/transformers/main_classes/text_generation](https://huggingface.co/docs/transformers/main_classes/text_generation)
15. Text generation \- Hugging Face, дата последнего обращения: ноября 24, 2025, [https://huggingface.co/docs/transformers/llm_tutorial](https://huggingface.co/docs/transformers/llm_tutorial)
16. Token size in Russian lang \- API \- OpenAI Developer Community, дата последнего обращения: ноября 24, 2025, [https://community.openai.com/t/token-size-in-russian-lang/30767](https://community.openai.com/t/token-size-in-russian-lang/30767)
17. Tokenization efficiency of current foundational large language models for the Ukrainian language \- PMC \- NIH, дата последнего обращения: ноября 24, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC12380774/](https://pmc.ncbi.nlm.nih.gov/articles/PMC12380774/)
18. LLM tokens and foreign languages \- Ivan Krivyakov, дата последнего обращения: ноября 24, 2025, [https://ikriv.com/blog/?p=5322](https://ikriv.com/blog/?p=5322)
19. What are OpenAI GPT tokens?, дата последнего обращения: ноября 24, 2025, [https://gptforwork.com/guides/openai-gpt-tokens](https://gptforwork.com/guides/openai-gpt-tokens)
20. Fine-tune a non-English GPT-2 Model with Huggingface \- Philschmid, дата последнего обращения: ноября 24, 2025, [https://www.philschmid.de/fine-tune-a-non-english-gpt-2-model-with-huggingface](https://www.philschmid.de/fine-tune-a-non-english-gpt-2-model-with-huggingface)
21. Training GPT-2 on a Russian language corpus \- l4rz.net, дата последнего обращения: ноября 24, 2025, [https://l4rz.net/gpt-2-training/](https://l4rz.net/gpt-2-training/)
22. Small Language Models (SLMs) Can Still Pack a Punch: A survey \- arXiv, дата последнего обращения: ноября 24, 2025, [https://arxiv.org/html/2501.05465v1](https://arxiv.org/html/2501.05465v1)
23. Phi-2 took less A100 hours than TinyLlama to train : r/LocalLLaMA \- Reddit, дата последнего обращения: ноября 24, 2025, [https://www.reddit.com/r/LocalLLaMA/comments/1bxngcu/phi2_took_less_a100_hours_than_tinyllama_to_train/](https://www.reddit.com/r/LocalLLaMA/comments/1bxngcu/phi2_took_less_a100_hours_than_tinyllama_to_train/)
24. What's your primary local LLM at the end of 2024? : r/LocalLLaMA \- Reddit, дата последнего обращения: ноября 24, 2025, [https://www.reddit.com/r/LocalLLaMA/comments/1hqak1f/whats_your_primary_local_llm_at_the_end_of_2024/](https://www.reddit.com/r/LocalLLaMA/comments/1hqak1f/whats_your_primary_local_llm_at_the_end_of_2024/)
25. openai-community/gpt2-large · \[AUTOMATED\] Model Memory Requirements \- Hugging Face, дата последнего обращения: ноября 24, 2025, [https://huggingface.co/openai-community/gpt2-large/discussions/9](https://huggingface.co/openai-community/gpt2-large/discussions/9)
26. GPT-2 Model Size: Overview & Comparison 2025 \- BytePlus, дата последнего обращения: ноября 24, 2025, [https://www.byteplus.com/en/topic/499120](https://www.byteplus.com/en/topic/499120)
27. LLaMA Now Goes Faster on CPUs \- Justine Tunney, дата последнего обращения: ноября 24, 2025, [https://justine.lol/matmul/](https://justine.lol/matmul/)
28. GPT-2: (Hardware) requirements for fine-tuning the 774M model \[closed\], дата последнего обращения: ноября 24, 2025, [https://ai.stackexchange.com/questions/20591/gpt-2-hardware-requirements-for-fine-tuning-the-774m-model](https://ai.stackexchange.com/questions/20591/gpt-2-hardware-requirements-for-fine-tuning-the-774m-model)
29. float16 vs float32 for convolutional neural networks \- Stack Overflow, дата последнего обращения: ноября 24, 2025, [https://stackoverflow.com/questions/46613748/float16-vs-float32-for-convolutional-neural-networks](https://stackoverflow.com/questions/46613748/float16-vs-float32-for-convolutional-neural-networks)
