# **Технический отчет: Механизмы интеграции и генерации персонализированных концептов (Личная Печать) с использованием Low-Rank Adaptation (LoRA) в диффузионных моделях**

## **1\. Исполнительное резюме**

В современном ландшафте генеративного искусственного интеллекта наблюдается фундаментальный сдвиг от моделей общего назначения к узкоспециализированным, персонализированным системам. Задача "Квест 7.3: Генерация с личной Печатью" кристаллизует ключевую потребность индустрии: способность внедрять уникальные идентификаторы — будь то логотипы, специфические стилистические паттерны, изображения конкретных персонажей или объектов (далее "Печать") — в предварительно обученные модели без необходимости проведения ресурсоемкого полного переобучения. Использование Low-Rank Adaptation (LoRA) стало стандартом де\-факто для решения этой задачи благодаря уникальному балансу между эффективностью использования параметров и качеством генерации.  
Настоящий отчет представляет собой исчерпывающее техническое исследование механизмов работы LoRA в контексте латентных диффузионных моделей (LDM), таких как Stable Diffusion. В документе детально анализируются теоретические основы низкоранговой декомпозиции матриц весов, специфика взаимодействия адаптеров с слоями перекрестного внимания (cross-attention) архитектуры U-Net, а также инженерные аспекты внедрения, включая управление триггерными словами, оптимизацию точности вычислений и архитектуры высоконагруженного сервингa (S-LoRA, LoRAX).  
Анализ показывает, что LoRA позволяет сократить количество обучаемых параметров на несколько порядков (до 0,01-2% от общего числа), обеспечивая при этом возможность точного воспроизведения "Личной Печати". Однако эффективная эксплуатация этой технологии требует глубокого понимания механики активации весов через текстовые токены, нюансов слияния (fusion) адаптеров для оптимизации инференса и применения специализированных CUDA-ядер для масштабируемого обслуживания множества пользователей. Отчет синтезирует данные из более чем 200 технических источников, предоставляя дорожную карту для построения надежных систем персонализированной генерации.

## **2\. Архитектурные основы и проблема персонализации**

### **2.1 Дилемма размерности в фундаментальных моделях**

Обучение современных генеративных моделей, таких как Stable Diffusion XL (SDXL) или Flux, требует колоссальных вычислительных ресурсов и огромных объемов данных. Эти модели, обладающие миллиардами параметров, формируют обобщенное представление о визуальном мире. Однако, когда возникает задача внедрения "Личной Печати" — уникального концепта, отсутствующего в обучающей выборке (например, специфический фамильный герб или уникальный художественный стиль пользователя) — разработчики сталкиваются с дилеммой.  
Традиционное полное переобучение (full fine-tuning) модели для каждого нового концепта непрактично по двум причинам:

1. **Вычислительная и экономическая стоимость:** Обновление всех весов модели \\Phi требует значительных мощностей GPU и времени, что делает невозможным масштабирование сервиса на тысячи пользователей.
2. **Катастрофическое забывание (Catastrophic Forgetting):** При интенсивном обучении на небольшом наборе данных "Личной Печати" модель склонна терять свои обобщающие способности, деградируя в качестве генерации других объектов.

### **2.2 Гипотеза внутренней размерности**

Решение проблемы лежит в плоскости гипотезы о низкой внутренней размерности (intrinsic dimensionality) изменений, необходимых для адаптации модели. Исследования показывают, что, несмотря на то что нейронные сети параметризуются матрицами огромной размерности, обучение конкретной задаче фактически происходит на многообразии значительно меньшей размерности. Это наблюдение стало фундаментом для методов эффективной настройки параметров (PEFT \- Parameter-Efficient Fine-Tuning).  
В контексте "Личной Печати" это означает, что для того чтобы научить модель рисовать конкретный логотип, не нужно менять все миллиарды синаптических связей. Достаточно внести изменения в узкий спектр направлений в пространстве весов, которые отвечают за интерпретацию семантических признаков этого логотипа.

### **2.3 Сравнительный анализ методов адаптации**

Для обоснования выбора LoRA как оптимального инструмента для "Квеста 7.3", необходимо сравнить его с альтернативными подходами, применяемыми в экосистеме Stable Diffusion.

| Метод                 | Механизм действия                                 | Область воздействия                  | Размер артефакта | Качество "Печати"            | Гибкость композиции           |
| :-------------------- | :------------------------------------------------ | :----------------------------------- | :--------------- | :--------------------------- | :---------------------------- |
| **LoRA**              | Внедрение низкоранговых матриц \\Delta W \= BA    | Слои внимания (Cross/Self-Attention) | 2 МБ – 200 МБ    | Высокое (структура \+ стиль) | Высокая (смешивание весов)    |
| **Dreambooth**        | Полное обновление весов (Full Fine-tuning)        | Все слои U-Net (иногда Text Encoder) | 2 ГБ – 7 ГБ      | Очень высокое                | Низкая (сложно комбинировать) |
| **Textual Inversion** | Оптимизация векторного представления токена v\_\* | Эмбеддинги Text Encoder              | 4 КБ – 100 КБ    | Среднее (концептуальное)     | Средняя                       |
| **Hypernetworks**     | Вспомогательная сеть, генерирующая веса           | Слои внимания                        | 50 МБ – 300 МБ   | Среднее                      | Средняя                       |

**Анализ:**

- **Textual Inversion:** Создает "слово" для печати, но не учит U-Net новым визуальным паттернам. Если печать содержит уникальную геометрию, Textual Inversion может не справиться, пытаясь собрать её из существующих понятий модели.
- **Dreambooth:** Дает наилучшее качество, но требует сохранения полной копии модели для каждого пользователя. Это создает неприемлемые требования к хранилищу (storage bound) при масштабировании.
- **LoRA:** Занимает уникальную нишу. Изменяя механизм внимания (attention), LoRA позволяет модели "видеть" новые структуры, при этом размер файла остается управляемым, а вычислительные затраты на обучение и инференс — минимальными (при условии слияния весов).

## **3\. Математический аппарат Low-Rank Adaptation (LoRA)**

### **3.1 Линейная алгебра адаптации**

Суть LoRA заключается в заморозке предварительно обученных весов модели W_0 \\in \\mathbb{R}^{d \\times k} и добавлении к ним обучаемого ответвления. Вместо того чтобы оптимизировать матрицу изменений \\Delta W той же размерности, что и W_0, LoRA представляет \\Delta W как произведение двух матриц низкого ранга: B \\in \\mathbb{R}^{d \\times r} и A \\in \\mathbb{R}^{r \\times k}, где ранг r \\ll \\min(d, k).  
Уравнение прямого прохода (forward pass) для модифицированного слоя выглядит следующим образом:  
Здесь x — входной вектор, h — выходной вектор. Это уравнение демонстрирует, что вычисление адаптации BAx происходит параллельно с основным вычислением W_0x, а результаты суммируются.

### **3.2 Стратегия инициализации и стабильность обучения**

Критически важным аспектом для успешной генерации "Личной Печати" является начальное состояние LoRA.

- **Матрица A:** Инициализируется случайными значениями из гауссовского распределения. Это необходимо для нарушения симметрии и начала градиентного спуска.
- **Матрица B:** Инициализируется нулями.
- **Следствие:** В начале обучения \\Delta W \= B \\cdot A \= 0 \\cdot A \= 0\.

Это свойство гарантирует, что на старте процесса модель ведет себя абсолютно идентично исходной базовой модели. Это предотвращает "шок" весов и позволяет плавно внедрять новый концепт, минимизируя риск расхождения градиентов или разрушения уже выученных паттернов. Существуют и альтернативные методы инициализации, например, **PiSSA** (использует сингулярное разложение SVD для инициализации главных компонент) или **OLoRA** (ортогональная инициализация), которые могут ускорить сходимость, но стандартная схема (Gaussian/Zero) остается наиболее распространенной в экосистеме Diffusers.

### **3.3 Выбор ранга и масштабирование (Scaling)**

Ранг r является ключевым гиперпараметром.

- **Малый ранг (r=4-16):** Подходит для захвата общего стиля или простых объектов. Обеспечивает максимальную эффективность сжатия.
- **Высокий ранг (r=32-128):** Необходим для сложных "Печатей" с мелкими деталями, текстом или сложной геометрией. Увеличение ранга повышает выразительную способность адаптера, но также увеличивает объем памяти и риск переобучения (overfitting).

Влияние LoRA регулируется скалярным множителем \\alpha (alpha). Итоговая формула обновления весов часто записывается как:  
Это масштабирование позволяет изменять ранг r без необходимости переподбора гиперпараметров скорости обучения (learning rate).

## **4\. Взаимодействие с архитектурой U-Net и механизмом внимания**

### **4.1 Анатомия Stable Diffusion**

В основе Stable Diffusion лежит архитектура U-Net, которая выполняет итеративное удаление шума из латентного представления изображения. Процесс генерации управляется текстовыми подсказками (промптами) через механизм **перекрестного внимания (Cross-Attention)**. Именно здесь происходит магия появления "Личной Печати".  
Механизм внимания определяется формулой:  
Где:

- **Q (Query):** Проекция визуальных признаков (латентного изображения) из предыдущего слоя U-Net.
- **K (Key):** Проекция текстовых эмбеддингов (описания "Печати").
- **V (Value):** Проекция текстовых эмбеддингов (семантического содержания).

### **4.2 LoRA в слоях Cross-Attention**

LoRA применяется к матрицам проекций W_Q, W_K, W_V и W_O в блоках внимания трансформеров внутри U-Net. Для генерации "Личной Печати" наиболее критичны изменения в W_K и W_V.

- **Модификация W_K:** Изменяет то, _как_ визуальные участки изображения "обращают внимание" на текстовые токены. Обученная LoRA корректирует ключи так, чтобы при появлении триггерного слова (например, "sks seal") области изображения, где должна быть печать, давали высокий отклик (attention score).
- **Модификация W_V:** Изменяет то, _какая_ информация извлекается из текстового токена. Это определяет визуальное содержание печати (цвет, форма, текстура), которое будет перенесено в латентное пространство.

### **4.3 Визуализация и интерпретация карт внимания**

Анализ карт внимания (Attention Maps) позволяет наглядно увидеть работу LoRA.

- **До LoRA:** Токен "seal" (печать) может активировать размытые области или ассоциироваться с животным (тюленем).
- **После LoRA:** Триггерный токен активирует четко очерченную область, соответствующую форме "Личной Печати". Тепловая карта внимания становится сфокусированной и интенсивной именно в тех пространственных координатах, где генерируется объект.

Исследования показывают, что LoRA эффективно "перепрошивает" семантические связи. Она не просто запоминает картинку, а создает процедурную инструкцию для U-Net: "при встрече с вектором X активируй генерацию паттерна Y". Это объясняет, почему LoRA позволяет генерировать печать в новых ракурсах и контекстах, недоступных в обучающей выборке.

## **5\. Триггерные слова: Ключи активации и семантика**

### **5.1 Механизм активации через триггеры**

Триггерное слово (Trigger Word) — это уникальная последовательность символов, которая связывается с концептом "Печати" в процессе обучения. В датасете изображениям печати присваиваются подписи вида "photo of sks seal". Поскольку токен "sks" (или другой редкий токен) уникален для данных изображений, градиентный спуск "стягивает" всю информацию о специфических чертах печати именно на веса, активируемые этим токеном.  
Без триггерного слова активация LoRA может стать неконтролируемой. Если обучать модель просто на слове "seal" (печать), LoRA "загрязнит" общее понятие печати в модели. В таком случае, любой запрос на генерацию печати будет выдавать именно вашу "Личную Печать", что является формой переобучения или "протекания концепта" (concept bleeding).

### **5.2 Стратегии выбора и использования триггеров**

Существует два основных подхода к использованию триггеров, каждый из которых имеет свои последствия для инференса:

1. **Явные уникальные токены (sks, ohwx, pelarbues):**
   - _Преимущество:_ Изоляция концепта. Печать появляется только при явном указании триггера.
   - _Механизм:_ Вектор эмбеддинга редкого токена служит "адресом" в латентном пространстве, к которому LoRA "привязывает" новые визуальные признаки.
   - _Рекомендация:_ Идеально для конкретных объектов ("Личная Печать"), лиц или логотипов.
2. **Отсутствие триггера (Style Training):**
   - _Преимущество:_ Модификация глобального стиля.
   - _Механизм:_ Если в обучающих подписях нет уникального токена, LoRA выучивает смещение (bias) для всех изображений. Матрицы BA становятся ненулевыми константами, которые добавляются ко всем проекциям, сдвигая всё распределение генерации в сторону стиля печати (например, "гравюра").
   - _Риск:_ Невозможность отключить эффект без отключения LoRA.

### **5.3 Автоматизация управления метаданными**

В сложных пайплайнах (например, ComfyUI или Automatic1111) запоминание триггеров для сотен LoRA становится проблемой. Современные инструменты используют метаданные, зашитые в файл .safetensors, для автоматического извлечения триггерных слов. Расширения типа "Civitai Helper" или "Power Lora Loader" сканируют хеш модели, обращаются к базе данных (Civitai API) и автоматически добавляют нужный триггер в промпт при выборе модели. Это критически важно для пользовательского опыта (UX) в задаче "Квест 7.3", обеспечивая гарантированную активацию "Печати".

## **6\. Инженерная реализация: Загрузка, Инференс и Слияние**

Реализация генерации с "Личной Печатью" на базе библиотеки diffusers требует навигации между различными методами загрузки весов, каждый из которых имеет свои последствия для производительности и совместимости.

### **6.1 Парадигмы загрузки весов**

Существует два основных метода интеграции LoRA в пайплайн: использование нативных методов diffusers и оберток библиотеки PEFT.

#### **6.1.1 Метод load_lora_weights (Native Diffusers)**

Это наиболее предпочтительный метод для инференса. Он загружает веса непосредственно в существующие модули U-Net и Text Encoder.  
`from diffusers import DiffusionPipeline`  
`import torch`

`pipeline = DiffusionPipeline.from_pretrained(`  
 `"stabilityai/stable-diffusion-xl-base-1.0",`  
 `torch_dtype=torch.float16,`  
 `variant="fp16",`  
 `use_safetensors=True`  
`).to("cuda")`

`# Загрузка LoRA Личной Печати`  
`pipeline.load_lora_weights(`  
 `"./models/lora/personal_seal",`  
 `weight_name="seal_v1.safetensors",`  
 `adapter_name="seal_adapter"`  
`)`

**Преимущества:**

- **Совместимость:** Сохраняет структуру UNet2DConditionModel, что обеспечивает корректную работу таких функций, как enable_model_cpu_offload() (выгрузка модели на CPU для экономии VRAM).
- **Мульти-адаптеры:** Позволяет загружать несколько LoRA одновременно (например, "Печать" \+ "Стиль бумаги") и управлять их весами через pipeline.set_adapters(\["seal", "paper"\], adapter_weights=\[0.8, 0.5\]).
- **Динамическое управление:** Возможность активировать и деактивировать адаптеры "на лету" без перезагрузки базовой модели.

#### **6.1.2 Метод PeftModel.from_pretrained (HF PEFT)**

Этот метод оборачивает базовую модель в класс PeftModel.  
`from peft import PeftModel`  
`from diffusers import UNet2DConditionModel`

`base_unet = UNet2DConditionModel.from_pretrained(...)`  
`peft_unet = PeftModel.from_pretrained(base_unet, "./models/lora/personal_seal")`  
`pipeline = DiffusionPipeline.from_pretrained(..., unet=peft_unet)`

**Недостатки и риски:**

- **Конфликты выгрузки (Offloading):** Обертка PeftModel может скрывать внутренние атрибуты U-Net от пайплайна diffusers. Это часто приводит к ошибкам при использовании enable_model_cpu_offload() или enable_sequential_cpu_offload(), так как хуки (hooks) библиотеки accelerate не могут корректно идентифицировать подмодули для перемещения между CPU и GPU. Ошибки вида "Expected all tensors to be on the same device" характерны для этого сценария.
- **Проблемы с сохранением:** Сохранение пайплайна с обернутой моделью через save_pretrained может создавать нестандартную структуру директорий, усложняя последующую загрузку.

### **6.2 Слияние весов (Weight Fusion)**

Для оптимизации скорости инференса применяется техника слияния. Метод fuse_lora() математически складывает матрицы LoRA с весами базовой модели: W\_{fused} \= W_0 \+ \\text{scale} \\cdot (B \\times A).  
**Преимущества слияния:**

- **Скорость:** Устраняет необходимость выполнения отдельных матричных умножений для B и A во время прямого прохода. Модель работает так же быстро, как и оригинальная, без накладных расходов.
- **Память:** Освобождает память, занятую отдельными тензорами адаптеров.

**Недостатки слияния:**

- **Потеря гибкости:** После слияния невозможно быстро изменить вес LoRA или отключить её. Для этого требуется выполнить обратную операцию unfuse_lora(), которая вычитает веса. Если было слито несколько LoRA, корректное "расцепление" (unfusing) может быть математически неточным или невозможным без полной перезагрузки базовой модели.
- **Проблемы квантования:** При слиянии LoRA (обычно FP16/FP32) с квантованной базовой моделью (например, Int8 или 4-bit через bitsandbytes) могут возникать значительные потери качества. Веса LoRA могут быть "выбросами" (outliers), которые плохо квантуются, приводя к деградации изображения. Рекомендуется сливать веса в высоком разрешении (FP16/BF16) перед квантованием.

### **6.3 Оптимизация точности и памяти**

При работе с LoRA важно учитывать форматы точности данных:

- **FP16 (Float16):** Стандарт де\-факто для инференса. Снижает потребление VRAM вдвое по сравнению с FP32. Однако на некоторых GPU (старые серии GTX, некоторые серверные карты) или при определенных операциях в VAE (Variational Autoencoder) FP16 может приводить к переполнению (Overflow) и появлению значений NaN (черные квадраты вместо изображений). Решение — принудительный перевод VAE в FP32 (pipeline.vae.to(dtype=torch.float32)) при сохранении U-Net в FP16.
- **BF16 (Bfloat16):** Предпочтительный формат для обучения и инференса на новых GPU (Ampere и новее). Обладает динамическим диапазоном FP32 при размере FP16, что исключает проблемы с переполнением.

**Таблица 1: Сравнение методов управления памятью**

| Метод                  | Описание                                                   | Совместимость с LoRA                          | Влияние на скорость              |
| :--------------------- | :--------------------------------------------------------- | :-------------------------------------------- | :------------------------------- |
| **Model CPU Offload**  | Выгружает неиспользуемые модули (Text Encoder, VAE) на CPU | Хорошая (при использовании load_lora_weights) | Низкое (передача данных по PCIe) |
| **Sequential Offload** | Выгружает слои U-Net послойно                              | Часто ломается с PeftModel                    | Высокое (сильное замедление)     |
| **VAE Slicing/Tiling** | Обрабатывает изображение частями                           | Полная                                        | Среднее                          |

## **7\. Масштабируемое развертывание: S-LoRA и LoRAX**

Если задача "Квеста" перерастает в создание сервиса, где тысячи пользователей имеют свои "Личные Печати", стандартные методы diffusers становятся узким местом. Загрузка новой модели для каждого запроса невозможна из\-за задержек и ограничений памяти.

### **7.1 LoRAX (LoRA Exchange)**

LoRAX — это фреймворк для инференса, построенный поверх Text Generation Inference (TGI), специально разработанный для мульти-арендной (multi-tenant) архитектуры.  
**Ключевые особенности:**

- **Динамическая загрузка адаптеров:** LoRAX позволяет указывать адаптер в каждом запросе. Система подгружает веса адаптера из хранилища (S3, локальный диск) асинхронно, не блокируя основной поток инференса.
- **Единая базовая модель:** В памяти GPU хранится только одна копия "тяжелой" базовой модели (Base Model). "Легкие" адаптеры (LoRA) подменяются на лету. Это позволяет обслуживать сотни различных "Печатей" на одном GPU.
- **Планировщик обмена (Exchange Scheduler):** Интеллектуально управляет кэшем адаптеров в VRAM, вытесняя редко используемые и предзагружая популярные.

### **7.2 S-LoRA: Глубокая оптимизация ядра**

S-LoRA (Scalable LoRA) предлагает решение на уровне CUDA-ядер для эффективной пакетной обработки (batching) запросов с _разными_ адаптерами.  
**Проблема:** В стандартном батчинге (GEMM) все примеры в пакете умножаются на одну и ту же матрицу весов. Но если пользователь A просит "Печать А", а пользователь B — "Печать B", матрицы весов \\Delta W для них разные.  
**Решение S-LoRA:**

1. **Unified Paging (Единая подкачка):** Использует пул памяти для хранения весов адаптеров и KV-кэша. Это решает проблему фрагментации памяти, возникающую из\-за того, что разные LoRA могут иметь разные ранги (и, следовательно, разные размеры матриц). Память выделяется блоками (pages), аналогично управлению виртуальной памятью в ОС.
2. **Гетерогенный батчинг (Heterogeneous Batching):** S-LoRA реализует кастомные CUDA-ядра (Gathered GEMM). Эти ядра способны выполнять матричное умножение для пакета данных, где каждый элемент пакета использует свои уникальные матрицы A и B, "собирая" их из несмежных областей памяти (Unified Paging). Это позволяет обрабатывать запросы с разными печатями в одном проходе GPU, повышая пропускную способность до 4 раз по сравнению с наивными реализациями.

**Таблица 2: Сравнение архитектур сервингa**

| Характеристика         | Стандартный Diffusers                 | LoRAX                                    | S-LoRA                                 |
| :--------------------- | :------------------------------------ | :--------------------------------------- | :------------------------------------- |
| **Загрузка адаптеров** | Последовательная, блокирующая         | Асинхронная, динамическая                | Предзагрузка в пул памяти              |
| **Батчинг**            | Гомогенный (только одна LoRA на батч) | Гетерогенный (через управление очередью) | **Гетерогенный (на уровне CUDA ядер)** |
| **Управление памятью** | Python GC / PyTorch Allocator         | LRU Cache                                | **Unified Paging** (дефрагментация)    |
| **Масштабируемость**   | 1 пользователь / 1 модель             | 100+ адаптеров / 1 GPU                   | **1000+ адаптеров / 1 GPU**            |

## **8\. Композиция и Конфликты**

"Квест" может потребовать не только генерации печати, но и стилизации изображения под старинную бумагу. Это подразумевает использование нескольких LoRA одновременно.

### **8.1 Смешивание адаптеров**

Метод set_adapters позволяет активировать несколько загруженных LoRA и задать им веса:  
`pipeline.set_adapters(["seal", "style"], adapter_weights=[1.0, 0.6])`

Здесь адаптер печати имеет полный вес, а стиль — ослабленный. Математически это означает:

### **8.2 Конфликты проекций**

Если две LoRA сильно изменяют одни и те же веса (например, обе агрессивно модифицируют W_K в слоях, отвечающих за геометрию), может возникнуть "коллапс" изображения или артефакты. В таких случаях помогает уменьшение весов или использование техники **LoRA Block Weighting**, где влияние адаптеров настраивается послойно (например, стиль применяется к начальным слоям U-Net, а печать — к средним и конечным).

## **9\. Заключение**

Выполнение "Квеста 7.3: Генерация с личной Печатью" с использованием LoRA — это многогранная задача, выходящая далеко за рамки простого вызова API. Она опирается на глубокие теоретические принципы низкоранговой аппроксимации матриц, позволяющей хирургически точно вмешиваться в когнитивные процессы диффузионной модели.  
Ключевыми факторами успеха являются:

1. **Понимание механизма Cross-Attention:** Осознание того, как LoRA перенаправляет внимание модели с общих концептов на специфические триггеры.
2. **Грамотный инжиниринг:** Использование нативных методов загрузки diffusers для обеспечения совместимости с оптимизациями памяти и отказ от оберток PeftModel в продакшн-пайплайнах.
3. **Управление данными:** Строгая дисциплина в использовании триггерных слов для предотвращения "протекания" концептов.
4. **Масштабируемая архитектура:** При переходе к массовому обслуживанию — внедрение систем класса S-LoRA, использующих специализированные ядра и управление памятью для эффективного мульти-арендного инференса.

Технология LoRA демократизировала процесс тонкой настройки ИИ, превратив возможность создания персонализированных моделей из привилегии исследовательских лабораторий в инструмент, доступный каждому разработчику. Эффективная реализация "Личной Печати" — это квинтэссенция этой технологической революции.

#### **Источники**

1\. Understanding LoRA for Efficient Stable Diffusion Fine-Tuning \- Hyperstack, https://www.hyperstack.cloud/blog/case-study/lora-for-stable-diffusion-fine-tuning-understand-why-it-s-efficient 2\. Using LoRA for Efficient Stable Diffusion Fine-Tuning \- Hugging Face, https://huggingface.co/blog/lora 3\. Understanding Stable Diffusion Architecture and UNet | by Yash Jain \- Medium, https://medium.com/@ydhupiya1710/understanding-stable-diffusion-architecture-and-unet-4aad410929c4 4\. Mastering Low-Rank Adaptation (LoRA): Enhancing Large Language Models for Efficient Adaptation | DataCamp, https://www.datacamp.com/tutorial/mastering-low-rank-adaptation-lora-enhancing-large-language-models-for-efficient-adaptation 5\. Using LoRA in Stable Diffusion \- MachineLearningMastery.com, https://machinelearningmastery.com/using-lora-in-stable-diffusion/ 6\. Visual Style Prompting with Swapping Self-Attention \- arXiv, https://arxiv.org/html/2402.12974v2 7\. LoRA is inferior to Full Fine-Tuning / DreamBooth Training \- A research paper just published : LoRA vs Full Fine-tuning: An Illusion of Equivalence : r/StableDiffusion \- Reddit, https://www.reddit.com/r/StableDiffusion/comments/1gmwlfs/lora\_is\_inferior\_to\_full\_finetuning\_dreambooth/ 8\. What are LoRA models and how to use them in AUTOMATIC1111 \- Stable Diffusion Art, https://stable-diffusion-art.com/lora/ 9\. LoRA \- Hugging Face, https://huggingface.co/docs/diffusers/en/tutorials/using\_peft\_for\_inference 10\. Using Peft LoRA for better, simpler UNet fine-tuning? · huggingface diffusers · Discussion \#9102 \- GitHub, https://github.com/huggingface/diffusers/discussions/9102 11\. Towards Understanding Cross and Self-Attention in Stable Diffusion for Text-Guided Image Editing \- arXiv, https://arxiv.org/html/2403.03431v1 12\. Towards Understanding Cross and Self-Attention in Stable Diffusion for Text-Guided Image Editing \- CVF Open Access, https://openaccess.thecvf.com/content/CVPR2024/papers/Liu\_Towards\_Understanding\_Cross\_and\_Self-Attention\_in\_Stable\_Diffusion\_for\_Text-Guided\_CVPR\_2024\_paper.pdf 13\. This is what Stable Diffusion's attention looks like : r/StableDiffusion \- Reddit, https://www.reddit.com/r/StableDiffusion/comments/18lgmn3/this\_is\_what\_stable\_diffusions\_attention\_looks/ 14\. LoRA: A Conceptual Deep Dive — Parameter-Efficient Fine-Tuning Explained \- Medium, https://medium.com/datadreamers/exploring-lora-unveiling-parameter-efficient-tuning-and-self-attention-mechanisms-in-depth-58e4c3b5ce30 15\. Training a LoRa of your face with Stable Diffusion 1.5 \- Pelayo Arbués, https://www.pelayoarbues.com/notes/Training-a-LoRa-of-your-face-with-Stable-Diffusion-1.5 16\. LoRA Advanced Techniques: Trigger Words, Multi-Character Generation, and Collection Management \- PixAI, https://pixai.art/articles/en/lora-advanced-techniques-trigger-words-multi-character-generation-and-collection-management/ 17\. Noobie Question: Trigger word necessary if you click the LORA? : r/StableDiffusion \- Reddit, https://www.reddit.com/r/StableDiffusion/comments/17qu8mi/noobie\_question\_trigger\_word\_necessary\_if\_you/ 18\. No trigger word, but lora influencing all prompts, I am baffled , Some theoretical insight please : r/StableDiffusion \- Reddit, https://www.reddit.com/r/StableDiffusion/comments/15wnfut/no\_trigger\_word\_but\_lora\_influencing\_all\_prompts/ 19\. How to set trigger word when trying to train a LoRA in OneTrainer? : r/StableDiffusion, https://www.reddit.com/r/StableDiffusion/comments/16ccp1m/how\_to\_set\_trigger\_word\_when\_trying\_to\_train\_a/ 20\. LoRA for persons: using trigger words vs not using trigger words? : r/StableDiffusion \- Reddit, https://www.reddit.com/r/StableDiffusion/comments/15xkuxa/lora\_for\_persons\_using\_trigger\_words\_vs\_not\_using/ 21\. LORA trigger words? : r/StableDiffusion \- Reddit, https://www.reddit.com/r/StableDiffusion/comments/1fnjibr/lora\_trigger\_words/ 22\. Revolutionize Your AI Art Workflow with ComfyUI-Lora-Auto-Trigger-Words \- AiToolGo, https://www.aitoolgo.com/learning/detail/comfyui-extension-comfyui-lora-auto-trigger-words 23\. Reduce memory usage \- Hugging Face, https://huggingface.co/docs/diffusers/en/optimization/memory 24\. Merge LoRAs \- Hugging Face, https://huggingface.co/docs/diffusers/main/en/using-diffusers/merge\_loras 25\. pipeline fail to move to "cuda" if one of the component is PeftModel · Issue \#10403 \- GitHub, https://github.com/huggingface/diffusers/issues/10403 26\. Pipelines fail if enable_model_cpu_offload is called twice · Issue \#2907 · huggingface/diffusers \- GitHub, https://github.com/huggingface/diffusers/issues/2907 27\. Using \`enable_model_cpu_offload\` with multiple XL pipelines triggers error Expected all tensors to be on the same device · Issue \#5281 · huggingface/diffusers \- GitHub, https://github.com/huggingface/diffusers/issues/5281 28\. Troubleshooting \- Hugging Face, https://huggingface.co/docs/peft/developer\_guides/troubleshooting 29\. Loading pretrained model after \`fuse_lora()\` and \`save_pretrained()\` results in an error · Issue \#6602 · huggingface/diffusers \- GitHub, https://github.com/huggingface/diffusers/issues/6602 30\. Understanding LoRA performance \- Fireworks AI Docs, https://docs.fireworks.ai/guides/understanding\_lora\_performance 31\. \`merge_and_unload\` for a quantized model ruins its quality · Issue \#31293 · huggingface/transformers \- GitHub, https://github.com/huggingface/transformers/issues/31293 32\. hakurei/waifu-diffusion-v1-3 · what is the difference between 16 32 and full? I don't know anything about this topic :/ \- Hugging Face, https://huggingface.co/hakurei/waifu-diffusion-v1-3/discussions/3 33\. FP16 vs FP32 on Nvidia CUDA: Huge Performance hit when forcing \--no-half \- Reddit, https://www.reddit.com/r/StableDiffusion/comments/11getnk/fp16\_vs\_fp32\_on\_nvidia\_cuda\_huge\_performance\_hit/ 34\. CogStudio: Difference between dtypes (bfloat16 vs float16) | Pinokio : r/StableDiffusion, https://www.reddit.com/r/StableDiffusion/comments/1g4iw23/cogstudio\_difference\_between\_dtypes\_bfloat16\_vs/ 35\. LoRAX: Multi-LoRA Inference Server — SkyPilot Docs, https://docs.skypilot.co/en/stable/examples/serving/lorax.html 36\. LoRAX Docs, https://loraexchange.ai/ 37\. predibase/lorax: Multi-LoRA inference server that scales to 1000s of fine-tuned LLMs \- GitHub, https://github.com/predibase/lorax 38\. S-LoRA/README.md at main \- GitHub, https://github.com/S-LoRA/S-LoRA/blob/main/README.md 39\. S-LoRA: Scalable Serving of LoRA Adapters \- Emergent Mind, https://www.emergentmind.com/papers/2311.03285 40\. Recipe for Serving Thousands of Concurrent LoRA Adapters \- LMSYS Org, https://lmsys.org/blog/2023-11-15-slora/ 41\. SLORA | Continuum Labs, https://training.continuumlabs.ai/inference/why-is-inference-important/slora
