# **От Симулякра к Реальности: Комплексный Анализ Стохастической Аугментации Данных в Глубоком Обучении и Компьютерном Зрении**

## **1\. Введение: Голем, Симулякр и Парадокс Восприятия**

В современной парадигме искусственного интеллекта существует фундаментальная философская и техническая дилемма, которая восходит к древним мифологическим архетипам. В еврейском фольклоре создание Голема — существа из глины, оживленного посредством сакрального текста (Шем), — представляло собой акт имитации божественного творения. Однако Голем, будучи мощным и исполнительным, традиционно изображался лишенным души, интуиции и, что наиболее важно, жизненного опыта. Он был чистым автоматом, способным выполнять приказы, но неспособным к истинному пониманию контекста окружающего мира.  
Современные глубокие нейронные сети (Deep Neural Networks, DNN) поразительно напоминают этого мифического Голема. Они обладают колоссальной вычислительной мощностью и способны аппроксимировать сложнейшие функции, однако их "знания" ограничены исключительно теми данными, которые были им "скармливали" в процессе обучения. Если Голем никогда не видел кошку под углом 45 градусов, для него она не существует. Если он обучался только на фотографиях, сделанных при дневном свете, вечерние сумерки делают его слепым. Это ограничение порождает критическую проблему: как научить искусственный разум воспринимать бесконечное разнообразие физического мира, имея в распоряжении лишь конечный, статичный набор данных (датасет)?  
Ответ кроется в концепции **Симулякра** — создании "иллюзорных копий", которые не являются реальностью, но несут в себе ее структурные свойства. В контексте машинного обучения этот процесс известен как **Аугментация Данных** (Data Augmentation). Это искусство и наука алгоритмического расширения обучающей выборки путем применения контролируемых искажений, трансформаций и пертурбаций к исходным данным.  
Данный отчет представляет собой исчерпывающее исследование методологии аугментации изображений, инициированное запросом на выполнение "Квеста 3.3: Усиление Иллюзий". Мы выйдем далеко за рамки простого выполнения скрипта quest_3_3.py и проведем глубокую деконструкцию каждого этапа: от математической природы геометрических искажений в библиотеке torchvision до экономической эффективности внедрения динамических пайплайнов в промышленные системы. Мы докажем, что аугментация — это не просто технический трюк для увеличения количества картинок, а фундаментальный механизм регуляризации, позволяющий преодолеть проклятие размерности и научить "цифрового Голема" видеть суть объектов (инвариантные признаки), а не запоминать их поверхностные текстуры (шум).

## **2\. Теоретический Фундамент: Почему Аугментация Необходима**

Чтобы понять важность "Квеста 3.3", необходимо погрузиться в математическую теорию обучения. Глубокое обучение оперирует в пространствах признаков чрезвычайно высокой размерности. Изображение размером всего лишь 256 \\times 256 пикселей с тремя цветовыми каналами (RGB) представляет собой вектор в пространстве размерностью 256 \\times 256 \\times 3 \\approx 196,608.

### **2.1 Проклятие Размерности и Ошибка Обобщения**

В таком многомерном пространстве даже "большие данные" (Big Data) являются исчезающе разреженными. Это явление известно как **Проклятие Размерности** (Curse of Dimensionality). Если мы представим пространство всех возможных изображений, то реальные изображения кошек занимают в нем ничтожно малый объем, образуя сложное многообразие (manifold).  
Модель, обладающая миллионами параметров (как современные ResNet или ViT), имеет достаточную "память", чтобы просто запомнить каждый обучающий пример. Это приводит к **переобучению** (overfitting). Переобученная модель идеально классифицирует тренировочные данные (ошибка стремится к нулю), но терпит крах на новых данных (валидационная ошибка растет). В терминах "Легенды Квеста": Голем запоминает конкретное расположение пикселей артефакта magical_cat.png, но не усваивает концепцию "кота".  
Аугментация данных искусственно увеличивает плотность выборки в окрестности обучающих примеров. Применяя, например, поворот (RandomRotation) к изображению, мы говорим модели: "Все точки в этой траектории поворота также принадлежат классу 'Кот'". Это сглаживает границы принятия решений (decision boundaries) и заставляет модель искать более робастные признаки.

### **2.2 Инвариантность против Эквивариантности в Сверточных Сетях**

Фундаментальная цель компьютерного зрения — достижение **инвариантности** (invariance). Мы хотим, чтобы классификатор выдавал метку y \= \\text{"Кот"} независимо от того, сдвинут ли кот влево, повернут ли он или освещен красным фонарем.  
Однако сверточные нейронные сети (CNN), являющиеся стандартом в этой области, по своей природе обладают свойством **эквивариантности** (equivariance) к трансляции (сдвигу), но не к повороту или масштабированию.

- _Трансляционная эквивариантность:_ Если мы сдвинем объект на входном изображении, карта признаков (feature map) внутри сети также сдвинется.
- _Отсутствие ротационной инвариантности:_ Стандартная операция свертки не знает, что повернутый на 90 градусов фильтр должен реагировать на тот же объект. Сеть воспринимает повернутый объект как совершенно новый паттерн.

Именно здесь вступает в игру аугментация. Явно предоставляя сети повернутые и искаженные версии изображения, мы заставляем процесс оптимизации (градиентный спуск) находить такие веса, которые минимизируют функцию потерь _для всех вариаций_. Мы "учим" сеть инвариантности, которой она не обладает архитектурно. Это превращает аугментацию из вспомогательной техники в необходимый компонент обучения.

### **2.3 Гипотеза Многообразия (Manifold Hypothesis)**

Согласно гипотезе многообразия, естественные данные (например, изображения лиц или котов) лежат на низкоразмерном нелинейном многообразии, вложенном в высокоразмерное пространство пикселей. Задача обучения — аппроксимировать форму этого многообразия.  
Аугментацию можно рассматривать как способ исследования локальной топологии этого многообразия.

- Небольшой поворот или изменение яркости — это движение вдоль поверхности многообразия (мы остаемся в классе "Кот").
- Слишком сильное искажение (например, добавление чрезмерного шума или поворот цифры "6" в "9") может выбросить точку за пределы многообразия или на многообразие другого класса.

Понимание этого баланса критически важно для "Техноманта": сила "магических линз" должна быть строго дозирована, чтобы искаженная копия оставалась верна сути оригинала (label-preserving transformation).

## **3\. Анатомия torchvision.transforms: Деконструкция Магического Конвейера**

В рамках "Квеста 3.3" нам предлагается использовать библиотеку torchvision, являющуюся стандартом де\-факто в экосистеме PyTorch для задач компьютерного зрения. Центральным элементом здесь выступает модуль transforms, который предоставляет строительные блоки для создания пайплайнов аугментации. Давайте проведем глубокий технический анализ каждого компонента, использованного в скрипте, опираясь на документацию и исходный код.

### **3.1 Архитектура Compose: Линейная Алгебра Трансформаций**

Конструкция T.Compose(\[...\]) является программной реализацией математической композиции функций. Если мы обозначим наши трансформации как функции f_1, f_2, \\dots, f_n, то объект transform_pipeline, созданный в скрипте, выполняет операцию:  
Важно отметить, что порядок операций имеет критическое значение.

1. **Детерминизм порядка:** В Compose трансформации применяются строго последовательно. Применение Resize _после_ поворота дало бы иной результат (и иные артефакты алиасинга), чем Resize _до_ поворота.
2. **Типизация данных:** Большинство трансформаций в torchvision (до версии v2) ожидают на входе либо объекты PIL.Image, либо torch.Tensor. Скрипт квеста работает с PIL.Image. Это важно, так как некоторые операции (например, аффинные преобразования) могут иметь разные реализации "под капотом" для PIL и тензоров, что иногда приводит к микроскопическим различиям в результатах.

Современные версии torchvision (начиная с 0.15+ и v2) поддерживают JIT-компиляцию (TorchScript), что позволяет встраивать весь конвейер аугментации непосредственно в граф модели для экспорта в продакшн (например, на мобильные устройства), но в контексте классического обучения, как в нашем квесте, это работает как Python-объект.

### **3.2 T.Resize: Проблема Дискретизации и Интерполяции**

Первая линза в конвейере — T.Resize((256, 256)).

- **Цель:** Приведение всех входных данных к единому тензорному размеру (C, H, W). Сверточные сети могут работать с произвольными размерами (будучи fully convolutional), но полносвязные слои (Fully Connected layers) в конце классификатора требуют фиксированного размера вектора признаков. Кроме того, батчинг (объединение изображений в пакеты) невозможен, если изображения имеют разный размер.
- **Скрытая сложность (Интерполяция):** Изменение размера цифрового изображения — это нетривиальная задача. При уменьшении размера мы теряем информацию, при увеличении — должны "изобретать" новые пиксели. torchvision по умолчанию использует **билинейную интерполяцию** (bilinear interpolation).
  - _Билинейная:_ Вычисляет значение нового пикселя как взвешенное среднее 4-х ближайших соседей.
  - _Бикубическая:_ Использует 16 соседей. Дает более гладкий результат, но медленнее.
  - _Ближайший сосед (Nearest Neighbor):_ Просто берет значение ближайшего пикселя. Используется для масок сегментации (чтобы не создавать несуществующие классы на границах), но для изображений дает "блочный" эффект.
- **Инсайт:** Использование Resize до аугментаций — спорное решение. Если исходное изображение огромно (4000x3000), а мы сжимаем его до 256x256, мы теряем мелкие детали _до_ того, как модель могла бы их увидеть при случайном кропе (RandomCrop). В более продвинутых пайплайнах часто сначала делают кроп, а потом ресайз, или ресайз до большего размера (напр., 280), а потом кроп до 256\.

### **3.3 T.RandomHorizontalFlip: Симметрия Реальности**

В скрипте используется T.RandomHorizontalFlip(p=1.0).

- **Параметр p:** Вероятность применения. Значение p=1.0 означает, что изображение будет отражено _всегда_.
- **Критика реализации Квеста:** В реальном обучении установка p=1.0 является ошибкой (или специфическим случаем), так как мы полностью заменяем исходное распределение зеркальным. Мы теряем информацию о том, как объекты выглядят в оригинале. Стандартная практика — p=0.5. Это гарантирует, что в пределе (на бесконечном числе эпох) модель увидит 50% левосторонних и 50% правосторонних объектов, достигая инвариантности к зеркальному отражению.
- **Семантическая валидность:** Горизонтальное отражение безопасно для большинства естественных объектов (коты, машины, деревья симметричны). Однако для текста (OCR) или дорожных знаков (поворот налево vs направо) эта аугментация разрушительна, так как меняет смысл класса. Для "Магического кота" это допустимо и полезно.

### **3.4 T.RandomRotation: Аффинные Преобразования и Артефакты Границ**

Третья линза — T.RandomRotation(degrees=45).

- **Механика:** Поворот изображения на угол \\theta \\in \[-45^{\\circ}, \+45^{\\circ}\]. Это аффинное преобразование, которое требует пересчета координат каждого пикселя (x, y) в (x', y') с использованием матрицы поворота: $$ \\begin{bmatrix} x' \\ y' \\end{bmatrix} \= \\begin{bmatrix} \\cos\\theta & \-\\sin\\theta \\ \\sin\\theta & \\cos\\theta \\end{bmatrix} \\begin{bmatrix} x \\ y \\end{bmatrix} $$
- **Проблема Пустых Углов:** При повороте квадратного изображения на угол, отличный от 90/180/270 градусов, углы исходного изображения выходят за границы, а в новые углы кадра "залезает" пустота.
  - По умолчанию torchvision заполняет эти области черным цветом (0).
  - **Риск:** Если модель обучается на изображениях с черными треугольниками по углам, она может выучить это как признак. В продакшене, если камера просто наклонена, черных углов не будет, и модель может сбиться.
  - **Решение:** Часто используют expand=True (расширение холста) или последующий CenterCrop, чтобы вырезать центральную часть без черных полей. Также используют padding_mode='reflection', заполняя пустоты зеркальным отражением краев изображения, что выглядит более естественно для сверток.

### **3.5 T.ColorJitter: Искажение в Пространстве Цветов**

Четвертая линза — T.ColorJitter(brightness=0.5, hue=0.3). Это фотометрическая аугментация. В отличие от геометрических, она не меняет координаты пикселей, но меняет их значения.

- **Яркость (brightness=0.5):** Коэффициент искажения b выбирается равномерно из \[1-0.5, 1+0.5\], то есть \[0.5, 1.5\]. Значения пикселей умножаются на этот коэффициент (с клиппингом до 255). Это симулирует разную экспозицию камеры или интенсивность освещения.
- **Оттенок (hue=0.3):** Это наиболее агрессивная часть. Для изменения оттенка изображение переводится из цветового пространства RGB в HSV (Hue, Saturation, Value). Канал H (Оттенок) представляет собой угол на цветовом круге (0-360^{\\circ} или 0-1 в нормализованном виде). Параметр 0.3 означает сдвиг оттенка на \\pm 30\\% от круга (\\pm 108^{\\circ}).
  - **Эффект:** Красный цвет может стать зеленым или фиолетовым. Для "Магического кота" это приемлемо (он магический). Но для задачи классификации фруктов (зеленое яблоко vs красное яблоко) такая аугментация сделает классы неразличимыми, внося шум в метки (label noise). "Техномант" должен с осторожностью применять джиттер оттенка.

## **4\. Стратегия Внедрения: Статика против Динамики**

В "Легенде Квеста" Техномант задает ключевой вопрос: нужно ли сохранять тысячи копий на диск? Ответ Мастера однозначен: **Нет**. Рассмотрим почему, сравнив статический и динамический подходы.

### **4.1 Статическая Аугментация (The Script Approach)**

Метод, показанный в скрипте quest_3_3.py:

1. Загрузить изображение.
2. Применить трансформации.
3. Сохранить результат на диск.

**Недостатки:**

- **Взрыв хранилища:** Если у вас 10,000 изображений и вы хотите создать 10 вариаций для каждого, вам нужно место для 100,000 изображений. В эпоху облачных вычислений (AWS S3) стоимость хранения растет линейно.
- **Ограниченная вариативность:** Даже если вы создадите 10 копий, модель увидит только эти 10 фиксированных вариантов. Она может переобучиться и на них.

Этот метод полезен только для дебаггинга (визуальной проверки того, что аугментации не слишком искажают суть), что и является целью квеста.

### **4.2 Динамическая Аугментация (On-the-Fly)**

Промышленный стандарт — использование torch.utils.data.DataLoader. **Архитектура процесса:**

1. **CPU (Worker Processes):** В то время как GPU занят обучением на батче N, процессор загружает батч N+1 с диска.
2. **Трансформация:** CPU применяет цепочку transforms к загруженным изображениям. Поскольку параметры (угол поворота, коэффициент яркости) выбираются генератором случайных чисел (random) в момент вызова, каждый раз получается **уникальная** версия.
3. **Передача на GPU:** Тензоры копируются в память видеокарты.

**Математика Бесконечности:** Если мы обучаем модель в течение 100 эпох на датасете из 1,000 изображений:

- В статике (без аугментации) модель видит одни и те же 1,000 матриц 100 раз.
- В динамике модель видит 1,000 \\times 100 \= 100,000 **разных** матриц. Исходное изображение кота никогда не повторяется в точности. Это эквивалентно обучению на бесконечном потоке данных, сэмплированном из распределения, заданного параметрами аугментации.

**Реализация в коде (для Свитка 15):**  
`from torch.utils.data import Dataset, DataLoader`

`class MagicalDataset(Dataset):`  
 `def __init__(self, file_paths, transform=None):`  
 `self.files = file_paths`  
 `self.transform = transform`

    `def __len__(self):`
        `return len(self.files)`

    `def __getitem__(self, idx):`
        `# 1. Загрузка оригинала с диска`
        `image = Image.open(self.files[idx])`

        `# 2. Применение магии "на лету"`
        `if self.transform:`
            `image = self.transform(image)`

        `return image, 0 # Возвращаем образ и метку`

При инициализации DataLoader критически важно использовать параметр num_workers. Трансформация изображений (особенно Resize и Rotation) — это тяжелая операция для CPU. Использование num_workers=4 или более позволяет распараллелить процесс подготовки данных, чтобы GPU никогда не простаивал в ожидании ("голодание GPU").

## **5\. Экономика и Бизнес-Ценность: Аугментация как Умножитель Капитала**

В корпоративной среде внедрение AI оценивается через призму ROI (Return on Investment). Аугментация данных предоставляет одни из самых высоких показателей ROI в машинном обучении.

### **5.1 Стоимость Разметки Данных (Labeling Cost)**

Сбор данных — это дорого. Разметка данных — это астрономически дорого.

- Стоимость разметки одного bounding box (рамки вокруг объекта) для задач детекции может варьироваться от $0.05 до $0.50.
- Семантическая сегментация (попиксельная маска) стоит еще дороже, так как требует кропотливой работы человека-асессора.

**Сценарий:** Компании нужно обучить дрон инспектировать линии электропередач на наличие дефектов (ржавчина, обрывы).

- _Без аугментации:_ Нужно собрать 50,000 фотографий дефектов в разных условиях (утро, день, туман, дождь, разные углы подлета). Это требует тысяч часов полетов и работы разметчиков.
- _С аугментацией:_ Достаточно собрать 1,000 качественных примеров. Используя ColorJitter (симуляция времени суток), GaussianBlur (симуляция расфокуса/тумана), RandomAffine (симуляция углов обзора), мы можем синтетически расширить этот датасет до эквивалента 50,000 примеров. **Экономия:** Сокращение затрат на сбор данных и разметку на 90-95% при сохранении сопоставимого качества модели.

### **5.2 Эффективность Использования Данных (Sample Efficiency)**

Научные исследования подтверждают количественный эффект.

- В **медицинской визуализации** (где данные пациентов приватны и редки), применение аугментации повысило точность сегментации костных структур с 0.87 до 0.95 (исследование на МРТ тазобедренного сустава). Это разница между моделью, непригодной для клиники, и инструментом уровня эксперта.
- В **обучении с подкреплением (RL)** аугментация визуальных наблюдений (например, RandomCrop кадров игры или симулятора) повышает sample efficiency (количество попыток, нужных для обучения агента) в разы, позволяя алгоритмам типа DrQ (Data-regularized Q) конкурировать с методами, использующими доступ к внутренним состояниям симулятора.

### **5.3 Страхование от Рисков ("Edge Cases")**

Бизнес-ценность также заключается в минимизации рисков. Модель, обученная только на "идеальных" студийных фото товара, провалится в реальном магазине, где освещение плохое, а камера дрожит. Аугментация (добавление шума, размытие, изменение контраста) готовит модель к худшим условиям эксплуатации. Это "страховой полис" для AI-продукта, гарантирующий, что он не сломается при первом же отклонении от нормы.

## **6\. Продвинутые Горизонты: За Пределами Базовых Фильтров**

Хотя Квест 3.3 фокусируется на базовых преобразованиях, современная "техномагия" ушла далеко вперед.

### **6.1 Автоматический Поиск Политик (AutoAugment)**

Ручной подбор параметров (degrees=45, hue=0.3) — это искусство, подверженное человеческим ошибкам. Почему 45, а не 30? Алгоритмы **AutoAugment** и **RandAugment** используют обучение с подкреплением или эволюционные алгоритмы для автоматического поиска оптимальной комбинации трансформаций для конкретного датасета. Они могут "открыть", что для датасета CIFAR-10 лучше всего работает комбинация "Equalize" и "ShearX", а для SVHN — "Invert" и "TranslateY".

### **6.2 Деструктивная Аугментация (MixUp и CutMix)**

Классическая аугментация стремится сохранить объект узнаваемым. Новые методы идут от обратного.

- **MixUp:** Берет два изображения (Кошка и Собака) и смешивает их пиксели с коэффициентом \\lambda (например, 0.6). Метка также смешивается: y \= 0.6 \\times \\text{Кот} \+ 0.4 \\times \\text{Собака}. Модель учится предсказывать не жесткий класс, а распределение. Это делает поведение сети линейным между классами, резко снижая уверенность на ошибочных предсказаниях.
- **CutMix:** Вырезает квадрат из изображения Собаки и вклеивает его на изображение Кошки. Это заставляет модель смотреть не только на самую характерную часть (например, уши), но и на остальные части тела, так как уши могут быть перекрыты.

### **6.3 Генеративная Аугментация (Синтез Данных)**

Использование **GAN** (Generative Adversarial Networks) и **Diffusion Models** (например, Stable Diffusion) для генерации полностью новых, синтетических изображений обучающего класса. Это позволяет создавать уникальные сценарии (например, "автомобильная авария в снежную бурю"), которые крайне сложно найти в реальности, но критически важно распознать.

## **7\. Заключение: Завершение Ритуала**

Квест 3.3, несмотря на свою кажущуюся простоту, открывает дверь в одну из самых глубоких тем искусственного интеллекта. Мы начали с одного артефакта (magical_cat.png) и скрипта из четырех строк. Но через призму анализа мы увидели, что эти строки кода являются ключом к созданию **робастного, обобщающего и экономически эффективного** ИИ.  
Мы выяснили, что:

1. **Аугментация — это математическая необходимость**, позволяющая исследовать многообразие данных и прививать моделям инвариантность, недоступную им архитектурно.
2. **torchvision — это мощный инструмент**, скрывающий сложную линейную алгебру и теорию сигналов (интерполяция, цветовые пространства) за простым интерфейсом. Понимание этих деталей отличает новичка от эксперта.
3. **Динамическая аугментация** — единственный верный путь для масштабного обучения. Она превращает конечный диск в бесконечный источник опыта для Голема.
4. **Бизнес-ценность** этой технологии измеряется не только в точности модели, но и в тысячах сэкономленных долларов на разметке и сборе данных.

Для Техноманта, задавшего вопрос о сохранении файлов, ответ теперь очевиден и обоснован на всех уровнях: от файловой системы до теоретической механики обучения. Мы не храним иллюзии в сундуках; мы творим их в моменте, каждый раз новые, заставляя Голема видеть мир во всем его хаотичном, изменчивом великолепии.

### **Сводная Таблица: Эффект Аугментации**

| Метрика                                    | Без Аугментации (Статика)      | С Динамической Аугментацией            | Примечание                            |
| :----------------------------------------- | :----------------------------- | :------------------------------------- | :------------------------------------ |
| **Размер обучающей выборки (эффективный)** | N (размер датасета)            | \\approx \\infty (уникальные вариации) | Модель никогда не видит точный повтор |
| **Риск переобучения**                      | Высокий (Memory lookup)        | Низкий (Feature learning)              | Регуляризация шумом                   |
| **Инвариантность**                         | Отсутствует (зависит от теста) | Выученная (Learned Invariance)         | Робастность к поворотам/свету         |
| **Стоимость хранения**                     | 1\\times                       | 1\\times                               | Генерация на лету (CPU cost only)     |
| **Точность (пример Medical MRI)**          | 87%                            | 95%                                    | Критично для Life-Critical систем     |

Так завершается разбор ритуала "Усиления Иллюзий". Пусть ваш конвейер будет быстрым, а loss — низким.

#### **Источники**

1\. The Golem in the age of artificial intelligence | UvA-DARE (Digital Academic Repository) \- Research Explorer, https://pure.uva.nl/ws/files/60099217/NECSUS\_2020\_9\_1\_101\_123\_Vudka\_Golem\_in\_the\_age\_of\_artificial\_intelligence\_.pdf 2\. (PDF) The Golem in the Age of Artificial Intelligence \- ResearchGate, https://www.researchgate.net/publication/357910123\_The\_Golem\_in\_the\_Age\_of\_Artificial\_Intelligence 3\. How to Create a Golem: From Artificial Intelligence to Artificial Human \- New Kabbalah, https://newkabbalah.com/philosophical-perspectives/how-to-create-a-golem-from-artificial-intelligence-to-artificial-human/ 4\. Word \- The Open University, https://www.open.edu/openlearn/digital-computing/machines-minds-and-computers/altformat-word 5\. torchvision.transforms \- PyTorch documentation, https://docs.pytorch.org/vision/0.11/transforms.html 6\. Data Augmentation: Benefits and Disadvantages \- Medium, https://medium.com/@pouyahallaj/data-augmentation-benefits-and-disadvantages-38d8201aead 7\. A Complete Guide to Data Augmentation | DataCamp, https://www.datacamp.com/tutorial/complete-guide-data-augmentation 8\. What is Data Augmentation? \- Data Augmentation Techniques Explained \- AWS, https://aws.amazon.com/what-is/data-augmentation/ 9\. Comparison of Different Image Data Augmentation Approaches \- PMC \- NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC8707550/ 10\. Why use Data Augmentation? : r/computervision \- Reddit, https://www.reddit.com/r/computervision/comments/1ay7bhl/why\_use\_data\_augmentation/ 11\. A Comprehensive Survey on Data Augmentation \- arXiv, https://arxiv.org/pdf/2405.09591 12\. A Practical Guide for Data Augmentation to Increase Model Accuracy in PyTorch \- Medium, https://medium.com/@BurtMcGurt/a-practical-guide-to-data-augmentation-in-pytorch-with-examples-and-visualizations-761ad5c2a903 13\. What is data augmentation? \- IBM, https://www.ibm.com/think/topics/data-augmentation 14\. vision/torchvision/transforms/transforms.py at main · pytorch/vision \- GitHub, https://github.com/pytorch/vision/blob/main/torchvision/transforms/transforms.py 15\. torchvideo.transforms \- Read the Docs, https://torchvideo.readthedocs.io/en/latest/transforms.html 16\. Ultimate Guide to Fine-Tuning in PyTorch : Part 3 —Deep Dive to PyTorch Data Transforms with Examples | by Ruman, https://rumn.medium.com/ultimate-guide-to-fine-tuning-in-pytorch-part-3-deep-dive-to-pytorch-data-transforms-53ed29d18dde 17\. torchvision.transforms.Compose() \- PyTorch documentation, https://docs.pytorch.org/vision/0.22/generated/torchvision.transforms.Compose.html 18\. Transforming images, videos, boxes and more — Torchvision 0.24 ..., https://docs.pytorch.org/vision/stable/transforms.html 19\. Data Augmentation in PyTorch \- python \- Stack Overflow, https://stackoverflow.com/questions/51677788/data-augmentation-in-pytorch 20\. ColorJitter — Torchvision main documentation, https://docs.pytorch.org/vision/main/generated/torchvision.transforms.ColorJitter.html 21\. PyTorch – Randomly change the brightness, contrast, saturation and hue of an image, https://www.tutorialspoint.com/pytorch-randomly-change-the-brightness-contrast-saturation-and-hue-of-an-image 22\. How to interpret arguments to torchvision.transform.ColorJitter? \- vision \- PyTorch Forums, https://discuss.pytorch.org/t/how-to-interpret-arguments-to-torchvision-transform-colorjitter/69319 23\. Data augmentation in PyTorch, https://discuss.pytorch.org/t/data-augmentation-in-pytorch/7925 24\. On-the-fly Data Augmentation for Forecasting with Deep Learning \- arXiv, https://arxiv.org/html/2404.16918v1 25\. Dynamic Dataloaders for on the fly modifications \- PyTorch Forums, https://discuss.pytorch.org/t/dynamic-dataloaders-for-on-the-fly-modifications/78870 26\. Datasets & DataLoaders — PyTorch Tutorials 2.9.0+cu128 documentation, https://docs.pytorch.org/tutorials/beginner/basics/data\_tutorial.html 27\. What Is Data Labeling? | IBM, https://www.ibm.com/think/topics/data-labeling 28\. How Businesses Can Benefit from Data Augmentation Solutions \- rinf.tech, https://www.rinf.tech/how-businesses-can-benefit-from-data-augmentation-solutions/ 29\. The impact of data augmentation and transfer learning on the performance of deep learning models for the segmentation of the hip on 3D magnetic resonance images \- PubMed Central, https://pmc.ncbi.nlm.nih.gov/articles/PMC11308385/ 30\. Learning Better with Less: Effective Augmentation for Sample-Efficient Visual Reinforcement Learning, https://proceedings.neurips.cc/paper\_files/paper/2023/file/bc26087d3f82e62044fc77752e86737e-Paper-Conference.pdf 31\. A Comprehensive Survey of Data Augmentation in Visual Reinforcement Learning \- arXiv, https://arxiv.org/html/2210.04561v4 32\. The Real Business Value of Computer Vision \- Viso Suite, https://viso.ai/deep-learning/computer-vision-business-value/ 33\. Amplifying Deep Learning: A Dive into Data Augmentation Strategies \- Analytics Vidhya, https://www.analyticsvidhya.com/blog/2024/01/amplifying-deep-learning-a-dive-into-data-augmentation-strategies/ 34\. Tied-Augment: Controlling Representation Similarity Improves Data Augmentation \- Proceedings of Machine Learning Research, https://proceedings.mlr.press/v202/kurtulus23a/kurtulus23a.pdf 35\. What You Need to Know About Data Augmentation in Machine Vision Systems \- UnitX, https://www.unitxlabs.com/data-augmentation-machine-vision-systems/ 36\. A Survey on Data Augmentation in Large Model Era \- arXiv, https://arxiv.org/html/2401.15422v2
