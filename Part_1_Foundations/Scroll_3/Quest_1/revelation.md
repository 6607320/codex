# **Квест 3.1: Призыв «Всевидящего Ока» — Архитектура MobileNetV2, парадокс сдвига домена и бизнес-императивы граничных вычислений**

## **Аннотация**

В современном технологическом ландшафте компьютерное зрение эволюционировало из нишевой академической дисциплины в вездесущую утилиту, которую метафорически можно назвать «Всевидящим Оком» искусственного интеллекта. Способность машин интерпретировать визуальные данные трансформирует отрасли — от автономного транспорта до медицинской диагностики. Данный отчет представляет собой исчерпывающее исследование развертывания этой способности с использованием архитектуры MobileNetV2 на наборе данных CIFAR-10. Хотя непосредственная техническая задача заключается в реализации скрипта инференса с помощью экосистемы Python-библиотек transformers и datasets, глубинные последствия касаются фундаментальных проблем современного ИИ: компромисса между эффективностью и точностью, вездесущей проблемы сдвига домена (domain shift) и перевода теоретических моделей в осязаемую бизнес-ценность.

MobileNetV2 знаменует собой водораздел в проектировании эффективных нейронных сетей, внедряя инвертированные остаточные блоки и линейные "бутылочные горлышка" для максимизации производительности на оборудовании с ограниченными ресурсами.1 Однако применение модели, предварительно обученной на высокодетальных изображениях ImageNet, к низкоразрешенной и онтологически отличной среде CIFAR-10 обнажает «слепые зоны» Всевидящего Ока. Этот отчет деконструирует математические основы модели, предоставляет надежную техническую реализацию, анализирует дивергенцию между исходным и целевым доменами и оценивает стратегические бизнес-императивы, стимулирующие внедрение легковесных моделей зрения.

---

## **Часть I: Теоретический каркас эффективного зрения**

### **1.1 Эволюция и стремление к всеведению**

Стремление к машинному зрению — способности компьютера интерпретировать визуальный мир аналогично человеческому глазу и коре головного мозга — прошло долгий путь от извлечения вручную созданных признаков (таких как SIFT и HOG) до парадигм сквозного обучения глубоких сверточных нейронных сетей (CNN). Метафора «Всевидящего Ока» отражает аспирацию этих систем достичь всеведения в классификации, идентифицируя объекты, сцены и аномалии со сверхчеловеческой точностью и скоростью.

Ранние модели глубокого обучения, такие как AlexNet (2012) и VGG (2014), продемонстрировали, что глубина и ширина сетевой архитектуры сильно коррелируют с точностью. VGG-16, например, содержит более 138 миллионов параметров и требует колоссального количества операций с плавающей запятой (FLOPs) для одного прохода инференса.2 В условиях дата-центра с мощными GPU это управляемо. Однако в контексте мобильных устройств, IoT-сенсоров или встроенных контроллеров дронов, где энергопотребление и тепловыделение являются критическими факторами, такие "тяжелые" архитектуры неприменимы. Это ограничение привело к дивергенции в исследованиях архитектур: один путь преследовал максимальную точность (например, ResNeXt, ViT-Huge), а другой — максимальную эффективность, породив семейство MobileNet.

### **1.2 MobileNetV2: Архитектурная деконструкция**

MobileNetV2, разработанная исследователями Google, является примером линии, ориентированной на эффективность. Она создана специально для того, чтобы перенести «Всевидящее Око» на переферию (edge), обеспечивая инференс на смартфонах и встроенных системах без необходимости в специализированных аппаратных ускорителях. Архитектура улучшает своего предшественника, MobileNetV1, решая проблему потери информации в нелинейных слоях и оптимизируя поток градиентов.2

#### **1.2.1 Глубинно-разделимые свертки (Depthwise Separable Convolutions)**

Краеугольным камнем семейства MobileNet является глубинно-разделимая свертка. В стандартной свертке ядро работает одновременно по всем входным каналам и пространственным измерениям, создавая огромную вычислительную нагрузку. Если вход имеет размеры $H \\times W \\times C\_{in}$, и мы применяем $C\_{out}$ фильтров размера $K \\times K$, вычислительная стоимость составляет:

$$Cost\_{std} \= H \\times W \\times C\_{in} \\times C\_{out} \\times K \\times K$$  
MobileNet разлагает эту операцию на два отдельных этапа:

1. **Глубинная свертка (Depthwise Convolution):** Применяет один фильтр к каждому входному каналу отдельно. Это фильтрует входные данные, но не комбинирует их для создания новых признаков.
2. **Поточечная свертка (Pointwise Convolution):** Применяет свертку $1 \\times 1$ для линейного комбинирования выходов глубинного слоя.

Это разложение радикально снижает вычислительную нагрузку. Стоимость становится:

$$Cost\_{dw} \= H \\times W \\times C\_{in} \\times K \\times K \+ H \\times W \\times C\_{in} \\times C\_{out}$$  
Коэффициент сокращения вычислений приблизительно равен $\\frac{1}{C\_{out}} \+ \\frac{1}{K^2}$. Для стандартного ядра $3 \\times 3$ это приводит к снижению объема вычислений в 8–9 раз при минимальной потере точности.3 Это математическое изящество позволяет запускать сложные модели классификации на устройствах с батарейным питанием.

#### **1.2.2 Инвертированные остаточные блоки и линейные "бутылочные горлышка"**

MobileNetV2 вводит новый строительный блок, известный как «Инвертированный остаточный блок с линейным узким местом» (Inverted Residual with Linear Bottleneck). Чтобы понять его значимость, необходимо противопоставить его традиционному остаточному блоку, используемому в ResNet.

- **Традиционный блок ResNet:** Широкий $\\rightarrow$ Узкий $\\rightarrow$ Широкий. Вход сжимается в низкоразмерное представление (узкое место) с помощью свертки $1 \\times 1$, обрабатывается, а затем расширяется. Это эффективно, но сжатие информации может привести к потере деталей.
- **Инвертированный блок MobileNetV2:** Узкий $\\rightarrow$ Широкий $\\rightarrow$ Узкий. Сеть расширяет низкоразмерный вход в высокоразмерное пространство (используя свертку $1 \\times 1$ с коэффициентом расширения, обычно $t=6$), применяет легкую глубинную свертку в этом богатом высокоразмерном пространстве, а затем проецирует его обратно в низкоразмерное представление.2

Термин «Линейное бутылочное горлышко» (Linear Bottleneck) относится к удалению нелинейной функции активации (например, ReLU) в конце проекционного слоя. Исследователи обнаружили, что применение нелинейности в пространствах низкой размерности разрушает информацию. Сохраняя линейный выход в узком месте, модель сохраняет "многообразие интереса" (manifold of interest), позволяя практически «без потерь» передавать информацию в следующий блок.2

Эта архитектура создает структуру, которая может быть встроена в подпространства низкой размерности, позволяя модели быть невероятно легкой, сохраняя при этом выразительность, необходимую для сложных задач классификации.1

---

## **Часть II: Экосистема данных — CIFAR-10 против ImageNet**

Для реализации «Всевидящего Ока» мы должны понимать данные, которые оно потребляет. Запрос пользователя определяет использование набора данных CIFAR-10, однако MobileNetV2 обычно предварительно обучается на ImageNet. Это сопоставление создает центральное напряжение реализации и является классическим примером проблемы переноса знаний.

### **2.1 Набор данных CIFAR-10: Микрокосм компьютерного зрения**

Набор данных CIFAR-10, созданный Канадским институтом передовых исследований (Canadian Institute for Advanced Research), является эталонным стандартом в компьютерном зрении. Он состоит из 60 000 цветных изображений, классифицированных по 10 взаимоисключающим классам: _самолет, автомобиль, птица, кошка, олень, собака, лягушка, лошадь, корабль и грузовик_.5

- **Объем:** 50 000 обучающих изображений и 10 000 тестовых изображений.
- **Разрешение:** $32 \\times 32$ пикселя.
- **Происхождение:** Размеченное подмножество набора данных «80 Million Tiny Images».6

Определяющей характеристикой CIFAR-10 является его низкое разрешение. При размере $32 \\times 32$ пикселя отличительные признаки часто размыты или блочны. «Кошка» в CIFAR-10 может представлять собой пятно пикселей, где уши едва различимы, тогда как «кошка» в наборах данных высокого разрешения содержит детали текстуры, такие как усы и узоры меха. Это низкое разрешение позволяет быстро создавать прототипы и проводить алгоритмический бенчмаркинг, но представляет собой серьезную проблему для моделей, привыкших к высокой детализации входных данных.6

Интересно отметить, что CIFAR-10 также используется для исследований в области нейроморфного зрения. Существуют версии датасета, такие как CIFAR10-DVS, где статические изображения преобразованы в потоки событий (event streams), имитирующие работу сетчатки глаза, что подчеркивает фундаментальную роль этого набора данных в развитии различных парадигм зрения.7

### **2.2 Стандарт ImageNet: Обучение Гигантов**

MobileNetV2, как и большинство современных архитектур, предварительно обучена на наборе данных ImageNet-1k (ILSVRC), содержащем 1,28 миллиона обучающих изображений в 1000 классах.1

- **Разрешение:** Обычно $224 \\times 224$ пикселя (стандартный вход для MobileNetV2).
- **Разнообразие:** Классы варьируются от конкретных пород собак (например, _Сибирский хаски, Маламут_) до геологических образований (_вулкан, долина_) и предметов домашнего обихода (_эспрессо-машина, туалетная бумага_).9

Различие между этими двумя наборами данных заключается не просто в размере, а в **гранулярности** и **онтологии**. Таксономия ImageNet основана на лексической базе WordNet, предлагая специфические гипонимы (например, «Полосатая кошка», «Египетская кошка», «Тигровая кошка») 9, тогда как CIFAR-10 использует грубую, агрегированную метку («Кошка»).11 Это различие имеет решающее значение при использовании предварительно обученной модели: «Всевидящее Око» обучено быть гиперспецифичным, но задача требует широкого обобщения.

### **2.3 Сравнительный анализ распределений**

| Характеристика         | CIFAR-10                                                   | ImageNet (ILSVRC)                             | Влияние на MobileNetV2                                                      |
| :--------------------- | :--------------------------------------------------------- | :-------------------------------------------- | :-------------------------------------------------------------------------- |
| **Количество классов** | 10 (Агрегированные)                                        | 1000 (Специфичные)                            | Модель выдаст специфичный класс (напр., "пудель") вместо общего ("собака"). |
| **Разрешение входа**   | $32 \\times 32$                                            | $224 \\times 224$ (и выше)                    | Требуется агрессивный апскейлинг, создающий артефакты интерполяции.         |
| **Содержание сцены**   | Объект занимает большую часть кадра, фон размыт или прост. | Сложные сцены, объект может быть не в центре. | Модель может переоценивать фоновые признаки.                                |
| **Источник данных**    | 80 Million Tiny Images                                     | Web scraping (Flickr и др.)                   | Различные bias в сборе данных (Covariate Shift).                            |

---

## **Часть III: Техническая реализация — Скрипт «Всевидящего Ока»**

В этом разделе подробно описывается реализация Python-скрипта для «Всевидящего Ока». Этот скрипт использует современные библиотеки Hugging Face transformers и datasets. Этот стек абстрагирует большую часть шаблонного кода, связанного с загрузчиками данных PyTorch и определением модели, позволяя сосредоточиться на логике инференса.

### **3.1 Архитектурные пререквизиты**

Скрипт опирается на следующие компоненты:

1. **Библиотека datasets:** Обеспечивает эффективный доступ к набору данных CIFAR-10 с отображением в память. Мы продемонстрируем возможность **потоковой передачи (streaming)**, которая позволяет обрабатывать наборы данных, превышающие объем оперативной памяти, путем итерации по ним без полной загрузки.12
2. **Библиотека transformers:** Предоставляет классы AutoModelForImageClassification и AutoImageProcessor. Процессор служит мостом между необработанными данными изображения и ожидаемым форматом ввода модели.14
3. **Модель MobileNetV2:** Используется конкретный чекпоинт google/mobilenet_v2_1.0_224.1

### **3.2 Проблема входных размерностей и препроцессинг**

Критически важной деталью реализации является размер входных данных. Изображения CIFAR-10 имеют размер $32 \\times 32$. MobileNetV2 обучена на $224 \\times 224$. Хотя CNN технически инвариантны к трансляции и могут принимать входные данные переменного размера (вплоть до $32 \\times 32$), предварительно обученные веса «выучили» признаки именно в масштабе $224$.15

Подача изображения $32 \\times 32$ непосредственно в сеть, ожидающую $224 \\times 224$, обычно приводит к:

1. **Несоответствию признаков:** Рецептивные поля ядер свертки эффективно видят все изображение как крошечный патч, не удается извлечь иерархические признаки.
2. **Деградации производительности:** Точность значительно падает, потому что пространственные иерархии (края $\\rightarrow$ формы $\\rightarrow$ объекты) искажаются.

Следовательно, **Image Processor** должен увеличить масштаб изображений CIFAR. AutoImageProcessor автоматически обрабатывает это изменение размера, если он правильно настроен, обычно изменяя размер кратчайшего края до 256, а затем выполняя центральную обрезку до 224\.1 Также важным этапом является нормализация: пиксели приводятся к диапазону, на котором обучалась сеть (среднее значение и стандартное отклонение ImageNet).

### **3.3 Полный код реализации**

Ниже приведен полный аннотированный скрипт. Он загружает набор данных в потоковом режиме, извлекает изображение, выполняет предварительную обработку и запускает инференс.

Python

import torch  
from datasets import load_dataset  
from transformers import AutoImageProcessor, AutoModelForImageClassification  
from PIL import Image  
import numpy as np

def run_all_seeing_eye():  
 """  
 Основная функция запуска квеста 'Всевидящее Око'.  
 Демонстрирует классификацию изображений CIFAR-10 с использованием MobileNetV2.  
 """  
 print("=== Квест 3.1: Призыв 'Всевидящего Ока' (MobileNetV2 на CIFAR-10) \===")

    \# \---------------------------------------------------------
    \# 1\. Конфигурация и выбор модели
    \# \---------------------------------------------------------
    \# Мы используем предварительно обученную MobileNetV2 от Google.
    \# Чекпоинт: google/mobilenet\_v2\_1.0\_224
    \# 1.0 — множитель глубины, 224 — входное разрешение.
    model\_checkpoint \= "google/mobilenet\_v2\_1.0\_224"

    print(f"\[+\] Инициализация 'Всевидящего Ока': {model\_checkpoint}")

    \# Загрузка Image Processor: Обрабатывает изменение размера (32-\>224),
    \# нормализацию и конвертацию в тензоры.
    \# Это гарантирует, что входные данные соответствуют статистическому распределению ImageNet.
    try:
        processor \= AutoImageProcessor.from\_pretrained(model\_checkpoint)
    except Exception as e:
        print(f"Ошибка загрузки процессора: {e}")
        return

    \# Загрузка модели: Архитектура с предварительно обученными весами.
    try:
        model \= AutoModelForImageClassification.from\_pretrained(model\_checkpoint)
    except Exception as e:
        print(f"Ошибка загрузки модели: {e}")
        return

    \# Перенос на GPU для эффективности, если доступно
    device \= "cuda" if torch.cuda.is\_available() else "cpu"
    model.to(device)
    print(f"\[+\] Модель загружена на устройство: {device}")

    \# \---------------------------------------------------------
    \# 2\. Поглощение данных (Режим Streaming)
    \# \---------------------------------------------------------
    \# Мы используем streaming=True для прогрессивной загрузки данных.
    \# Это симулирует реальный сценарий, когда данные могут быть живым потоком
    \# или слишком велики для диска/памяти (Big Data паттерн).
    print("\[+\] Подключение к потоку данных CIFAR-10...")
    \# Загружаем тестовый сплит, так как нас интересует оценка, а не обучение
    dataset \= load\_dataset("cifar10", split="test", streaming=True)

    \# Извлекаем первый пример из потока
    \# Итератор возвращает словарь: {'img': PIL.Image, 'label': int}
    data\_iterator \= iter(dataset)
    example \= next(data\_iterator)

    image \= example\['img'\]  \# Объект PIL Image (32x32)
    true\_label\_idx \= example\['label'\] \# Индекс класса CIFAR-10

    \# Карта меток CIFAR-10 (для отображения)
    cifar\_labels \= \['airplane', 'automobile', 'bird', 'cat', 'deer',
                    'dog', 'frog', 'horse', 'ship', 'truck'\]
    true\_label\_name \= cifar\_labels\[true\_label\_idx\]

    print(f"\[+\] Изображение получено. Разрешение: {image.size}")
    print(f"\[+\] Истинная метка (CIFAR-10): {true\_label\_name} (ID: {true\_label\_idx})")

    \# \---------------------------------------------------------
    \# 3\. Препроцессинг и сдвиг разрешения
    \# \---------------------------------------------------------
    \# Процессор автоматически масштабирует 32x32 изображение до 224x224.
    \# Он также нормализует пиксели на основе mean/std ImageNet.
    \# return\_tensors="pt" возвращает тензоры PyTorch.
    inputs \= processor(images=image, return\_tensors="pt")
    inputs \= {k: v.to(device) for k, v in inputs.items()}

    \# \---------------------------------------------------------
    \# 4\. Инференс (Взгляд Ока)
    \# \---------------------------------------------------------
    print("\[+\] Обработка изображения через MobileNetV2...")
    with torch.no\_grad():
        outputs \= model(\*\*inputs)

    \# Модель выдает 'logits' \- сырые, ненормализованные оценки.
    logits \= outputs.logits

    \# Применяем Softmax для получения вероятностей
    probabilities \= torch.nn.functional.softmax(logits, dim=-1)

    \# Получаем топ-5 предсказаний из 1000 классов ImageNet
    top5\_prob, top5\_catid \= torch.topk(probabilities, 5)

    print("\\n=== Интерпретация 'Всевидящего Ока' \===")
    print("Примечание: Модель предсказывает классы ImageNet (1000 категорий),")
    print("в то время как входной объект из CIFAR-10 (10 категорий).")
    print("Наблюдайте, как модель отображает общее понятие на конкретные экземпляры.\\n")

    domain\_match \= False
    predicted\_concept \= ""

    for i in range(top5\_prob.size(1)):
        \# Получаем имя класса из конфигурации модели (id2label)
        label\_idx \= top5\_catid\[i\].item()
        score \= top5\_prob\[i\].item()
        label\_name \= model.config.id2label\[label\_idx\]

        if i \== 0: predicted\_concept \= label\_name

        print(f"Ранг {i+1}: {label\_name} (Уверенность: {score:.4f})")

        \# Простая эвристика для проверки семантического совпадения
        if true\_label\_name in label\_name.lower() or label\_name.lower() in true\_label\_name:
            domain\_match \= True

    \# \---------------------------------------------------------
    \# 5\. Анализ сдвига домена (Domain Shift)
    \# \---------------------------------------------------------
    print("\\n=== Анализ сдвига домена \===")
    print(f"Задача модели: Найти 1 из 1000 специфичных классов.")
    print(f"Входной сигнал: Грубый класс '{true\_label\_name}'.")

    if domain\_match:
         print("Результат: Обнаружено прямое семантическое совпадение.")
         print("Модель успешно обобщила низкоразрешенный объект до специфического класса ImageNet.")
    else:
         print("Результат: Обнаружен потенциальный сдвиг домена или семантический дрифт.")
         print(f"Специфический тип ImageNet ('{predicted\_concept}') лексически не совпадает с меткой CIFAR.")
         print("Это может быть вызвано артефактами апскейлинга или разницей в онтологии классов.")

if \_\_name\_\_ \== "\_\_main\_\_":  
 run_all_seeing_eye()

### **3.4 Пояснение кода и механизмов**

1. **load_dataset(..., streaming=True)**: Эта функция изменяет парадигму работы с данными. Вместо загрузки полного набора данных CIFAR-10 (около 170 МБ) на локальный диск, она создает IterableDataset. Данные передаются по сети по мере необходимости. next(iter(dataset)) извлекает первый образец из буфера. Это критически важно для работы с петабайтными датасетами (например, LAION-5B), где полная загрузка невозможна.12
2. **AutoImageProcessor**: Это «переводчик» между необработанными пикселями CIFAR и ожиданиями MobileNetV2. Конфигурационный файл preprocessor_config.json, связанный с google/mobilenet_v2_1.0_224, диктует, что изображения должны быть изменены в размере (параметр resize_mode, часто shortest_edge=256), обрезаны по центру до 224x224 и нормализованы с использованием среднего значения \[0.485, 0.456, 0.406\] и стандартного отклонения \[0.229, 0.224, 0.225\].1 Без этого этапа модель получала бы данные с неправильным статистическим распределением и пространственными размерами, что превратило бы выходные данные в шум.
3. **model.config.id2label**: MobileNetV2 предсказывает индексы от $0$ до $999$ (или 1000 с фоновым классом). Этот словарь сопоставляет эти целые числа с понятными человеку строками ImageNet (например, «281: tabby, tabby cat»). Это отображение имеет решающее значение для интерпретации «мыслей» Всевидящего Ока.9

---

## **Часть IV: Феномен сдвига домена (Domain Shift)**

Скрипт, приведенный выше, подчеркивает критическую проблему: модель выдает предсказания вроде «Tabby Cat» (Полосатая кошка) или «Airliner» (Авиалайнер), в то время как истина проста — «Cat» (Кошка) или «Airplane» (Самолет). Это несоответствие — лишь верхушка айсберга, известного как **сдвиг домена** (Domain Shift), центральная «слепая зона» Всевидящего Ока.

### **4.1 Определение и математическая природа**

Сдвиг домена возникает, когда совместное распределение вероятностей входных признаков $X$ и меток $Y$ отличается между этапом обучения (Исходный домен, $D\_S$) и этапом инференса (Целевой домен, $D\_T$).

$$P\_S(X, Y) \\neq P\_T(X, Y)$$  
В нашем сценарии Квеста 3.1 этот сдвиг проявляется в нескольких измерениях:

1. **Ковариантный сдвиг (Covariate Shift, $P\_S(X) \\neq P\_T(X)$):** Изменяется распределение входных изображений.
   - _Исходный (ImageNet):_ Высокое разрешение ($224 \\times 224$), профессиональная фотография, центрированные объекты, четкое освещение, разнообразные фоны.8
   - _Целевой (CIFAR-10):_ Низкое разрешение ($32 \\times 32$), пикселизация, потенциально отличные артефакты сжатия, менее строгое центрирование объектов.6
   - _Влияние:_ Признаки, выученные MobileNetV2 (детекторы краев, анализаторы текстур), могут не срабатывать корректно на размытых, блочных краях изображений CIFAR. Шаг изменения размера в скрипте пытается смягчить это путем интерполяции пикселей, но он не может «галлюцинировать» отсутствующие детали достоверно.
2. **Сдвиг меток (Label Shift, $P\_S(Y) \\neq P\_T(Y)$):** Изменяется распределение классов.
   - ImageNet имеет 1000 классов с сильным перекосом в сторону пород собак (более 100 классов) и специфической биологии.9
   - CIFAR-10 имеет 10 классов с равномерным распределением (6000 изображений на класс).6
   - _Влияние:_ Модель «сверхподготовлена» для собак и «недоподготовлена» для общих категорий. Это априорное смещение (prior bias) в весах модели искажает предсказания.
3. **Концептуальный сдвиг (Concept Shift):**
   - Функция отображения $X \\rightarrow Y$ меняет определение. В ImageNet «Кошка» — это не одна метка, а иерархия из «Персидской», «Сиамской», «Тэбби» и т. д..9 В CIFAR «Кошка» — это агрегация всех этих понятий.
   - _Путаница «Всевидящего Ока»:_ Если модель видит размытую кошку и предсказывает «дверной коврик» (doormat), потому что текстура пикселей напоминает плетеный узор, который она выучила на ImageNet, это провал переноса концепции, вызванный деградацией разрешения.

### **4.2 Примеры сбоев в реальном мире: Когда Око моргает**

Хотя неправильная классификация кошки в CIFAR — это безобидное академическое упражнение, сдвиг домена вызывает катастрофические сбои в ответственных бизнес-средах и системах безопасности.

#### **4.2.1 Медицинская диагностика**

В медицинской визуализации модели, обученные на рентгеновских снимках из одной больницы (Исходный домен), часто выходят из строя при развертывании в другой больнице (Целевой домен).20

- **Сдвиг:** Разные производители аппаратов (GE против Siemens) создают изображения с небольшими различиями в контрасте, шуме и разрешении. Демография пациентов (возраст, раса) также может отличаться.
- **Последствие:** Инструмент триажа на основе MobileNetV2 может ошибочно классифицировать случай «Пневмонии» как «Нормальный» просто потому, что рентгеновский аппарат использовал другую настройку напряжения, изменяющую распределение интенсивности пикселей. Это классический сбой **ковариантного сдвига**.22

#### **4.2.2 Автономное вождение**

Беспилотные автомобили полагаются на модели зрения для обнаружения пешеходов и знаков.

- **Сдвиг:** Модель, обученная на дневных данных (Источник), развернутая ночью или в дождь (Цель), испытывает серьезный сдвиг домена из\-за изменения освещения.22 Аналогично, обучение в Калифорнии (солнечно, четкая разметка полос) и развертывание в сельской Норвегии (снег, скрытая разметка) представляет собой экологический сдвиг.
- **Последствие:** «Всевидящее Око» становится слепым к препятствиям, которые выпадают за пределы статистического распределения его обучающих данных, что приводит к авариям.24

#### **4.2.3 Другие примеры**

Исследования также показывают значительный сдвиг домена в таких наборах данных, как **HAM10k** (дерматоскопия), где изменения в освещении или типе кожи могут резко снизить точность диагностики меланомы, или **Stanford Dogs**, где модели часто путают породы из\-за фона, а не самих собак.25

### **4.3 Решение проблемы: Fine-Tuning и адаптация**

Предоставленный скрипт использует предварительно обученную модель (режим Zero-Shot/Transfer). Чтобы действительно устранить сдвиг домена, мы должны применить **Адаптацию домена** через **Тонкую настройку (Fine-Tuning)**.

Этот процесс включает:

1. **Замораживание (Freezing)** начальных слоев MobileNetV2 (которые детектируют универсальные признаки, такие как края).
2. **Замену** классификационной головы (последнего слоя) из 1000 нейронов на новый слой из 10 нейронов (для CIFAR-10).
3. **Переобучение (Retraining)** сети на обучающем наборе CIFAR-10 с использованием небольшого learning rate.

Этот процесс обновляет внутренние веса модели (особенно в глубоких слоях), чтобы приспособиться к низкому разрешению и специфическим признакам нового домена, эффективно «корректируя» зрение Всевидящего Ока для новой среды.26 Альтернативные стратегии включают использование методов обучения без учителя (unsupervised domain adaptation) для выравнивания распределений признаков без меток целевого домена.

---

## **Часть V: Анализ бизнес-ценности и стратегические выводы**

Почему стоит внедрять MobileNetV2? Почему бизнесу нужно заботиться о сдвиге домена? Ответ кроется в трансформационной ценности **Edge AI** (ИИ на периферии).

### **5.1 Дивиденд эффективности и экономика Edge AI**

MobileNetV2 разработана для минимизации **операций умножения-накопления (MACs)**. Стандартная ResNet-50 может требовать \~4 GFLOPs на изображение. MobileNetV2 (1.0) требует всего \~300 MFLOPs.2

- **Влияние на бизнес:** Это снижает требования к оборудованию с сервера NVIDIA GPU стоимостью 5000 долларов до Raspberry Pi за 50 долларов или стандартного чипа смартфона.
- **Масштабируемость:** Компании могут развертывать тысячи датчиков (камер) без линейного увеличения затрат на облачные вычисления. Экономия на пропускной способности сети (bandwidth) колоссальна, так как видеопотоки не нужно отправлять в облако.

### **5.2 Латентность и Приватность («Всевидящий» Край)**

Запуская инференс локально (On-Device):

1. **Латентность (Latency):** Отсутствует круговой путь в облако. Беспилотный автомобиль или промышленный робот не могут позволить себе задержку в 200 мс на отправку изображения на сервер AWS. MobileNetV2 обеспечивает реакцию почти в реальном времени.4
2. **Приватность (Privacy):** В здравоохранении или домашней безопасности отправка видеопотоков в облако является юридическим риском. Локальная обработка гарантирует, что изображения никогда не покидают устройство, обеспечивая соответствие нормам GDPR и HIPAA. Данные обрабатываются, извлекается метаинформация (класс объекта), а само изображение удаляется.28

### **5.3 Вертикальные сценарии использования**

1. **Ритейл (Управление запасами):** Камеры на полках идентифицируют уровни запасов (классификация изображений). Сдвиг домена здесь является проблемой, если меняется упаковка товара или освещение отличается от обучающего набора.28 Модели позволяют автоматизировать ресток и планограммы.
2. **Сельское хозяйство (Точное земледелие):** Дроны, оснащенные MobileNetV2, классифицируют культуры и сорняки или обнаруживают болезни листьев. Эффективность модели позволяет дронам летать дольше (меньше разряд батареи от вычислений). Модель должна адаптироваться к изменяющимся погодным условиям (Сдвиг домена).4
3. **Производство (Контроль качества):** Обнаружение дефектов на сборочных линиях. Модель должна быть тонко настроена на конкретное освещение и ракурсы заводского цеха, чтобы избежать ложных срабатываний. Здесь цена ошибки (сдвиг домена) — остановка конвейера или брак продукции.28

---

## **Часть VI: Будущее и заключение**

Квест 3.1 служит микрокосмом более широких проблем искусственного интеллекта. Написание скрипта для классификации изображения CIFAR-10 с помощью MobileNetV2 — это лишь техническая точка входа. Истинная глубина задачи заключается в понимании того, что «Всевидящее Око» не является универсально всеведущим; оно ограничено распределением данных, на которых оно было обучено.

Дивергенция между ImageNet (тренировочный полигон) и CIFAR-10 (тестовый полигон) иллюстрирует критическую концепцию **сдвига домена**. Несоответствие в разрешении ($224^2$ против $32^2$) и онтологии (1000 гранулярных классов против 10 агрегированных) требует надежного препроцессинга и, для оптимальной производительности, тонкой настройки (fine-tuning).

Для бизнеса ценностное предложение MobileNetV2 неоспоримо. Оно демократизирует компьютерное зрение, перемещая его из дорогих дата-центров на периферию сети — в телефоны, автомобили и поля. Однако успешное развертывание требует стратегического понимания ограничений модели. Будущее этой области лежит в дальнейшем развитии эффективных архитектур, таких как **TinyML** и адаптации **Vision Transformers (ViT)** (например, NaFlexViT) для мобильных устройств, которые обещают еще большую гибкость в обработке различных разрешений и доменов.32

### **Ключевые выводы**

| Концепция             | Описание                                                               | Значение для бизнеса                                                                          |
| :-------------------- | :--------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------- |
| **MobileNetV2**       | Эффективная CNN с использованием инвертированных остатков.             | Позволяет использовать ИИ на дешевом, маломощном оборудовании (Edge AI).                      |
| **Сдвиг домена**      | Несоответствие между обучающими данными (Source) и реальными (Target). | Основная причина провала проектов ИИ; требует мониторинга и переобучения.                     |
| **Потоковые данные**  | Загрузка наборов данных по запросу, а не скачивание целиком.           | Необходима для работы с Big Data пайплайнами и реальными потоками данных.                     |
| **Разрыв разрешения** | Использование high-res моделей на low-res данных (CIFAR).              | Требует тщательного препроцессинга (апскейлинг) или Fine-tuning во избежание потери точности. |

#### **Источники**

1. google/mobilenet_v2_1.0_224 \- Hugging Face, дата последнего обращения: ноября 28, 2025, [https://huggingface.co/google/mobilenet_v2_1.0_224](https://huggingface.co/google/mobilenet_v2_1.0_224)
2. MobileNetV2: Inverted Residuals and Linear Bottlenecks | IEEE Conference Publication, дата последнего обращения: ноября 28, 2025, [https://ieeexplore.ieee.org/document/8578572](https://ieeexplore.ieee.org/document/8578572)
3. A Summary of the “MobileNetV2: Inverted Residuals and Linear Bottlenecks” Paper | by Zaynab Awofeso | CodeX | Medium, дата последнего обращения: ноября 28, 2025, [https://medium.com/codex/a-summary-of-the-mobilenetv2-inverted-residuals-and-linear-bottlenecks-paper-e19b187cb78a](https://medium.com/codex/a-summary-of-the-mobilenetv2-inverted-residuals-and-linear-bottlenecks-paper-e19b187cb78a)
4. Exploring the Efficiency of Image Classification With MobileNetV2 \- Analytics Vidhya, дата последнего обращения: ноября 28, 2025, [https://www.analyticsvidhya.com/blog/2025/02/image-classification-with-mobilenetv2-model/](https://www.analyticsvidhya.com/blog/2025/02/image-classification-with-mobilenetv2-model/)
5. CIFAR-10 and CIFAR-100 datasets, дата последнего обращения: ноября 28, 2025, [https://www.cs.toronto.edu/\~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)
6. CIFAR-10 \- Wikipedia, дата последнего обращения: ноября 28, 2025, [https://en.wikipedia.org/wiki/CIFAR-10](https://en.wikipedia.org/wiki/CIFAR-10)
7. CIFAR10-DVS: An Event-Stream Dataset for Object Classification \- Frontiers, дата последнего обращения: ноября 28, 2025, [https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2017.00309/full](https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2017.00309/full)
8. ImageNet \- Wikipedia, дата последнего обращения: ноября 28, 2025, [https://en.wikipedia.org/wiki/ImageNet](https://en.wikipedia.org/wiki/ImageNet)
9. IMAGENET 1000 Class List \- WekaDeeplearning4j, дата последнего обращения: ноября 28, 2025, [https://deeplearning.cms.waikato.ac.nz/user-guide/class-maps/IMAGENET/](https://deeplearning.cms.waikato.ac.nz/user-guide/class-maps/IMAGENET/)
10. text: imagenet 1000 class idx to human readable labels (Fox, E., & Guestrin, C. (n.d.). Coursera Machine Learning Specialization.) · GitHub, дата последнего обращения: ноября 28, 2025, [https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a)
11. Quickstart \- Flower Datasets 0.5.0, дата последнего обращения: ноября 28, 2025, [https://flower.ai/docs/datasets/tutorial-quickstart.html](https://flower.ai/docs/datasets/tutorial-quickstart.html)
12. Load a Dataset in Streaming mode \- Hugging Face, дата последнего обращения: ноября 28, 2025, [https://huggingface.co/docs/datasets/v1.11.0/dataset_streaming.html](https://huggingface.co/docs/datasets/v1.11.0/dataset_streaming.html)
13. Stream \- Hugging Face, дата последнего обращения: ноября 28, 2025, [https://huggingface.co/docs/datasets/en/stream](https://huggingface.co/docs/datasets/en/stream)
14. Image Processor \- Hugging Face, дата последнего обращения: ноября 28, 2025, [https://huggingface.co/docs/transformers/main/main_classes/image_processor](https://huggingface.co/docs/transformers/main/main_classes/image_processor)
15. MobileNet V2 \- Hugging Face, дата последнего обращения: ноября 28, 2025, [https://huggingface.co/docs/transformers/en/model_doc/mobilenet_v2](https://huggingface.co/docs/transformers/en/model_doc/mobilenet_v2)
16. Best way to resize images for object recognition \- Google AI Developers Forum, дата последнего обращения: ноября 28, 2025, [https://discuss.ai.google.dev/t/best-way-to-resize-images-for-object-recognition/22139](https://discuss.ai.google.dev/t/best-way-to-resize-images-for-object-recognition/22139)
17. MobileNet V2 \- Hugging Face, дата последнего обращения: ноября 28, 2025, [https://huggingface.co/docs/transformers/model_doc/mobilenet_v2](https://huggingface.co/docs/transformers/model_doc/mobilenet_v2)
18. Stream — datasets 1.18.2 documentation \- Hugging Face, дата последнего обращения: ноября 28, 2025, [https://huggingface.co/docs/datasets/v1.18.2/stream.html](https://huggingface.co/docs/datasets/v1.18.2/stream.html)
19. дата последнего обращения: ноября 28, 2025, [https://medium.com/@bensalemh300/tackling-domain-shift-in-ai-a-deep-dive-into-domain-adaptation-c2debd758edd\#:\~:text=This%20problem%20is%20known%20as,9%E2%80%9D%20as%20%E2%80%9C8%E2%80%9D.](https://medium.com/@bensalemh300/tackling-domain-shift-in-ai-a-deep-dive-into-domain-adaptation-c2debd758edd#:~:text=This%20problem%20is%20known%20as,9%E2%80%9D%20as%20%E2%80%9C8%E2%80%9D.)
20. Domain Adaptation for Medical Image Analysis: A Survey \- PMC \- PubMed Central, дата последнего обращения: ноября 28, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC9011180/](https://pmc.ncbi.nlm.nih.gov/articles/PMC9011180/)
21. (PDF) Domain Shift and Robustness in Medical Imaging \- ResearchGate, дата последнего обращения: ноября 28, 2025, [https://www.researchgate.net/publication/397614658_Domain_Shift_and_Robustness_in_Medical_Imaging](https://www.researchgate.net/publication/397614658_Domain_Shift_and_Robustness_in_Medical_Imaging)
22. Tackling Domain Shift in AI: A Deep Dive into Domain Adaptation | by Houssem Ben Salem, дата последнего обращения: ноября 28, 2025, [https://medium.com/@bensalemh300/tackling-domain-shift-in-ai-a-deep-dive-into-domain-adaptation-c2debd758edd](https://medium.com/@bensalemh300/tackling-domain-shift-in-ai-a-deep-dive-into-domain-adaptation-c2debd758edd)
23. Domain Shifts in Machine Learning Based Covid-19 Diagnosis From Blood Tests \- PMC, дата последнего обращения: ноября 28, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC8960704/](https://pmc.ncbi.nlm.nih.gov/articles/PMC8960704/)
24. Applications of Computer Vision in Autonomous Vehicles: Methods, Challenges and Future Directions \- arXiv, дата последнего обращения: ноября 28, 2025, [https://arxiv.org/html/2311.09093v3](https://arxiv.org/html/2311.09093v3)
25. Beyond ImageNet: Understanding Cross-Dataset Robustness of Lightweight Vision Models, дата последнего обращения: ноября 28, 2025, [https://arxiv.org/html/2511.00335v1](https://arxiv.org/html/2511.00335v1)
26. Introducing Transfer Learning Fundamentals: CIFAR-10, MobileNetV2, Fine-Tuning, and Beyond | by Anaant Raj | GDGC VIT Bhopal | Medium, дата последнего обращения: ноября 28, 2025, [https://medium.com/dsc-vit-bhopal/introducing-transfer-learning-fundamentals-cifar-10-mobilenetv2-fine-tuning-and-beyond-d747f73f216c](https://medium.com/dsc-vit-bhopal/introducing-transfer-learning-fundamentals-cifar-10-mobilenetv2-fine-tuning-and-beyond-d747f73f216c)
27. How To Actually Fine-Tune MobileNetV2 | Classify 9 Fish Species \- YouTube, дата последнего обращения: ноября 28, 2025, [https://www.youtube.com/watch?v=9FMVlhOGDoo](https://www.youtube.com/watch?v=9FMVlhOGDoo)
28. AI Image Classification: Benefits, Use Cases & Top Tools \- Iflexion, дата последнего обращения: ноября 28, 2025, [https://www.iflexion.com/artificial-intelligence/image-classification](https://www.iflexion.com/artificial-intelligence/image-classification)
29. How Can Image Recognition Technology Be Used in Business? \- Chekkee, дата последнего обращения: ноября 28, 2025, [https://chekkee.com/image-recognition-technology/](https://chekkee.com/image-recognition-technology/)
30. Transparency Note and use cases for Image Analysis \- Azure AI services | Microsoft Learn, дата последнего обращения: ноября 28, 2025, [https://learn.microsoft.com/en-us/azure/ai-foundry/responsible-ai/computer-vision/image-analysis-transparency-note](https://learn.microsoft.com/en-us/azure/ai-foundry/responsible-ai/computer-vision/image-analysis-transparency-note)
31. Image classification \- Hugging Face, дата последнего обращения: ноября 28, 2025, [https://huggingface.co/docs/transformers/en/tasks/image_classification](https://huggingface.co/docs/transformers/en/tasks/image_classification)
32. huggingface/pytorch-image-models: The largest collection of PyTorch image encoders / backbones. Including train, eval, inference, export scripts, and pretrained weights \-- ResNet, ResNeXT, EfficientNet, NFNet, Vision Transformer (ViT), MobileNetV4, MobileNet-V3 & V2, RegNet, DPN, CSPNet, Swin \- GitHub, дата последнего обращения: ноября 28, 2025, [https://github.com/huggingface/pytorch-image-models](https://github.com/huggingface/pytorch-image-models)
