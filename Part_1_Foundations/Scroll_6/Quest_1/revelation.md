# **Экстракция акустической эссенции: Фундаментальный анализ архитектуры Wav2Vec 2.0 и парадигмы Transfer Learning в обработке аудиосигналов**

## **Глава 1\. Исполнительное резюме**

Современный ландшафт цифровой обработки сигналов (DSP) и обработки естественного языка (NLP) претерпевает тектонический сдвиг, обусловленный переходом от чисто контролируемого обучения (Supervised Learning) к самоконтролируемому (Self-Supervised Learning, SSL). Исторически сложилось так, что обучение машин "пониманию" аудио требовало создания гигантских, аннотированных вручную датасетов — процесс, который являлся ресурсоемким узким местом, сдерживающим инновации, особенно для языков с ограниченными ресурсами и узкоспециализированных акустических задач. Появление архитектуры **Wav2Vec 2.0**, разработанной исследователями Meta AI (ранее Facebook AI), знаменует собой критический поворотный момент в этой эволюции.1

Данный отчет представляет собой исчерпывающий технический анализ архитектуры Wav2Vec 2.0, с особым фокусом на механизме **извлечения эмбеддингов** — процессе преобразования непрерывных, высокоразмерных аудиоволн в компактные, информационно насыщенные векторы. В контексте поставленной задачи ("Квест 6.1") мы рассматриваем модель как "Духа-Эмпата", способного извлекать "эссенцию" голоса. Этот отчет демистифицирует этот процесс, переводя метафоры "магии" на строгий язык линейной алгебры, теории информации и нейросетевой архитектуры.

Мы исследуем теоретические основы этого ритуала **Transfer Learning**, где предобученный гигант (feature extractor) наделяет легковесные последующие модели (downstream models) способностью выполнять сложные классификации, такие как распознавание эмоций, идентификация диктора и детектирование звуковых событий. Анализируя слои модели — от сверточного кодировщика признаков (CNN) до контекстной сети на базе Transformer — мы даем точную интерпретацию 768-мерному вектору "эссенции". Мы также проводим сравнительный анализ Wav2Vec 2.0 с современными аналогами, такими как HuBERT и Whisper, оцениваем коммерческую жизнеспособность в граничных вычислениях (Edge AI) и аналитику колл-центров, а также подвергаем критическому анализу риски конфиденциальности, связанные с атаками инверсии модели.

Этот документ призван служить исчерпывающим руководством для исследователей и инженеров, стремящихся овладеть "магией" трансферного обучения в аудиодомене, и строго следует протоколу эксперимента, описанному в задании, расширяя его глубоким теоретическим обоснованием.

## ---

**Глава 2\. Теоретический фундамент самообучения аудиорепрезентациям**

Чтобы в полной мере оценить значимость извлечения "эссенции голоса", необходимо сначала понять теоретический ландшафт, который сделал этот подход необходимым. Традиционные системы автоматического распознавания речи (ASR) десятилетиями полагались на контролируемое обучение, где входные спектрограммы напрямую отображались на вероятности фонем или символов. Хотя этот подход эффективен, он чрезвычайно хрупок: он требует тысяч часов транскрибированного аудио и не способен к обобщению на задачи, выходящие за рамки транскрипции, такие как определение сарказма или идентификация говорящего.1

### **2.1. Проблема непрерывности данных**

В отличие от текста, который по своей природе дискретен (состоит из отдельных слов или символов, имеющих четкие границы), аудиосигнал является непрерывным. Звуковая волна представляет собой поток чисел с плавающей запятой, описывающих изменения давления воздуха во времени. Здесь нет четкой границы, где заканчивается одна фонема и начинается другая. Эта непрерывность создает фундаментальную проблему для целевых функций предварительного обучения на основе маскирования (подобных BERT в NLP), которые полагаются на скрытие "токена" и просьбу к модели предсказать его. Как можно замаскировать и предсказать непрерывный сигнал без заранее определенного словаря?

### **2.2. Контрастивное предсказательное кодирование (CPC)**

Wav2Vec 2.0 решает проблему непрерывности с помощью механизма, уходящего корнями в **Контрастивное Предсказательное Кодирование (Contrastive Predictive Coding)**. Вместо того чтобы пытаться предсказать точное числовое значение следующей миллисекунды аудио — что вычислительно расточительно, так как сигнал на микроуровне содержит много шума и стохастичности, — модель учится предсказывать _латентное представление_ будущего аудиосегмента.4

Наш "Дух-Эмпат" (Wav2Vec 2.0) обучается на контрастивной задаче: он должен идентифицировать истинный будущий аудиосегмент (positive sample) среди набора "дистракторов" (negative samples), взятых из других частей той же аудиозаписи. Это заставляет модель отбрасывать низкоуровневый шум (например, фоновое шипение или особенности канала записи) и сохранять высокоуровневую структурную информацию (например, фонетический контент, просодию и характеристики голоса), которая позволяет отличить истинный сегмент от ложного. Этот процесс эффективно создает "кодовую книгу" (codebook) речевых единиц, дискретизируя непрерывную волну в конечное множество представлений. Это аналогично тому, как младенец учится различать фонемы родного языка, еще не зная смысла слов.

### **2.3. Парадигма Transfer Learning: От гигантов к специалистам**

Извлечение эмбеддингов представляет собой основное ценностное предложение трансферного обучения. Предобучаясь на тысячах часов неразмеченного аудио (например, на наборе данных Librispeech 960h или на 53 000 часов для более крупных моделей, таких как Wav2Vec 2.0-Large-LV60), модель строит внутреннюю статистическую модель человеческой речи. Она изучает зависимости между звуками, ритмику речи и тембральные характеристики голосов.6

Когда мы выполняем "ритуал извлечения" — прогоняя прямой проход (forward pass) и забирая скрытые состояния (last_hidden_state), — мы используем это накопленное "знание". Мы не просим модель транскрибировать речь; мы просим её описать аудио, используя её выученный внутренний язык. Этот вектор, "эссенция", становится входными данными для последующих задач (downstream tasks), позволяя простому классификатору, обученному всего на нескольких минутах размеченных данных, достигать результатов уровня State-of-the-Art (SOTA). Эта эффективность и есть та самая "магия": тяжелая работа по пониманию структуры звука уже выполнена предобученным "духом".1

## ---

**Глава 3\. Анатомия "Духа-Эмпата": Архитектура Wav2Vec 2.0 Base**

Модель wav2vec2-base, используемая в нашем "квесте", представляет собой сложную нейросетевую архитектуру, состоящую из четко дифференцированных функциональных блоков. Понимание этих блоков необходимо для правильной интерпретации форм тензоров (shapes), с которыми мы сталкиваемся в коде.

### **3.1. Кодировщик признаков (Feature Encoder): "Абсолютный слух"**

Первым этапом конвейера является кодировщик признаков. Этот компонент принимает сырую волновую форму (одномерный массив значений float32) и обрабатывает её через стек многослойных сверточных нейронных сетей (CNN).

- **Структура:** Кодировщик состоит из 7 блоков временных сверток (temporal convolutions).2
- **Снижение размерности (Downsampling):** Сырое аудио, дискретизированное с частотой 16 000 Гц, имеет чрезвычайно высокую размерность (16 000 точек данных на секунду). CNN применяют шаг (stride) для прореживания этого сигнала. Общий шаг кодировщика составляет 320, что вычисляется как произведение шагов по всем слоям: $(5 \\times 2 \\times 2 \\times 2 \\times 2 \\times 2 \\times 2\) \= 320$.7
- **Временное разрешение:** При частоте 16 кГц, 320 сэмплов соответствуют ровно 20 миллисекундам ($320 / 16000 \= 0.02$ с). Таким образом, кодировщик признаков выдает один вектор признаков на каждые 20 мс аудио.
- **Выход:** Кодировщик генерирует латентные представления речи $Z \= z\_1, z\_2, \\dots, z\_T$. Эти векторы являются относительно низкоуровневыми, фиксируя акустические свойства, такие как высота тона, частотные форманты и энергия сигнала, на локальном уровне.

### **3.2. Контекстная сеть (Context Network): "Разум"**

Представления $Z$ от кодировщика поступают в контекстную сеть, которая представляет собой архитектуру Transformer (аналогичную BERT).

- **Конфигурация (Base Model):** Модель содержит 12 блоков Трансформера, размерность модели ($d\_{model}$) составляет 768, количество голов внимания — 12, а размерность промежуточного полносвязного слоя (feed-forward) — 3072\.8
- **Позиционное кодирование:** В отличие от стандартных Трансформеров, использующих фиксированные синусоидальные кодировки, Wav2Vec 2.0 использует сверточный слой для изучения относительной позиционной информации. Это позволяет модели понимать последовательность звуков независимо от их абсолютной позиции в файле, что критично для аудио, где фраза может начаться в любой момент.1
- **Механизм внимания:** Слои Трансформера применяют механизм самовнимания (Self-Attention), позволяя каждому 20-миллисекундному временному шагу "обращать внимание" на любой другой шаг в последовательности. Это агрегирует информацию по всему аудиоклипу. Звук в начале предложения может влиять на интерпретацию звука в конце (например, интонационный паттерн, указывающий на вопрос).
- **Выход:** Финальным выходом является Контекстуальное Представление $C \= c\_1, c\_2, \\dots, c\_T$. Каждый вектор $c\_t$ — это 768-мерный эмбеддинг, представляющий $t$-й фрейм (20 мс), но обогащенный информацией из всего контекста высказывания.

### **3.3. Модуль квантования (Quantization Module): "Переводчик"**

Хотя модуль квантования используется в основном во время предобучения для вычисления контрастивной потери, его наличие фундаментально для понимания "логики" модели. Он дискретизирует выходы кодировщика $Z$ в конечное множество записей из кодовой книги (codebook). Wav2Vec 2.0 использует _product quantization_ (квантование произведений), выбирая записи из нескольких кодовых книг и конкатенируя их. Это заставляет модель принимать дискретные решения о том, что она "слышит", подталкивая представления к дискретным лингвистическим единицам (фонемам), вместо того чтобы оставаться в непрерывном акустическом "размытии".1

## ---

**Глава 4\. Ритуал Извлечения: Методология и Реализация (Анализ Квеста 6.1)**

Практическое выполнение извлечения эмбеддингов требует точного соблюдения последовательности операций и понимания нюансов программного обеспечения. В этом разделе мы подробно разбираем этапы "Квеста 6.1", объясняя технические причины каждого шага.

### **4.1. Гармонизация Гримуаров: Управление зависимостями**

В легенде квеста указано требование: pip install datasets==2.16.1. Почему именно эта версия?

- **Проблема удаленного кода:** Библиотека datasets от Hugging Face претерпела значительные изменения в политике безопасности. Начиная с версии 2.16.0 и выше, загрузка наборов данных, которые используют пользовательские скрипты (а librispeech_asr относится к таким, так как требует сложной логики скачивания и распаковки архивов), требует явного разрешения на выполнение удаленного кода.
- **Параметр trust_remote_code=True:** В квесте используется этот флаг. Без него современные версии библиотеки (включая 2.16.1) выбросят исключение FutureWarning или ошибку, блокируя загрузку данных.9 Это мера защиты от потенциально вредоносных скриптов, которые могут быть встроены в репозитории на Hugging Face Hub. Использование фиксированной версии гарантирует воспроизводимость эксперимента в условиях меняющегося API.

### **4.2. Призыв Аудио-Послания: Работа с Librispeech**

Мы используем сплит test.clean из корпуса librispeech_asr.

- **Характеристика данных:** "Clean" означает, что аудиозаписи взяты из аудиокниг (проект LibriVox), начитанных дикторами с четкой артикуляцией и минимальным уровнем фонового шума. Средний Word Error Rate (WER) на этом подмножестве у современных моделей составляет менее 2-3%.12
- **Потоковая передача (streaming=True):** Использование потокового режима критически важно для работы с большими аудио-корпусами. Вместо того чтобы скачивать и распаковывать сотни гигабайт данных на диск (архив test-clean весит около 346 МБ, но полный тренировочный сет — сотни ГБ), мы загружаем и декодируем аудио "на лету". Это позволяет запускать "ритуал" даже на машинах с ограниченным дисковым пространством.9

### **4.3. Подготовка звука: Роль Wav2Vec2FeatureExtractor**

Прежде чем сырой звук попадет в модель, он должен пройти через Wav2Vec2FeatureExtractor.

- **Ресемплинг (Resampling):** Модель wav2vec2-base была обучена на аудио с частотой дискретизации 16 кГц. Если подать на вход аудио с частотой 44.1 кГц или 48 кГц без ресемплинга, спектральные характеристики будут искажены (эффект "замедленного воспроизведения" или сдвига формант), и модель выдаст бессмысленный результат. Экстрактор гарантирует соответствие частот.7
- **Нормализация (Normalization):** Параметр do_normalize=True (по умолчанию) нормализует волновую форму так, чтобы она имела нулевое среднее и единичную дисперсию (zero mean, unit variance). Это критично для стабильности активаций нейронной сети, так как амплитуда записи может варьироваться в зависимости от микрофона и условий записи. Без нормализации громкая запись могла бы вызвать насыщение нейронов, а тихая — затухание сигнала.15

### **4.4. Анализ Тензоров: Декодирование "Ауры"**

В результате выполнения кода мы получаем тензор voice_essence (это last_hidden_state). Рассмотрим его форму, упомянутую в квесте: torch.Size().

#### **4.4.1. Размерность пакета (Batch Size): 1**

Первое измерение равно 1, так как мы обрабатываем один аудиофайл.

#### **4.4.2. Длина последовательности (Sequence Length): 175**

Откуда берется число 175? Это ключевой момент для понимания временного разрешения модели.

- Мы знаем, что шаг (stride) модели составляет 320 сэмплов (20 мс) при 16 кГц.
- Если длина последовательности токенов равна 175, это означает, что модель "видит" 175 временных окон.
- Общая длительность аудиозаписи, которая породила этот тензор, может быть оценена как: $175 \\times 0.02 \\text{ сек} \= 3.5 \\text{ сек}$.
- Точная формула свертки для расчета выходного размера: $L\_{out} \= \\lfloor \\frac{L\_{in} \- K}{S} \\rfloor \+ 1$. Однако для Wav2Vec2 упрощенная аппроксимация (1 вектор на 20 мс) является достаточно точной для практических целей.7
- Этот тензор представляет собой **"Ауру во времени"**. Он показывает, как меняется состояние модели по мере развертывания речи — начало гласной, пауза между словами, повышение тона в конце вопроса.

#### **4.4.3. Размерность признаков (Hidden Dimension): 768**

Это ширина слоев Трансформера в модели Base. Каждый из 175 фреймов описывается вектором из 768 чисел с плавающей запятой. Именно эти числа и есть искомая "эссенция".

#### **4.4.4. Единый отпечаток (Aggregated Essence): torch.Size()**

Для задач классификации всего клипа (например, "Кто этот диктор?") нам нужен один вектор, а не последовательность. В квесте используется операция **Mean Pooling** (.mean(dim=1)), которая усредняет 768-мерные векторы по временной оси.

- **Результат:** Один вектор размером 768\. Это "центр масс" аудиоклипа в многомерном латентном пространстве модели.
- **Альтернативы:** В более сложных системах вместо среднего используют **Attention Pooling** (взвешенное среднее, где модель учит, какие моменты важнее) или используют первый токен (аналог \`\` в BERT), хотя Wav2Vec2 не имеет явного CLS-токена при предобучении.17

## ---

**Глава 5\. "Язык Чувств" для машин: Декодирование 768 чисел**

Вопрос Техноманта в квесте — _"Что означают эти числа?"_ — затрагивает суть интерпретируемости нейросетей. Ответ "Нет, мы не можем понять их напрямую" верен, но требует уточнения через концепцию **Латентной Семантики**.

### **5.1. Геометрия Смысла**

768 чисел — это координаты точки в 768-мерном гиперпространстве.

- **Кластеризация:** В этом пространстве все аудиозаписи "сердитых мужчин" будут группироваться в одном кластере, а "смеющихся детей" — в другом. "Близость" в этом пространстве (измеряемая, например, косинусным расстоянием) отражает семантическую или акустическую схожесть.19
- **Запутанность (Entanglement):** Отдельные измерения (например, нейрон №42) редко отвечают за одно понятное свойство (например, "громкость"). Информация "размазана" по всему вектору. Однако методы анализа, такие как PCA (метод главных компонент), могут найти направления в этом пространстве, которые коррелируют с полом говорящего, эмоцией или фонетическим содержанием.

### **5.2. Иерархия абстракций по слоям**

Исследования с использованием "зондирования" (probing) слоев Wav2Vec 2.0 показывают четкую иерархию 20:

1. **Нижние слои (1-4):** Действуют как "слуховой нерв". Они кодируют сырые акустические признаки: высоту тона (F0), форманты, наличие голоса (VAD). Они чувствительны к шуму и каналу записи.
2. **Средние слои (5-8):** Действуют как "языковые центры". Здесь происходит переход от акустики к фонетике. Модель начинает различать гласные и согласные, формируя представления, близкие к фонемам. Для задач распознавания речи это самые информативные слои.
3. **Верхние слои (9-12):** Кодируют семантику и синтаксис. Они улавливают зависимости на длинных дистанциях (например, согласование слов). Примечательно, что для задач типа идентификации диктора верхние слои могут быть _менее_ полезны, так как модель "учится" игнорировать индивидуальные особенности голоса в пользу лингвистического содержания (инвариантность к диктору).

**Инсайт для бизнеса:** Если ваша задача — **Идентификация Диктора**, использование эмбеддингов с _средних_ слоев часто дает лучший результат, чем с последнего. Если задача — **Распознавание эмоций**, важна комбинация просодии (средние слои) и семантики (верхние слои).23

## ---

**Глава 6\. Бизнес-ценность и Практическое Применение**

Подход, продемонстрированный в квесте, является фундаментом современной аудио-аналитики. Вместо обучения с нуля (Tabula Rasa), мы используем Transfer Learning.

### **6.1. Сценарий 1: Распознавание эмоций в голосе (SER)**

- **Проблема:** Оценка удовлетворенности клиентов в колл-центрах.
- **Решение:** Извлекаем эмбеддинги Wav2Vec2. Обучаем простой классификатор (Linear Layer) отображать вектор 768 \-\> 4 класса (Нейтрально, Радость, Гнев, Печаль).
- **Преимущество:** Традиционные методы требовали ручного расчета MFCC, джиттера и шиммера. Wav2Vec2 извлекает эти признаки автоматически и контекстуализирует их. Тесты на датасете RAVDESS показывают, что этот подход превосходит классические методы на 10-15% точности.24

### **6.2. Сценарий 2: Классификация звуковых событий**

- **Проблема:** Детектирование аварийных ситуаций (разбитое стекло, выстрел, крик) в системах городской безопасности.
- **Нюанс:** Хотя Wav2Vec2 обучен на речи, его нижние слои выучивают универсальные слуховые фильтры (аналогичные фильтрам Габора), которые применимы и к неречевым звукам. Это позволяет использовать модель для задач общего аудио-мониторинга.

### **6.3. Сценарий 3: Идентификация диктора (Speaker Verification)**

- **Метод:** Использование косинусной близости между эмбеддингами.
- **Сравнение:** Здесь Wav2Vec2 сталкивается с конкуренцией. Модели, специально обученные для верификации (как **WavLM** или **TitaNet**), часто превосходят его, так как Wav2Vec2 стремится "забыть" диктора ради текста, а WavLM обучается сохранять эту информацию.26

## ---

**Глава 7\. Сравнительный анализ в современной экосистеме**

Как "Дух-Эмпат" смотрится на фоне новых сущностей?

### **7.1. Wav2Vec 2.0 против HuBERT**

- **Механизм:** Wav2Vec 2.0 использует контрастивное обучение. **HuBERT** (Hidden Unit BERT) использует итеративную кластеризацию (k-means) для создания псевдо-меток.
- **Результат:** HuBERT обычно показывает лучшие результаты на задачах распознавания речи (ASR) и семантического понимания, так как кластеризация заставляет модель формировать более четкие фонетические категории. Однако Wav2Vec 2.0 остается конкурентоспособным и иногда более стабильным на задачах с шумными данными.28

### **7.2. Wav2Vec 2.0 против OpenAI Whisper**

- **Парадигма:** Whisper — это **Supervised** модель (обучена на 680 тыс. часов размеченных пар аудио-текст). Wav2Vec 2.0 — **Self-Supervised** (неразмеченное аудио).
- **Эмбеддинги:** Whisper превосходен для транскрипции "из коробки". Но как _извлекатель признаков_ для задач, не связанных с текстом (например, эмоции), он может уступать SSL-моделям. Энкодер Whisper жестко "заточен" под текст и может отбрасывать паралингвистическую информацию (интонацию), которую Wav2Vec 2.0 сохраняет.
- **Латентность:** Модели Whisper (особенно Large) тяжелее и медленнее в инференсе, чем энкодер Wav2Vec 2.0 Base, что делает последний предпочтительным для real-time приложений.30

## ---

**Глава 8\. Граничные вычисления и Оптимизация (Edge AI)**

Можем ли мы поместить "Духа" в маленькую шкатулку (Raspberry Pi)?

### **8.1. Квантование и производительность**

Оригинальная модель весит около 360 МБ (FP32). Применив **квантование** (перевод весов в INT8), размер можно сократить до \~90 МБ.

- **Raspberry Pi 4:** Эксперименты показывают, что квантованная версия Wav2Vec 2.0 может работать на RPi 4 с приемлемой задержкой (Real Time Factor \~0.3-0.5), что позволяет создавать автономные устройства голосового управления без отправки данных в облако.32

## ---

**Глава 9\. Безопасность и Приватность: Темная сторона магии**

Существует опасное заблуждение, что превращение голоса в вектор (эмбеддинг) анонимизирует его.

### **9.1. Атаки инверсии модели (Model Inversion)**

Исследования показывают, что можно обучить нейросеть-декодер, которая восстановит исходное аудио из 768-мерных векторов с пугающей точностью.

- **Угроза:** Злоумышленник, перехвативший "анонимизированные" векторы, может восстановить голос пользователя, его акцент и содержание разговора.35
- **Причина:** Эмбеддинги (особенно с нижних слоев) сохраняют достаточно акустической информации для реконструкции спектрограммы.
- **Защита:** Для истинной приватности необходимо использовать методы **Adversarial Regularization** при обучении, чтобы явно "наказывать" модель за сохранение информации о личности диктора, оставляя только лингвистическую суть.

## ---

**Глава 10\. Заключение**

Ритуал извлечения эмбеддингов из wav2vec2-base — это не просто упражнение в написании кода на PyTorch; это демонстрация фундаментального сдвига в искусственном интеллекте. Мы перешли от ручного конструирования признаков к использованию универсальных предобученных репрезентаций.

768 чисел, полученных в ходе квеста, представляют собой сжатую, многомерную проекцию физической реальности звука. Они отсекают тишину и нерелевантный шум, оставляя математический слепок фонетической и просодической идентичности высказывания.

**Ключевые выводы:**

1. **Эффективность данных:** Transfer Learning позволяет решать задачи аудио-аналитики, имея в 100 раз меньше размеченных данных, чем требовалось ранее.
2. **Глубина понимания:** Слои модели образуют иерархию от акустики к семантике, и выбор слоя должен зависеть от бизнес-задачи.
3. **Универсальность и Риск:** Та же "эссенция", что позволяет распознавать эмоции, может быть использована для реконструкции голоса, что требует строгих протоколов безопасности данных.

### **Табличные данные**

#### **Таблица 1\. Спецификации моделей семейства Wav2Vec 2.0**

| Параметр                            | Wav2Vec 2.0 Base     | Wav2Vec 2.0 Large              |
| :---------------------------------- | :------------------- | :----------------------------- |
| **Количество слоев (Transformer)**  | 12                   | 24                             |
| **Размерность модели (Hidden Dim)** | 768                  | 1024                           |
| **Головы внимания**                 | 12                   | 16                             |
| **Количество параметров**           | \~95 млн             | \~317 млн                      |
| **Данные предобучения**             | Librispeech (960 ч)  | Libri-Light (60 000 ч)         |
| **Сценарий использования**          | Real-time, Edge, ASR | High-accuracy ASR, Server-side |

#### **Таблица 2\. Сравнение производительности (Word Error Rate на Librispeech Test-Clean)**

| Модель                    | Объем размеченных данных | WER (%) |
| :------------------------ | :----------------------- | :------ |
| **Wav2Vec 2.0 Base**      | 10 мин                   | 4.8     |
| **Wav2Vec 2.0 Large**     | 10 мин                   | 2.9     |
| **Wav2Vec 2.0 Large**     | 100 часов                | 1.8     |
| **DeepSpeech (Baseline)** | 960 часов                | \~3.8   |

Примечание: Данные таблицы 2 демонстрируют, что Wav2Vec 2.0, используя всего 10 минут (\!) размеченных данных, приближается к результатам моделей, обученных на тысячах часов, благодаря качеству извлекаемых эмбеддингов.1

#### **Источники**

1. wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations \- NIPS papers, дата последнего обращения: декабря 7, 2025, [https://proceedings.neurips.cc/paper/2020/file/92d1e1eb1cd6f9fba3227870bb6d7f07-Paper.pdf](https://proceedings.neurips.cc/paper/2020/file/92d1e1eb1cd6f9fba3227870bb6d7f07-Paper.pdf)
2. Brief Review — wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations \- Sik-Ho Tsang, дата последнего обращения: декабря 7, 2025, [https://sh-tsang.medium.com/brief-review-wav2vec-2-0-a-framework-for-self-supervised-learning-of-speech-representations-9b9a8fdab85e](https://sh-tsang.medium.com/brief-review-wav2vec-2-0-a-framework-for-self-supervised-learning-of-speech-representations-9b9a8fdab85e)
3. wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations \- arXiv, дата последнего обращения: декабря 7, 2025, [https://arxiv.org/abs/2006.11477](https://arxiv.org/abs/2006.11477)
4. Wav2Vec2: Self-A Supervised Learning Technique for Speech Representations \- GeeksforGeeks, дата последнего обращения: декабря 7, 2025, [https://www.geeksforgeeks.org/nlp/wav2vec2-self-a-supervised-learning-technique-for-speech-representations/](https://www.geeksforgeeks.org/nlp/wav2vec2-self-a-supervised-learning-technique-for-speech-representations/)
5. An Illustrated Tour of Wav2vec 2.0 | Jonathan Bgn, дата последнего обращения: декабря 7, 2025, [https://jonathanbgn.com/2021/09/30/illustrated-wav2vec-2.html](https://jonathanbgn.com/2021/09/30/illustrated-wav2vec-2.html)
6. facebook/wav2vec2-base \- Hugging Face, дата последнего обращения: декабря 7, 2025, [https://huggingface.co/facebook/wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base)
7. Wav2Vec2 \- Hugging Face, дата последнего обращения: декабря 7, 2025, [https://huggingface.co/docs/transformers/en/model_doc/wav2vec2](https://huggingface.co/docs/transformers/en/model_doc/wav2vec2)
8. Wav2Vec2 \- Hugging Face, дата последнего обращения: декабря 7, 2025, [https://huggingface.co/docs/transformers/model_doc/wav2vec2](https://huggingface.co/docs/transformers/model_doc/wav2vec2)
9. Unable to Load Dataset Using \`load_dataset\` \- Hugging Face Forums, дата последнего обращения: декабря 7, 2025, [https://discuss.huggingface.co/t/unable-to-load-dataset-using-load-dataset/144579](https://discuss.huggingface.co/t/unable-to-load-dataset-using-load-dataset/144579)
10. DanielPFlorian/Hugging-Face-Datasets-Github-Issues, дата последнего обращения: декабря 7, 2025, [https://huggingface.co/datasets/DanielPFlorian/Hugging-Face-Datasets-Github-Issues](https://huggingface.co/datasets/DanielPFlorian/Hugging-Face-Datasets-Github-Issues)
11. Use trust_remote_code for dataset load · Issue \#176 \- GitHub, дата последнего обращения: декабря 7, 2025, [https://github.com/bigcode-project/bigcode-evaluation-harness/issues/176](https://github.com/bigcode-project/bigcode-evaluation-harness/issues/176)
12. LibriSpeech Dataset, дата последнего обращения: декабря 7, 2025, [https://datasets.activeloop.ai/docs/ml/datasets/librispeech-dataset/](https://datasets.activeloop.ai/docs/ml/datasets/librispeech-dataset/)
13. openslr/librispeech_asr · Datasets at Hugging Face, дата последнего обращения: декабря 7, 2025, [https://huggingface.co/datasets/openslr/librispeech_asr](https://huggingface.co/datasets/openslr/librispeech_asr)
14. Fine-tuning Wav2Vec2 for English ASR with Transformers \- Colab, дата последнего обращения: декабря 7, 2025, [https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_tuning_Wav2Vec2_for_English_ASR.ipynb](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_tuning_Wav2Vec2_for_English_ASR.ipynb)
15. hfblog/fine-tune-wav2vec2-english.md at main \- GitHub, дата последнего обращения: декабря 7, 2025, [https://github.com/vwxyzjn/hfblog/blob/main/fine-tune-wav2vec2-english.md](https://github.com/vwxyzjn/hfblog/blob/main/fine-tune-wav2vec2-english.md)
16. Working With Wav2vec2 Part 1: Finetuning XLS-R for Automatic Speech Recognition, дата последнего обращения: декабря 7, 2025, [https://hackernoon.com/working-with-wav2vec2-part-1-finetuning-xls-r-for-automatic-speech-recognition](https://hackernoon.com/working-with-wav2vec2-part-1-finetuning-xls-r-for-automatic-speech-recognition)
17. Attentive Statistics Pooling for Deep Speaker Embedding \- ISCA Archive, дата последнего обращения: декабря 7, 2025, [https://www.isca-archive.org/interspeech_2018/okabe18_interspeech.pdf](https://www.isca-archive.org/interspeech_2018/okabe18_interspeech.pdf)
18. Improving Out-of-Domain Audio Deepfake Detection via Layer Selection and Fusion of SSL-Based Countermeasures \- arXiv, дата последнего обращения: декабря 7, 2025, [https://arxiv.org/html/2509.12003v1](https://arxiv.org/html/2509.12003v1)
19. What is Vector Embedding? | IBM, дата последнего обращения: декабря 7, 2025, [https://www.ibm.com/think/topics/vector-embedding](https://www.ibm.com/think/topics/vector-embedding)
20. Layer-Stratified Wav2Vec2 Representations \- Emergent Mind, дата последнего обращения: декабря 7, 2025, [https://www.emergentmind.com/topics/layer-stratified-wav2vec2-representations](https://www.emergentmind.com/topics/layer-stratified-wav2vec2-representations)
21. ankitapasad/layerwise-analysis: Layer-wise analysis of self-supervised pre-trained speech representations \- GitHub, дата последнего обращения: декабря 7, 2025, [https://github.com/ankitapasad/layerwise-analysis](https://github.com/ankitapasad/layerwise-analysis)
22. What Do Self-Supervised Speech Models Know About Words? \- MIT Press Direct, дата последнего обращения: декабря 7, 2025, [https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00656/120586/What-Do-Self-Supervised-Speech-Models-Know-About](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00656/120586/What-Do-Self-Supervised-Speech-Models-Know-About)
23. Comprehensive Layer-wise Analysis of SSL Models for Audio Deepfake Detection \- arXiv, дата последнего обращения: декабря 7, 2025, [https://arxiv.org/html/2502.03559v2](https://arxiv.org/html/2502.03559v2)
24. (PDF) Unveiling embedded features in Wav2vec2 and HuBERT msodels for Speech Emotion Recognition \- ResearchGate, дата последнего обращения: декабря 7, 2025, [https://www.researchgate.net/publication/379129098_Unveiling_embedded_features_in_Wav2vec2_and_HuBERT_msodels_for_Speech_Emotion_Recognition](https://www.researchgate.net/publication/379129098_Unveiling_embedded_features_in_Wav2vec2_and_HuBERT_msodels_for_Speech_Emotion_Recognition)
25. \[PDF\] A Fine-tuned Wav2vec 2.0/HuBERT Benchmark For Speech Emotion Recognition, Speaker Verification and Spoken Language Understanding | Semantic Scholar, дата последнего обращения: декабря 7, 2025, [https://www.semanticscholar.org/paper/A-Fine-tuned-Wav2vec-2.0-HuBERT-Benchmark-For-and-Wang-Boumadane/86f389b5cbec5cb06575d04f4d35eaaaf30ec1da](https://www.semanticscholar.org/paper/A-Fine-tuned-Wav2vec-2.0-HuBERT-Benchmark-For-and-Wang-Boumadane/86f389b5cbec5cb06575d04f4d35eaaaf30ec1da)
26. WavLM model ensemble for audio deepfake detection \- arXiv, дата последнего обращения: декабря 7, 2025, [https://arxiv.org/html/2408.07414v1](https://arxiv.org/html/2408.07414v1)
27. Comparison of Modern Deep Learning Models for Speaker Verification \- MDPI, дата последнего обращения: декабря 7, 2025, [https://www.mdpi.com/2076-3417/14/4/1329](https://www.mdpi.com/2076-3417/14/4/1329)
28. Advancing Speech Emotion Recognition \- DiVA portal, дата последнего обращения: декабря 7, 2025, [http://www.diva-portal.org/smash/get/diva2:1886207/FULLTEXT01.pdf](http://www.diva-portal.org/smash/get/diva2:1886207/FULLTEXT01.pdf)
29. Comparing Audio Transformers: Hubert vs Wav2Vec vs WavLM vs Data2Vec Performance Analysis | AI Builders Hub, дата последнего обращения: декабря 7, 2025, [https://www.aibuildershub.ai/blog/comparing-audio-transformers-hubert-vs-wav2vec-vs-wavlm-vs-data2vec-performance-analysis-muratozturk-20250610-132239](https://www.aibuildershub.ai/blog/comparing-audio-transformers-hubert-vs-wav2vec-vs-wavlm-vs-data2vec-performance-analysis-muratozturk-20250610-132239)
30. Comparison between the results obtained using Wav2Vec2.0 and Whisper for ASR (left) and KWS (right). … \- ResearchGate, дата последнего обращения: декабря 7, 2025, [https://www.researchgate.net/figure/Comparison-between-the-results-obtained-using-Wav2Vec20-and-Whisper-for-ASR-left-and_fig5_368378522](https://www.researchgate.net/figure/Comparison-between-the-results-obtained-using-Wav2Vec20-and-Whisper-for-ASR-left-and_fig5_368378522)
31. Benchmarking Open Source Speech Recognition in 2025: Whisper vs. wav2vec2 vs. Kaldi, дата последнего обращения: декабря 7, 2025, [https://graphlogic.ai/blog/ai-trends-insights/voice-technology-trends/benchmarking-top-open-source-speech-recognition-models-whisper-facebook-wav2vec2-and-kaldi/](https://graphlogic.ai/blog/ai-trends-insights/voice-technology-trends/benchmarking-top-open-source-speech-recognition-models-whisper-facebook-wav2vec2-and-kaldi/)
32. (PDF) Wav2Vec2.0 on the Edge: Performance Evaluation \- ResearchGate, дата последнего обращения: декабря 7, 2025, [https://www.researchgate.net/publication/358603972_Wav2Vec20_on_the_Edge_Performance_Evaluation](https://www.researchgate.net/publication/358603972_Wav2Vec20_on_the_Edge_Performance_Evaluation)
33. \[2202.05993\] Wav2Vec2.0 on the Edge: Performance Evaluation \- arXiv, дата последнего обращения: декабря 7, 2025, [https://arxiv.org/abs/2202.05993](https://arxiv.org/abs/2202.05993)
34. Performance Evaluation of Offline Speech Recognition on Edge Devices \- Semantic Scholar, дата последнего обращения: декабря 7, 2025, [https://pdfs.semanticscholar.org/f06d/0182f779f496416e47625f4546d0002d2040.pdf](https://pdfs.semanticscholar.org/f06d/0182f779f496416e47625f4546d0002d2040.pdf)
35. \[2301.03206\] Introducing Model Inversion Attacks on Automatic Speaker Recognition \- arXiv, дата последнего обращения: декабря 7, 2025, [https://arxiv.org/abs/2301.03206](https://arxiv.org/abs/2301.03206)
36. How to Recover Long Audio Sequences Through Gradient Inversion Attack With Dynamic Segment-based Reconstruction \- ISCA Archive, дата последнего обращения: декабря 7, 2025, [https://www.isca-archive.org/interspeech_2025/zeng25_interspeech.pdf](https://www.isca-archive.org/interspeech_2025/zeng25_interspeech.pdf)
