# **Экспертный Отчет: Оптимизация Классификатора Намерения в Режиме Transfer Learning**

## **I. Исполнительное Резюме: Стратегическая Ценность Архитектурной Оптимизации в Условиях Ограничений**

### **1.1 Контекст: Завершение Этапа Transfer Learning 6.2**

Начальный этап разработки системы классификации намерения, основанный на использовании простого линейного слоя (nn.Linear) поверх замороженных эмбеддингов Wav2Vec2, успешно подтвердил принципиальную эффективность подхода Transfer Learning \[Quest Legend\]. Полученные результаты свидетельствуют о том, что предварительно обученная модель, выступающая в роли извлекателя признаков, формирует "эссенции голоса", которые уже обладают высокой семантической дискриминационной способностью, релевантной задаче классификации 14 финансовых намерений, определенных в наборе данных MInDS-14.  
Текущая задача заключается в переходе от минимального линейного классификатора к более мощной, нелинейной системе — Многослойному Перцептрону (MLP). Целью этого перехода является устранение математических ограничений линейности и достижение максимально возможной точности классификации за счет моделирования сложных, нелинейных взаимосвязей в 768-мерном пространстве признаков \[Quest Legend\].

### **1.2 Необходимость MLP и Риск Переобучения**

Хотя простая линейная модель доказала свою состоятельность, она ограничена способностью моделировать только линейно разделимые классы. Введение MLP, состоящего из нескольких линейных слоев, разделенных нелинейными функциями активации (таких как ReLU), теоретически позволяет создать универсальный аппроксиматор функций, способный улавливать более тонкие и неаддитивные зависимости в голосовых эмбеддингах. Это необходимо для снижения _смещения_ (underfitting), которое неизбежно присутствует в линейном классификаторе.  
Однако расширение архитектуры классификатора приводит к резкому увеличению числа обучаемых параметров. Для рекомендованной структуры MLP количество параметров может достичь \\approx 200,000. В контексте экстремального малого набора обучающих данных (N=50) это создает катастрофический риск _переобучения_ (overfitting), или высокого _разброса_ (high variance), где модель вместо обобщения начинает запоминать шум в тренировочных примерах.

### **1.3 Основная Рекомендация**

Для достижения баланса между емкостью модели (для снижения смещения) и риском переобучения (для управления разбросом) рекомендована следующая стратегия:  
Оптимальная производительность будет достигнута при использовании неглубокого, но достаточного по ширине MLP (например, архитектура 768 \\rightarrow 256 \\rightarrow 14), дополненного _строгими_ протоколами регуляризации. Эти протоколы включают: агрессивный стохастический Dropout (p \\ge 0.4), L2-регуляризацию (уменьшение весов) и обязательное применение Ранней Остановки (Early Stopping) на стабильно измеряемом валидационном наборе, полученном методом K-Fold кросс-валидации.

### **1.4 Согласование с Бизнес-Ценностью (MVP)**

Необходимо признать, что хотя MLP повышает теоретический предел точности, он также увеличивает сложность разработки, время на настройку гиперпараметров и немного увеличивает задержку инференса. Подход "золотого стандарта" для MVP, как описано в легенде квеста, фокусируется на быстрых и дешевых решениях. Переход к MLP оправдан только в том случае, если линейный классификатор не достигает требуемого уровня точности, и если ожидаемый маргинальный прирост метрики (например, с 95% до 97%) критически важен для минимально жизнеспособного продукта.

## **II. Фундаментальный Анализ: Пространство Признаков и Ограничение Линейности**

### **2.1 Деконструкция «Эссенций Голоса» Wav2Vec2**

Модель Wav2Vec2, являясь мощным трансформерным кодировщиком, предварительно обученным без учителя на огромных объемах речевых данных, выполняет роль высококачественного извлекателя признаков. Особенностью данной архитектуры является то, что ее нижние слои (CNN-кодировщик и, возможно, часть трансформера) замораживаются, и их задача — генерировать контекстуализированные латентные представления, часто называемые «речевыми единицами» (speech units). Эти представления передают богатую информацию о фонемах и акустических характеристиках речи.  
Ключевым параметром является размерность входного вектора. Для Wav2Vec2-Base или аналогичных моделей, размерность скрытых слоев (d\_{\\text{model}}) обычно составляет 768\. Следовательно, каждый временной шаг аудиосигнала представлен 768-мерным вектором.  
Поскольку задача классификации намерения (MInDS-14) является задачей классификации _последовательности_ (Sequence Classification), а не пошаговой классификацией, последовательность признаков переменной длины, полученная от Wav2Vec2, должна быть агрегирована (усреднена) в единый фиксированный вектор размерностью 768\. Стандартным подходом является Mean Pooling (усреднение по времени) или использование выходных данных специального токена пула, который затем подается на классификационную голову.

### **2.2 Целевое Пространство MInDS-14**

Набор данных MInDS-14 предназначен для обнаружения намерения в разговорных данных, относящихся к сфере электронного банкинга. Он включает 14 различных классов намерения. В результате, классификационная голова должна преобразовать входной 768-мерный вектор в 14 выходных скалярных значений (логитов), соответствующих каждому классу. Для получения вероятностного распределения по классам, эти логиты обычно обрабатываются функцией активации Softmax.

### **2.3 Математическое Ограничение Линейного Перцептрона**

Простой слой nn.Linear (одномерный перцептрон) выполняет классификацию, которая по своей природе является _линейной_. Это означает, что он может разделить классы только путем нахождения серии гиперплоскостей в 768-мерном пространстве признаков.  
Если линейный классификатор показывает высокую точность, это свидетельствует о том, что Wav2Vec2 уже выполнил всю сложную, нелинейную работу, и что 14 классов намерения являются почти линейно разделимыми в пространстве 768-мерных эмбеддингов. Однако, практически невозможно, чтобы _все_ сложные акустические и семантические паттерны, определяющие тонкое различие между схожими намерениями (например, 'transfer money' и 'check balance'), были идеально линейными.  
**Анализ Ограничения Смещения (Bias):** Линейный классификатор обладает высоким смещением относительно истинной сложности классификационной задачи. Он не способен моделировать неаддитивные взаимодействия между признаками. Например, может существовать сложное правило, где признак **A** (высокая частота голоса) \+ признак **B** (произношение определенной фонемы) **ИЛИ** признак **C** (фоновый шум) совместно указывают на конкретное намерение. Линейная модель не может эффективно уловить такое нелинейное, логическое **И/ИЛИ** взаимодействие. Таким образом, производительность линейного слоя ограничена _потолком, определяемым его недостаточной емкостью_, что делает переход к MLP архитектурно необходимым для устранения этого смещения.

## **III. Архитектурное Развитие: Проектирование Многослойного Перцептрона (MLP)**

Переход к MLP — это прямое решение проблемы недостаточной емкости (высокого смещения) линейного классификатора.

### **3.1 Теоретический Императив: Нелинейность**

Ключевое отличие MLP от линейного слоя заключается во введении нелинейных функций активации между слоями. Без нелинейности, какими бы глубокими ни были слои, вся сеть остается эквивалентной одному линейному преобразованию. Это свойство, известное как Теорема об Универсальной Аппроксимации, утверждает, что двухслойная нейронная сеть с нелинейной активацией способна аппроксимировать любую непрерывную функцию.  
**Выбор Функции Активации (ReLU):** Рекомендуется использовать функцию активации Rectified Linear Unit (ReLU), определяемую как f(x) \= \\max(0, x).

1. **Нелинейность и Простота:** ReLU, хотя и является кусочно-линейной, в целом является нелинейной функцией, обеспечивая необходимую для моделирования сложных связей вычислительную мощь.
2. **Эффективность Градиентов:** В отличие от старых функций (Sigmoid или Tanh), ReLU имеет простую производную (0 или 1), что предотвращает проблему "исчезающих градиентов" (vanishing gradient problem) и обеспечивает более стабильный и быстрый процесс обратного распространения ошибки.
3. **Разреженная Активация:** ReLU устанавливает нулевой выход для отрицательных входных значений, что создает разреженность активации в сети. Это может способствовать более эффективным вычислениям и лучшей обобщающей способности.

### **3.2 Оптимальный Выбор Глубины и Ширины для Условий Few-Shot**

Разработка MLP для классификации поверх замороженных эмбеддингов требует тщательного выбора числа скрытых слоев и их размерности. Входная размерность (N_i) составляет 768, выходная (N_o) — 14, а количество образцов (N_s) критически мало — 50\.  
**Архитектурные Эвристики:** Классические эвристики для определения размера скрытого слоя (N_h) предполагают, что он должен быть 2/3 от размера входного слоя плюс размер выходного слоя. Для 768 \\rightarrow 14 это дает N_h \\approx 526\. Другие исследования показывают, что при работе с высококачественными эмбеддингами от больших самоконтролируемых моделей, проекционные головы часто используют ширину, которая значительно снижает размерность, например, до 512, 256 или 128\.  
**Рекомендация на Основе Ограничения Few-Shot:** Учитывая крайне малое количество данных N=50, целью является минимальная достаточная емкость. Чрезмерная глубина или ширина приведет к мгновенному переобучению.  
Предлагаемая архитектура MLP (Одно скрытое семейство):  
Выбор N_h \= 256 является компромиссом:

1. **Емкость:** Размер 256 позволяет уловить значительное число нелинейных комбинаций из 768 входных признаков, эффективно снижая смещение.
2. **Параметризация:** Количество обучаемых параметров в этой конфигурации составит примерно 768 \\times 256 \+ 256 \\times 14 \\approx 200,000. Это значительный рост по сравнению с \\approx 10,766 параметрами линейного слоя.

**Взаимосвязь Емкости и Сложности Входных Признаков:** Традиционные правила ограничения размера нейронной сети в зависимости от числа примеров (N*s) могут рекомендовать существенно меньшую сеть для N_s=50. Однако здесь критически важно учитывать, что входные признаки (768-мерные эмбеддинги Wav2Vec2) обладают чрезвычайно высокой информативностью. Следовательно, выбор ширины скрытого слоя (N_h) в первую очередь определяется необходимостью взаимодействия между N_i=768 признаками, а не только количеством примеров N_s. Высокая емкость головы оправдана для захвата всех потенциальных нелинейных зависимостей, но это *требует\_ беспрецедентно агрессивной регуляризации для обеспечения обобщения.

## **IV. Управление Инженерными Рисками: Кризис Переобучения Few-Shot**

Переход к MLP с \\approx 200 \\text{k} параметрами при N=50 примерах переводит задачу в режим высокого разброса (high-variance regime). Меры регуляризации и валидации должны быть внедрены как неотъемлемая часть тренировочного ритуала.

### **4.1 Режим Высокого Разброса**

Соотношение числа параметров к числу обучающих примеров (P/N \\approx 4000\) является крайне неблагоприятным. В этой ситуации модель почти гарантированно запомнит (memorize) тренировочный набор, включая шум и артефакты разметки, что приведет к исключительно низкой производительности на новых, невидимых данных. Эффективное обучение MLP в таких условиях требует, чтобы каждый элемент архитектуры и процесса обучения был направлен на снижение разброса и повышение обобщающей способности.

### **4.2 Стратегия Регуляризации 1: Стохастический Dropout**

Dropout является ключевым инструментом для предотвращения переобучения в глубоких сетях. Применение Dropout к скрытым слоям MLP случайным образом обнуляет активации нейронов во время обучения. Это заставляет оставшиеся нейроны учиться более надежным (redundant) представлениям и снижает риск соадаптации (co-adaptation), когда два нейрона всегда полагаются друг на друга.  
**Технические Детали:** Учитывая высокий риск переобучения на 50 образцах, стандартные низкие ставки Dropout (p=0.1 или 0.2) будут недостаточны. Рекомендуется использовать относительно высокую вероятность исключения, в диапазоне p \\in \[0.4, 0.5\]. Это максимальное ограничение емкости, которое может быть реализовано, не вызывая при этом недостаточного обучения (underfitting) из\-за слишком сильного усечения сети.

### **4.3 Стратегия Регуляризации 2: L2-Регуляризация**

L2-регуляризация (или затухание весов, weight decay) должна быть активирована в оптимизаторе (например, AdamW). Эта техника добавляет штраф к функции потерь, пропорциональный квадрату величины весов, тем самым стимулируя сеть к поиску решений с меньшими весами. Меньшие веса коррелируют с более простыми и более обобщающими моделями, что напрямую борется с высоким разбросом.  
**Настройка Параметра \\alpha:** Сила штрафа (\\alpha) должна быть настроена агрессивно. В контексте few-shot обучения, высокая \\alpha необходима для поддержания простоты модели. Этот гиперпараметр требует итеративной настройки, поскольку он является прямым регулятором сложности и должен быть откалиброван таким образом, чтобы не допустить роста разброса.

### **4.4 Протокол Гиперпараметров и Стабильность**

**Скорость Обучения (Learning Rate, LR):** Поскольку основной Wav2Vec2 кодировщик заморожен, LR применяется только к новым, случайно инициализированным весам MLP. В Transfer Learning принято использовать очень низкую LR (например, 10^{-4} или меньше) для обеспечения инкрементальной адаптации новых слоев. Чрезмерно высокая LR может привести к нестабильности или неспособности модели сойтись к оптимальному минимуму.  
**Ранняя Остановка (Early Stopping):** Это критически важный механизм для обучения на малом наборе данных. Модель должна обучаться только до тех пор, пока ее производительность улучшается на _валидационном_ наборе. Как только валидационная ошибка перестает уменьшаться или начинает расти (классический признак начала переобучения/меморизации), тренировка должна быть немедленно остановлена. Использование Early Stopping позволяет найти оптимальный компромисс между подгонкой данных (достаточное количество эпох для обучения) и предотвращением переобучения (слишком большое количество эпох).

### **4.5 Надежная Методология Валидации**

Проблема N=50 образцов заключается в том, что стандартное деление (например, 80% обучение / 20% тест, что дает 40/10) приводит к тому, что метрики, полученные на 10 примерах, будут обладать высоким разбросом и не будут надежно отражать истинную производительность.  
**Обязательная Кросс-Валидация:** Для получения статистически надежной оценки обобщающей способности, особенно для принятия решений о выборе гиперпараметров, необходимо использовать Стратифицированную K-Fold Кросс-Валидацию (например, K=5 или K=10). Стратификация гарантирует, что распределение 14 классов равномерно представлено в каждом фолде. K-Fold позволяет использовать _каждый_ из 50 образцов и для обучения, и для валидации в рамках общего процесса, предоставляя усредненную метрику, которая является гораздо более стабильной и достоверной оценкой истинной производительности "Голема-Ученика".  
\#\# V. Сравнительный Анализ: Эффективность, Ресурсы и Бизнес-Контекст  
Переход от линейного слоя к MLP влияет на ключевые нефункциональные требования, связанные с развертыванием в рамках концепции MVP.

### **5.1 Латентность Инференса и Вычислительные Затраты (FLOPs)**

С точки зрения вычислительной нагрузки, MLP добавляет дополнительные матричные умножения и нелинейные операции (ReLU) к процессу инференса.

1. **Линейная Модель:** Минимальные вычислительные затраты. Идеальна для максимальной пропускной способности и минимальной задержки.
2. **MLP Модель:** Вычислительный объем увеличивается, хотя это увеличение незначительно по сравнению с ресурсоемкостью самого кодировщика Wav2Vec2. Тем не менее, в условиях, требующих сверхнизкой задержки (например, обработка речи в реальном времени на периферийных устройствах или в облаке с высокой загрузкой), даже этот маргинальный рост может повлиять на целевые показатели уровня обслуживания (SLOs). Инженерное решение в пользу MLP должно быть обосновано четкой демонстрацией того, что прирост точности окупает небольшое увеличение латентности.

### **5.2 Объем Памяти и Размер Артефакта**

Размер файла обученных весов (voice_classifier_knowledge.pth) существенно увеличится при переходе к MLP. Слой 768 \\righta\[span_12\](start_span)\[span_12\](end_span)rrow 14 содержит около 10,766 весов, тогда как MLP 768 \\rightarrow 256 \\rightarrow 14 содержит около 200,000 весов.  
Это увеличение размера артефакта влечет за собой несколько последствий:

- **Память:** Более крупный классификатор потребляет больше памяти, что особенно важно при развертывании на устройствах с ограниченными ресурсами, таких как L4 GPU (где, например, выделенная память может составлять всего 24 ГБ).
- **Время Холодного Старта:** В серверных архитектурах или бессерверных функциях (serverless), больший размер файла модели увеличивает время загрузки, или "холодного старта."

Для смягчения этих проблем после обучения MLP могут быть рассмотрены методы уменьшения модели, такие как квантование (перевод весов, например, в INT8), что может значительно уменьшить размер артефакта (в некоторых случаях, в 3.4 раза) и сократить потребление памяти.

### **5.3 Сложность Разработки («Время Техноманта»)**

Самое существенное нетехническое последствие перехода к MLP — это увеличение инженерных затрат. Простой линейный слой требует минимальной настройки гиперпараметров. MLP же, работающий в режиме few-shot, требует:

- Итеративного поиска оптимальной ширины (N_h) и глубины.
- Тонкой настройки _двух_ регуляризационных параметров: p (Dropout) и \\alpha (L2).
- Строгого внедрения K-Fold Cross-Validation, которое усложняет тренировочный цикл.

Все это увеличивает время, необходимое для создания минимально жизнеспособного продукта (MVP), что противоречит принципу быстрого и дешевого запуска. Стратегическое решение о переходе к MLP должно быть принято только после того, как будет подтверждено, что линейный классификатор не удовлетворяет минимальным требованиям к точности.

## **VI. Синтез и Действенные Рекомендации для Техноманта**

### **6.1 Стратегия Внедрения: Поэтапное Развертывание**

Для контролируемого перехода от линейной модели к MLP предлагается трехфазный подход:

1. **Фаза 1 (Базовая оценка):** Провести тщательное тестирование nn.Linear классификатора на полном наборе N=50 с использованием K-Fold Cross-Validation для установления надежного потолка производительности линейной модели.
2. **Фаза 2 (Конструирование MLP):** Внедрить рекомендуемую неглубокую архитектуру MLP (768 \\rightarrow 256 \\rightarrow 14\) с активацией ReLU.
3. **Фаза 3 (Строгое Обучение):** Применить рекомендованный протокол регуляризации (Dropout, L2) и Раннюю Остановку, используя K-Fold CV для мониторинга валидационных метрик.

### **6.2 Детальная Архитектурная Спецификация («Мудрый Голем»)**

Архитектура MLP должна быть построена с учетом необходимости балансировки высокой емкости входных признаков (N_i=768) и малого числа образцов (N_s=50).  
Таблица 1: Технические Характеристики Задачи Классификации

| Характеристика | Входная Размерность Wav2Vec2 | Количество Выходных Классов MInDS-14 | Ограничение Обучающей Выборки (N)       |
| :------------- | :--------------------------- | :----------------------------------- | :-------------------------------------- |
| Значение       | 768 Измерений                | 14 Намерений                         | 50 Образцов (Few-Shot) \[Quest Legend\] |

Таблица 2: Рекомендации по Дизайну и Гиперпараметрам для Оптимизации MLP

| Параметр/Слой              | Базовая Модель (Линейный Слой) | Рекомендованный Неглубокий MLP               | Техническое Обоснование                                                                                                                                     |
| :------------------------- | :----------------------------- | :------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Архитектура                | 1 Слой: 768 \\rightarrow 14    | 2 Слой: 768 \\rightarrow 256 \\rightarrow 14 | Минимальная глубина, обеспечивающая нелинейное преобразование; N_h=256 достаточно для компрессии/проекции 768 признаков, избегая избыточной параметризации. |
| Функция Активации          | Отсутствует                    | ReLU в Скрытом Слое                          | Необходима для создания универсального аппроксиматора и обеспечения стабильности градиентов.                                                                |
| Число Обучаемых Параметров | \\approx 10,766                | \\approx 200,000                             | Резкое увеличение емкости требует агрессивного контроля разброса.                                                                                           |
| Первичная Регуляризация    | Н/П                            | Dropout (p=0.4 до 0.5)                       | Обязательна для Few-Shot обучения, чтобы предотвратить соадаптацию нейронов и высокий разброс.                                                              |
| Вторичная Регуляризация    | Н/П                            | L2 Weight Decay (\\alpha: Высокий)           | Штрафует большие веса, что является мощным инструментом против переобучения на малых данных.                                                                |
| Скорость Обучения (LR)     | Умеренная (напр., 10^{-3})     | Низкая (напр., 5 \\times 10^{-5} до 10^{-4}) | Обеспечивает стабильное и инкрементальное обучение новых весов, что стандартно для Transfer Learning.                                                       |
| Протокол Обучения          | Стандартные Эпохи              | Ранняя Остановка \+ K-Fold CV                | Критически важен для остановки обучения в момент начала меморизации и для получения надежных, низковариантных метрик.                                       |

### **6.3 Заключительный Вердикт**

Внедрение MLP (как более "Мудрого Голема") является оправданным шагом для повышения точности классификации намерения за счет снижения структурного смещения, присущего линейной модели. Теоретически, MLP, как универсальный аппроксиматор, может улавливать нелинейные паттерны, недоступные линейному слою, и, вероятно, покажет более высокую точность.  
Однако, этот путь сопряжен со значительными инженерными требованиями. Успех MLP напрямую зависит от способности инженера-разработчика справиться с экстремальным риском высокого разброса (переобучения), вызванным критически малым объемом данных N=50. Только строгое применение комбинированных методов регуляризации (Dropout, L2) и использование надежных методов валидации (K-Fold CV с Ранней Остановкой) позволит добиться лучших результатов, сохраняя при этом обобщающую способность, необходимую для промышленного MVP. Если линейный классификатор уже обеспечивает достаточную точность (например, выше 95%), переход к MLP может не стоить сопутствующего увеличения сложности и времени разработки.

#### **Источники**

1\. README.md · PolyAI/minds14 at main \- Hugging Face, https://huggingface.co/datasets/PolyAI/minds14/blob/main/README.md 2\. Teaching Wav2Vec2 the Language of the Brain \- arXiv, https://arxiv.org/html/2501.09459v1 3\. Perceptron \- Wikipedia, https://en.wikipedia.org/wiki/Perceptron 4\. Activation function \- Wikipedia, https://en.wikipedia.org/wiki/Activation\_function 5\. Difference between Multilayer Perceptron and Linear Regression \- GeeksforGeeks, https://www.geeksforgeeks.org/machine-learning/difference-between-multilayer-perceptron-and-linear-regression/ 6\. Transfer learning & fine-tuning \- Keras, https://keras.io/guides/transfer\_learning/ 7\. Varying regularization in Multi-layer Perceptron \- Scikit-learn, https://scikit-learn.org/stable/auto\_examples/neural\_networks/plot\_mlp\_alpha.html 8\. How does the number of training epochs during fine-tuning affect the quality of a Sentence Transformer model versus the risk of overfitting? \- Milvus, https://milvus.io/ai-quick-reference/how-does-the-number-of-training-epochs-during-finetuning-affect-the-quality-of-a-sentence-transformer-model-versus-the-risk-of-overfitting 9\. 5.6. Dropout — Dive into Deep Learning 1.0.3 documentation, http://d2l.ai/chapter\_multilayer-perceptrons/dropout.html 10\. AI MVP Development: How to Build Smarter Minimum Viable Products with AI \- WiserBrand, https://wiserbrand.com/ai-mvp-development-how-to-build-smarter-minimum-viable-products-with-ai/ 11\. How to Build an MVP with AI | Step-by-Step Guide \- Product School, https://productschool.com/blog/artificial-intelligence/ai-mvp 12\. A WAV2VEC2-Based Experimental Study on Self-Supervised Learning Methods to Improve Child Speech Recognition \- IEEE Xplore, https://ieeexplore.ieee.org/iel7/6287639/10005208/10122501.pdf 13\. Wav2Vec2 \- Hugging Face, https://huggingface.co/docs/transformers/en/model\_doc/wav2vec2 14\. Evaluating the Effectiveness of Transformer Layers in Wav2Vec 2.0, XLS-R, and Whisper for Speaker Identification Tasks \- arXiv, https://arxiv.org/html/2509.00230v1 15\. PolyAI/minds14 · Datasets at Hugging Face, https://huggingface.co/datasets/PolyAI/minds14 16\. Guide to Non-Linear Activation Functions in Deep Learning | by Pralabh Saxena | Heartbeat, https://heartbeat.comet.ml/guide-to-non-linear-activation-functions-in-deep-learning-6f3725e3a73d 17\. Overcoming Common Pitfalls in Multilayer Perceptron: A Guide to Understand & Handling Overfitting, Underfitting, and Gradient Issues \- Medium, https://medium.com/the-modern-scientist/overcoming-common-pitfalls-in-multilayer-perceptron-a-guide-to-understand-handling-overfitting-8131b5e94e47 18\. Rectified linear unit \- Wikipedia, https://en.wikipedia.org/wiki/Rectified\_linear\_unit 19\. ReLU Activation Function in Deep Learning \- GeeksforGeeks, https://www.geeksforgeeks.org/deep-learning/relu-activation-function-in-deep-learning/ 20\. How to choose the number of hidden layers and nodes? \- Stack Overflow, https://stackoverflow.com/questions/52485608/how-to-choose-the-number-of-hidden-layers-and-nodes 21\. Scaling to Multimodal and Multichannel Heart Sound Classification: Fine-Tuning Wav2Vec 2.0 with Synthetic and Augmented Biosignals \- arXiv, https://arxiv.org/html/2509.11606v2 22\. Unable to overfit using MLP \- Data Science Stack Exchange, https://datascience.stackexchange.com/questions/31343/unable-to-overfit-using-mlp 23\. \[2301.11015\] Explore the Power of Dropout on Few-shot Learning \- arXiv, https://arxiv.org/abs/2301.11015 24\. Regularization in Machine Learning \- GeeksforGeeks, https://www.geeksforgeeks.org/machine-learning/regularization-in-machine-learning/ 25\. Transfer learning, fine-tuning and hyperparameter tuning — Deep learning with TensorFlow, https://developmentseed.org/tensorflow-eo-training-2/docs/Lesson7c\_transfer\_learning\_hyperparam\_opt.html 26\. Epochs, Batch Size, Iterations \- How are They Important to Training AI and Deep Learning Models \- SabrePC, https://www.sabrepc.com/blog/Deep-Learning-and-AI/Epochs-Batch-Size-Iterations 27\. Cross-Validation in Machine Learning: How to Do It Right \- Neptune.ai, https://neptune.ai/blog/cross-validation-in-machine-learning-how-to-do-it-right 28\. 3.1. Cross-validation: evaluating estimator performance \- Scikit-learn, https://scikit-learn.org/stable/modules/cross\_validation.html 29\. HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading \- arXiv, https://arxiv.org/html/2502.12574v1 30\. Leading MLPerf Inference v3.1 Results with NVIDIA GH200 Grace Hopper Superchip Debut, https://developer.nvidia.com/blog/leading-mlperf-inference-v3-1-results-gh200-grace-hopper-superchip-debut/ 31\. Is there a rule of thumb for how many layers should be in a neural net? What about the weights initial range? \- Reddit, https://www.reddit.com/r/MachineLearning/comments/ncgbl/is\_there\_a\_rule\_of\_thumb\_for\_how\_many\_layers/ 32\. \[2203.08490\] Learning Audio Representations with MLPs \- ar5iv, https://ar5iv.labs.arxiv.org/html/2203.08490
