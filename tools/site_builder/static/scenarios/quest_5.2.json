{
  "_meta": {
    "input_hash": "c3dba1815bbd255500db406b3499a872"
  },
  "scenario": [
    {
      "command": "pip install peft bitsandbytes sentencepiece",
      "output": "Collecting peft\n  Downloading peft-0.4.0-py3-none-any.whl (68 kB)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.40.2-cp38-cp38-manylinux1_x86_64.whl (1.0 MB)\nCollecting sentencepiece\n  Downloading sentencepiece-0.1.99-cp38-cp38-manylinux1_x86_64.whl (871 kB)\nInstalling collected packages: sentencepiece, bitsandbytes, peft\nSuccessfully installed sentencepiece-0.1.99 bitsandbytes-0.40.2 peft-0.4.0"
    },
    {
      "command": "pip install -U datasets",
      "output": "Collecting datasets\n  Downloading datasets-2.16.1-py3-none-any.whl (2.3 MB)\nInstalling collected packages: datasets\nSuccessfully installed datasets-2.16.1"
    },
    {
      "command": "python quest_5_2.py",
      "output": "Подготовка к ритуалу Наставления...\nDownloading model distilgpt2 from huggingface.co... Done.\nTokenizer ready. pad_token set to eos_token.\nLoading quantization config...\nApplying 4-bit quantization (nf4)...\nModel loaded: distilgpt2\nStreaming dataset: databricks/databricks-dolly-15k (train, streaming)\nProcessing first 1000 examples...\nLoRA config prepared: r=16, lora_alpha=32, target_modules=['c_attn', 'c_proj'], lora_dropout=0.05\nModel prepared for kbit training\nНачинаю Великий Ритуал Наставления...\n***** Running training *****\n  Num examples = 1000\n  Num Epochs = 1\n  Total optimization steps = 250\nStep 20/250 - loss: 3.12\nStep 40/250 - loss: 2.89\nStep 60/250 - loss: 2.60\nStep 80/250 - loss: 2.34\nStep 100/250 - loss: 2.18\nStep 120/250 - loss: 2.01\nStep 140/250 - loss: 1.86\nStep 160/250 - loss: 1.72\nStep 180/250 - loss: 1.62\nStep 200/250 - loss: 1.50\nStep 220/250 - loss: 1.42\nStep 240/250 - loss: 1.28\nStep 250/250 - loss: 1.15\nРитуал завершен! Голем получил новые знания.",
      "is_final": true
    }
  ]
}
