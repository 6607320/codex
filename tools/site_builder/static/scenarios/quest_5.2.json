[
  {
    "command": "pip install peft bitsandbytes sentencepiece",
    "output": "Collecting peft\n  Downloading peft-0.12.0-py3-none-any.whl (559 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 559.3/559.3 kB 5.2 MB/s eta 0:00:00\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (131.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 131.5/131.5 MB 8.9 MB/s eta 0:00:00\nCollecting sentencepiece\n  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 7.1 MB/s eta 0:00:00\nSuccessfully installed bitsandbytes-0.43.1 peft-0.12.0 sentencepiece-0.2.0\nuser@ubuntu:~$ "
  },
  {
    "command": "pip install -U datasets",
    "output": "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.6)\nCollecting datasets\n  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 510.8/510.8 kB 12.3 MB/s eta 0:00:00\n... (upgrading dependencies)\nSuccessfully installed datasets-2.18.0 ... (other deps)\nuser@ubuntu:~$ "
  },
  {
    "command": "python quest_5_2.py",
    "output": "Подготовка к ритуалу Наставления...\nDownloading config.json from https://huggingface.co/distilgpt2/resolve/main/config.json ... 100%|██████████| 481/481 [00:00<00:00, 1.2kB/s]\nDownloading tokenizer_config.json ... 100%|██████████| 274/274 [00:00<00:00]\nDownloading tokenizer.json ... 100%|██████████| 466k/466k [00:00<00:00, 2.1MB/s]\nDownloading vocab.json ... 100%|██████████| 1.05M/1.05M [00:00<00:00, 4.5MB/s]\nDownloading merges.txt ... 100%|██████████| 456k/456k [00:00<00:00, 1.9MB/s]\nDownloading model.safetensors ... 100%|██████████| 355M/355M [00:01<00:00, 250MB/s]\n\nНачинаю Великий Ритуал Наставления...\n[20/250 00:45<07:30, 0.50it/s] {'loss': 3.2500, 'learning_rate': 2.0000e-04, 'epoch': 0.08}\n[40/250 01:25<06:35, 0.52it/s] {'loss': 2.9800, 'learning_rate': 2.0000e-04, 'epoch': 0.16}\n[60/250 02:05<05:55, 0.53it/s] {'loss': 2.7200, 'learning_rate': 2.0000e-04, 'epoch': 0.24}\n[80/250 02:48<05:18, 0.51it/s] {'loss': 2.5100, 'learning_rate': 2.0000e-04, 'epoch': 0.32}\n[100/250 03:28<04:37, 0.52it/s] {'loss': 2.3400, 'learning_rate': 2.0000e-04, 'epoch': 0.40}\n[120/250 04:09<03:58, 0.52it/s] {'loss': 2.1900, 'learning_rate': 2.0000e-04, 'epoch': 0.48}\n[140/250 04:50<03:18, 0.52it/s] {'loss': 2.0600, 'learning_rate': 2.0000e-04, 'epoch': 0.56}\n[160/250 05:31<02:38, 0.52it/s] {'loss': 1.9400, 'learning_rate': 2.0000e-04, 'epoch': 0.64}\n[180/250 06:12<01:58, 0.52it/s] {'loss': 1.8300, 'learning_rate': 2.0000e-04, 'epoch': 0.72}\n[200/250 06:53<01:18, 0.52it/s] {'loss': 1.7200, 'learning_rate': 2.0000e-04, 'epoch': 0.80}\n[220/250 07:34<00:38, 0.52it/s] {'loss': 1.6200, 'learning_rate': 2.0000e-04, 'epoch': 0.88}\n[240/250 08:15<00:00, 0.52it/s] {'loss': 1.5300, 'learning_rate': 2.0000e-04, 'epoch': 0.96}\n{'train_runtime': 501.2, 'train_samples_per_second': 7.98, 'train_steps_per_second': 0.499, 'total_flos': 1.234567e+16, 'train_loss': 2.234567890123456}\n\nРитуал завершен! Голем получил новые знания.",
    "is_final": true
  }
]