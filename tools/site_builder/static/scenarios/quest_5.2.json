{
  "_meta": {
    "input_hash": "7dafa93713ccd15d6e509e375060426b"
  },
  "scenario": [
    {
      "command": "pip install peft bitsandbytes sentencepiece",
      "output": "Collecting peft\n  Downloading peft-0.12.0-py3-none-any.whl (316 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 316.4/316.4 kB 5.2 MB/s eta 0:00:00\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (141 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 141.0/141.0 MB 8.9 MB/s eta 0:00:00\nCollecting sentencepiece\n  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 7.1 MB/s eta 0:00:00\nSuccessfully installed bitsandbytes-0.43.1 peft-0.12.0 sentencepiece-0.2.0\n"
    },
    {
      "command": "pip install -U datasets",
      "output": "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\nCollecting datasets\n  Downloading datasets-2.21.0-py3-none-any.whl (543 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 543.9/543.9 kB 12.3 MB/s eta 0:00:00\nSuccessfully installed datasets-2.21.0\n"
    },
    {
      "command": "python quest_5_2.py",
      "output": "Подготовка к ритуалу Наставления...\nDownloading (…)okenizer_config.json: 100%|██████████| 548/548 [00:00<00:00, 1.2MB/s]\nDownloading tokenizer.json: 100%|██████████| 2.08M/2.08M [00:00<00:00, 15.2MB/s]\nDownloading (…)tencepiece_bpe.model: 100%|██████████| 5.07M/5.07M [00:00<00:00, 28.1MB/s]\nDownloading config.json: 100%|██████████| 484/484 [00:00<00:00, 1.8MB/s]\nDownloading pytorch_model.bin: 100%|██████████| 353M/353M [00:01<00:00, 298MB/s]\nDetected CUDA available device: cuda:0\nLoading model with 4bit quantization\n\nНачинаю Великий Ритуал Наставления...\n[250/250 05:23, Epoch 0.16/1] {'loss': 9.2345, 'learning_rate': 2e-05, 'epoch': 0.03, 'step': 20}\n[250/250 10:12, Epoch 0.06/1] {'loss': 8.1234, 'learning_rate': 2e-05, 'epoch': 0.06, 'step': 40}\n[250/250 15:01, Epoch 0.09/1] {'loss': 7.4567, 'learning_rate': 2e-05, 'epoch': 0.09, 'step': 60}\n[250/250 19:45, Epoch 0.12/1] {'loss': 6.7890, 'learning_rate': 2e-05, 'epoch': 0.12, 'step': 80}\n[250/250 24:28, Epoch 0.15/1] {'loss': 5.9876, 'learning_rate': 2e-05, 'epoch': 0.15, 'step': 100}\n[250/250 29:10, Epoch 0.18/1] {'loss': 5.1234, 'learning_rate': 2e-05, 'epoch': 0.18, 'step': 120}\n[250/250 33:52, Epoch 0.21/1] {'loss': 4.5678, 'learning_rate': 2e-05, 'epoch': 0.21, 'step': 140}\n[250/250 38:35, Epoch 0.24/1] {'loss': 3.9876, 'learning_rate': 2e-05, 'epoch': 0.24, 'step': 160}\n[250/250 43:17, Epoch 0.27/1] {'loss': 3.4567, 'learning_rate': 2e-05, 'epoch': 0.27, 'step': 180}\n[250/250 47:59, Epoch 0.30/1] {'loss': 2.9876, 'learning_rate': 2e-05, 'epoch': 0.30, 'step': 200}\n[250/250 52:42, Epoch 0.33/1] {'loss': 2.5678, 'learning_rate': 2e-05, 'epoch': 0.33, 'step': 220}\n[250/250 57:24, Epoch 0.36/1] {'loss': 2.1234, 'learning_rate': 2e-05, 'epoch': 0.36, 'step': 240}\nSaving model checkpoint to ./results/checkpoint-250\nTrainer.train: 100%|██████████| 250/250 [57:25<00:00, 13.78s/it, loss=2.1234, step=250]\n\nРитуал завершен! Голем получил новые знания.\n{'train_runtime': 3445.2, 'train_samples_per_second': 0.29, 'train_steps_per_second': 0.07, 'total_flos': 1.23456789e+17, 'train_loss': 4.56789, 'epoch': 0.36}",
      "is_final": true
    }
  ]
}
