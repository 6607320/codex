# === quest_11_1.py (Финальная Версия с Полными Комментариями) ===
# Квест: 11.1 - Создание заклинания "Фокуса"
# Цель: Визуализировать магию Attention, чтобы интуитивно понять,
# как слова в предложении "смотрят" друг на друга и находят смысловые связи.

# --- Легенда Квеста: Эмпатический Разговор ---
# Представь, что 4 мага ("Король", "сел", "принц", "упал") сидят в комнате.
# У каждого есть своя "аура" (вектор).
# Attention - это ритуал, который позволяет КАЖДОМУ магу "почувствовать" ауру
# всех остальных и понять, кто из них ему "ближе по духу".
# Наша тепловая карта - это визуализация этого "эмпатического поля".

# --- Акт 1: Подготовка Гримуаров ---
# Призываем наш силовой гримуар PyTorch для работы с тензорами.
import torch
# Призываем функциональную часть PyTorch для заклинания Softmax.
import torch.nn.functional as F
# Призываем "Художника" для рисования нашей карты.
import matplotlib.pyplot as plt
# Призываем 'numpy' для математических операций (вычисления корня).
import numpy as np

# --- Акт 2: Призыв "Магов" и их "Аур" ---
# Создаем "ауры" (эмбеддинги) для 4-х условных слов.
# Мы специально делаем ауры "Короля" и "принца" похожими (оба вектора "смотрят" вверх и вправо).
# Ауры "сел" и "упал" тоже похожи (смотрят вниз).
# torch.tensor([...]) - создает тензор (многомерный массив) из наших данных.
qkv = torch.tensor([
    [1.0, 1.0, 0.0],  # Аура 1: "Король"
    [0.0, -1.0, 0.0], # Аура 2: "сел"
    [0.9, 0.9, 0.0],  # Аура 3: "принц" (похожа на Ауру 1)
    [0.1, -0.9, 0.0]  # Аура 4: "упал" (похожа на Ауру 2)
]).unsqueeze(0) # .unsqueeze(0) - добавляет "батч" измерение, чтобы тензор имел форму (1, 4, 3).

print("--- Исходные 'Ауры' наших Магов ---")
# Печатаем исходные данные, чтобы видеть, с чего мы начали.
print(qkv)
# В ритуале Self-Attention ("внимание к себе") один и тот же тензор
# используется для всех трех ролей: Запроса (Query), Ключа (Key) и Значения (Value).

# --- Акт 3: Ритуал "Фокуса Мысли" ---
# Этот акт вычисляет "Карту Внимания", показывающую, кто на кого "смотрит".

# Шаг 3.1: "Оценка Похожести" (Матричное Умножение)
# Мы умножаем матрицу "аур" саму на себя (транспонированную), чтобы получить
# "карту симпатий". `@` - это оператор матричного умножения.
# .transpose(-2, -1) - меняет местами последние два измерения (строки и столбцы).
attention_scores = qkv @ qkv.transpose(-2, -1)

# Шаг 3.2: "Магическая Нормализация" (Масштабирование)
# qkv.size(-1) - получаем "глубину смысла" нашей ауры (в данном случае, 3).
d_k = qkv.size(-1)
# Делим все "симпатии" на корень из этой глубины. Это делает обучение
# более стабильным в настоящих Трансформерах.
scaled_scores = attention_scores / np.sqrt(d_k)

# Шаг 3.3: "Ритуал Сосредоточения" (Softmax)
# F.softmax - это заклинание, которое берет любые числа и превращает их
# в вероятности (числа от 0 до 1), сумма которых по строке равна 1.
# Это и есть наша финальная "Карта Внимания" в процентах.
attention_weights = F.softmax(scaled_scores, dim=-1)

print("\n--- Карта Внимания (в процентах, округлено) ---")
print("Кто (строки) на кого (столбцы) 'смотрит' больше всего:")
# torch.round(...) - округляем результат для наглядности.
print(torch.round(attention_weights * 100))

# --- Акт 4: Визуализация Эмпатического Поля ---
print("\nСоздаю визуализацию 'Эмпатического Поля' (Attention Map)...")

# .squeeze(0) - убираем "батч" измерение. .detach() - "открепляем" от графов вычислений.
# .numpy() - превращаем в массив numpy, понятный "Художнику" matplotlib.
heatmap_data = attention_weights.squeeze(0).detach().numpy()
# Создаем список подписей для осей нашей карты.
labels = ["Король", "сел", "принц", "упал"]

# Создаем "холст" (ax) и "рамку" (fig) для нашей картины.
fig, ax = plt.subplots()
# ax.imshow(...) - главное заклинание рисования. Оно рисует наш массив
# как "тепловую карту", используя красивую цветовую схему 'viridis'.
im = ax.imshow(heatmap_data, cmap='viridis')

# Настраиваем подписи на осях X и Y.
ax.set_xticks(np.arange(len(labels)), labels=labels)
ax.set_yticks(np.arange(len(labels)), labels=labels)
# Поворачиваем подписи на оси X для лучшей читаемости.
plt.setp(ax.get_xticklabels(), rotation=45, ha="right", rotation_mode="anchor")

# Запускаем цикл, чтобы в каждую ячейку нашей карты вписать ее значение в процентах.
for i in range(len(labels)):
    for j in range(len(labels)):
        text = ax.text(j, i, f"{heatmap_data[i, j]*100:.0f}%",
                       ha="center", va="center", color="w")

# Добавляем название всей картине и осям.
ax.set_title("Эмпатическое Поле (Карта Внимания)")
ax.set_xlabel("На кого 'смотрит' (Key)")
ax.set_ylabel("Кто 'смотрит' (Query)")
# Автоматически подгоняем размеры, чтобы все поместилось.
fig.tight_layout()

# Сохраняем нашу "карту" в виде файла-артефакта.
plt.savefig("attention_map.png")
print("Магия визуализирована! Открой файл 'attention_map.png'.")

# --- Акт 5: Расшифровка Визуального Результата ---
print("\n--- Что означает эта карта? ---")
# Это финальные инструкции для ученика, объясняющие, как интерпретировать результат.
print("Посмотри на картинку 'attention_map.png':")
print("1. Найди строку 'Король'. Самая яркая (горячая) ячейка в этой строке - в столбце 'принц'.")
print("   Это значит, что маг 'Король' 'почувствовал' самое сильное родство с 'принцем'.")
print("2. Теперь найди строку 'сел'. Самая яркая ячейка - в столбце 'упал'.")
print("   Маг 'сел' 'почувствовал' наибольшую симпатию к магу 'упал'.")
print("Это и есть магия Attention: он автоматически находит смысловые связи в данных!")