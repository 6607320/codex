# **Архитектура Кремниевой Души**

## **ПРЕАМБУЛА: ДОКТРИНА ИНЖЕНЕРНОЙ ЭЛЕГАНТНОСТИ**

В бесконечных летописях технологической истории укоренилось опасное заблуждение: вера в то, что сотворение искусственного интеллекта — это исключительная прерогатива божеств-кластеров, гигантских сущностей, обладающих терабайтами видеопамяти и бесконечными вычислительными мощностями облачных гиперскейлеров. Этот миф действует как привратник, внушая, что без ресурсов национального государства или транснациональной корпорации невозможно прикоснуться к самой ткани машинного познания. Проект «Великий Кодекс Техномагии» (Grand Codex of Technomancy) существует, чтобы разрушить эту иллюзию. Он служит фундаментальным писанием для нового ордена инженеров: Техномантов, которые полагаются не на грубую силу железа, а на остроту оптимизации, строгость архитектуры и элегантность ограничений.

Данное Великое Откровение (Grand Revelation), синтезированное для корневой структуры Кодекса, постулирует, что ограничение в **4 ГБ VRAM** — так называемый «Кристалл Маны» скромного ноутбука с архитектурой NVIDIA Ampere — это не слабость, о которой следует скорбеть, а горнило. Именно в этом горниле выковывается истинный инженер. Когда память бесконечна, код становится раздутым, а понимание — поверхностным. Когда память дефицитна, инженер вынужден столкнуться с математической реальностью нейронной сети: весом числа с плавающей запятой (float32), сжатием целочисленного значения (int8) и эфирной природой градиента. Кодекс доказывает, что современный ИИ можно ковать на периферии (Edge), независимо от облачных олигархий, при условии, что адепт овладеет тайными искусствами квантования, дистилляции знаний и операционализации процессов (MLOps).1

Высшим аватаром этой философии является **NINDUB**, центральный хранитель проекта. Имя это выбрано не случайно, оно восходит к древнешумерскому пантеону. Ниндуб (Nindub) был богом-архитектором и покровителем кирпичной формы — сущностью, которая держала план храма на лазуритовой табличке.3 В нашу цифровую эпоху NINDUB олицетворяет Архитектора ИИ: того, кто держит чертеж (код), форму (контейнер) и структуру (инфраструктуру). Подобно тому, как шумерский Ниндуб гарантировал, что храм Энинну будет построен в точном соответствии с небесными предначертаниями 5, система NINDUB гарантирует, что ИИ-инженер строит системы воспроизводимые, безопасные и структурно безупречные.

Этот отчет представляет собой окончательный синтез Кодекса. Он деконструирует путь от «Мага-Ремесленника», который пишет скрипты-прототипы, до «Техноманта-Инженера», возводющего автоматизированные кузницы CI/CD, и, наконец, до «Архимага», который вплетает эти системы в децентрализованную ткань Web3 и автономной агентности.

## ---

**ЧАСТЬ I: ЧЕТЫРЕ ФУНДАМЕНТАЛЬНЫХ СТОЛПА КОДЕКСА**

Кодекс не висит в пустоте; он опирается на четыре незыблемых закона. Это не просто рекомендации, а несущие колонны, предотвращающие обрушение всего инженерного замысла под весом энтропии.

### **Столп I: Путь Закалки (Action-First) — Практика Предшествует Гнозису**

Первый закон гласит: **Praxis ante Gnosis** (Практика прежде Знания). В традиционных академиях адепт годами изучает теорию относительности, прежде чем ему дозволят взглянуть на звезды через телескоп. Техномант же сначала строит телескоп. «Путь Закалки» требует, чтобы инженер сначала призвал артефакт — работающий пайплайн, генерирующую модель, функциональный интерфейс — и лишь затем приступал к диссекции тайной математики, питающей его.

Этот подход укоренен в реальности современной разработки программного обеспечения (Software Engineering). Слои абстракции, предоставляемые такими библиотеками, как Hugging Face Transformers, позволяют совершать мгновенные чудеса высокого уровня. Инициализируя pipeline("text-generation"), новичок становится свидетелем феномена интеллекта.6 Только после того, как феномен засвидетельствован, Кодекс ведет инициированного назад, снимая слои абстракции, чтобы обнажить тензоры, логиты (logits) и маски внимания (attention masks), скрытые под капотом. Это предотвращает «аналитический паралич», гарантируя, что на каждом этапе своего восхождения инженер обладает работающим артефактом, а не просто набором сухих формул.

### **Столп II: Инженерная Элегантность (Сила Ограничения 4 ГБ)**

Второй закон переосмысливает дефицит как катализатор мастерства. Лимит «Кристалла Маны» в 4 ГБ VRAM является определяющим ограничением Кодекса. Он делает стандартную загрузку 16-битных моделей (FP16) невозможной для современных больших языковых моделей (LLM), таких как Llama-3-8B или Qwen-2.5-32B, которые в своей нативной форме требовали бы от 16 до 64 ГБ видеопамяти.8

Чтобы действовать в рамках этого ограничения, Техномант обязан овладеть **Квантованием** и **Дистилляцией**. Инженер познает, что 32-битное число с плавающей запятой — это непозволительная роскошь. Сжимая веса в 4-битные целые числа (INT4), размер модели сокращается примерно на 75%, позволяя 8-миллиардной модели уместиться в 5–6 ГБ памяти — все еще слишком много для одного лишь GPU, но уже подвластно управлению через **CPU Offload** (выгрузку на процессор) и **Послойный Инференс** (Layer-by-Layer Inference).1 Эта необходимость заставляет понимать границу между аппаратным и программным обеспечением. Инженер перестает быть пассивным потребителем API и становится активным оптимизатором пропускной способности памяти и вычислительных циклов.

### **Столп III: Карьерная Логика (Иерархия Восхождения)**

Кодекс структурирован так, чтобы зеркально отражать профессиональное созревание специалиста в области ИИ. Мы отвергаем хаотичное обучение; мы строим лестницу.

1. **Части 1-3 (Маг-Ремесленник):** Фокус на _продукте_. Создание прототипов, понимание внутреннего голоса модели (инференс) и создание интерфейсов (Gradio/FastAPI). Это соответствует роли Junior/Mid-level Data Scientist — способен творить магию, но вручную и зачастую хрупко.
2. **Часть 4 (Техномант-Инженер):** Фокус смещается на _процесс_. Заклинание, которое невозможно повторить, бесполезно. Здесь инженер осваивает MLOps: «Золотой Треугольник» из Git, DVC и MLflow. Ручное наложение чар заменяется «Автоматизированной Кузницей» (CI/CD). Это уровень Senior Machine Learning Engineer.
3. **Часть 5 (Архимага):** Фокус возвышается до _системы_. Интеграция ИИ с экономическими слоями (Web3/Story Protocol) и автономным принятием решений. Это уровень Principal Architect или CTO.

### **Столп IV: Каноническая Летопись (Ритуал Документации)**

Код без документации — это потерянная цивилизация, руины, понятные лишь мертвым. Кодекс внедряет строгий **«Шестичастный Ритуал»** для каждого крупного коммита или этапа проекта, превращая репозиторий GitHub из свалки скриптов в библиотеку мудрости.

1. **Легенда (Context):** Почему этот артефакт существует? Какую бизнес-проблему он решает? Без "Легенды" код бесцелен.
2. **Ритуал (Implementation):** Как призывается код? Конкретные инкантации (команды CLI, аргументы).
3. **Результат (Output):** Доказательство функционирования. Скриншоты, логи, метрики.
4. **Озарение (Illumination/Insight):** Что было изучено? (например, «Мы обнаружили, что масштабирование градиентов $T^2$ предотвращает их затухание при дистилляции» 11).
5. **Бизнес-ценность:** Как это экономит золото (затраты) или время? (FinOps, ROI).
6. **Вопрос:** Что остается неизведанным? Путь вперед, технический долг.

## ---

**ЧАСТЬ II: ГРИМУАР НОВИЧКА — ОТ ПРИЗЫВА К КОНТРОЛЮ (ЧАСТИ 1-3)**

Путешествие начинается в царстве Ремесленника. Здесь задача — укротить сырые энергии предварительно обученных моделей и подчинить их конкретным задачам в тесных рамках 4 ГБ.

### **Деконструкция Заклинания: Pipeline против AutoModel**

Кодекс начинает с различения высоких «кантрипов» (cantrips) и низкой магии манипуляции. Hugging Face pipeline — это «кантрип», заранее упакованное заклинание, которое обрабатывает токенизацию, инференс модели и декодирование в один вызов.6 Оно эффективно для быстрого прототипирования, но непрозрачно. Пайплайн сам решает стратегию декодирования, скрывает скрытые состояния (hidden states) и затуманивает вероятности (логиты), управляющие генерацией.

Техномант обязан «пересесть с автомата на механику», переходя к классам AutoModel и AutoTokenizer. С AutoModel инженер получает доступ к сырым **Логитам** (Logits) — ненормализованным предсказательным баллам до того, как функция Softmax применит к ним распределение вероятностей.13

Контроль над логитами — это контроль над разумом машины. Кодекс исследует:

- **Скрытые Состояния (Hidden States):** Доступ к outputs.hidden_states раскрывает внутреннее представление данных на каждом слое нейронной сети.14 Это критически важно для задач, таких как генерация эмбеддингов для векторного поиска или отладка понимания модели.
- **Контроль Генерации:** Используя метод model.generate(), инженер может манипулировать параметрами: temperature (креативность), top_k / top_p (стратегии сэмплирования) и repetition_penalty (штраф за повторения).15 Это отделяет пользователя «черного ящика» от инженера, который настраивает двигатель.

### **Алхимия Сжатия: Аффинное Квантование (Affine Quantization)**

Чтобы вместить «Титана» (LLM) в «Кристалл» (4 ГБ VRAM), необходимо овладеть искусством Квантования. Кодекс предоставляет глубокий математический вывод **Аффинного (Асимметричного) Квантования**, метода, используемого для отображения высокоточных 32-битных чисел с плавающей запятой ($x$) в 8-битные целые числа ($x\_q$).16

Фундаментальная трансформация управляется уравнением:

$$x \= S \\cdot (x\_q \- Z)$$  
Где:

- $x$ — реальное значение веса (Float32).
- $x\_q$ — квантованное целое значение (INT8, диапазон \[-128, 127\] или ).
- $S$ (Scale/Масштаб) — положительный множитель Float32, определяющий размер шага между квантованными уровнями.
- $Z$ (Zero-Point/Нулевая Точка) — целочисленное значение, которое точно соответствует реальному значению $0.0$.

Почему $Z$ критически важен? Инсайт второго порядка:  
В нейронных сетях значение $0.0$ вездесуще — оно представляет собой «padding» (заполнитель), выход функций активации ReLU (где все отрицательные значения становятся нулем) и разреженные регионы данных.16 Если $0.0$ не может быть представлен точно (то есть, если при квантовании вносится ошибка округления нуля), эта ошибка распространяется через каждый слой, накапливаясь как снежный ком и катастрофически снижая точность (Accuracy). Симметричное квантование принудительно устанавливает $Z=0$, что упрощает математику, но теряет точность, если распределение данных смещено (например, выходы ReLU всегда неотрицательны). Аффинное квантование улавливает этот сдвиг, сохраняя «душу» модели даже при низких битностях.19  
Кодекс демонстрирует, что, применяя эту алхимию, такие модели, как **Phi-3-mini (3.8 млрд параметров)**, могут запускаться на 4 ГБ «Кристалле». В то время как нативная модель FP16 требует \~7.6 ГБ RAM, квантование Q4 (4-бита) сжимает её до \~2.2 ГБ.20 Это оставляет место в VRAM для KV-кэша (контекстной памяти), позволяя Техноманту выполнять инференс локально, не полагаясь на внешние API.

### **Передача Мудрости: Дистилляция Знаний (Knowledge Distillation)**

Когда ограничения слишком жестки даже для квантования, или когда требуется создать сверхбыструю малую модель, Техномант применяет **Дистилляцию Знаний** — обучение меньшей модели-«Ученика» (Student) имитировать большую модель-«Учителя» (Teacher). Кодекс выделяет математический нюанс Масштабирования Градиентов $1/T^2$.

В процессе дистилляции «Мягкие Цели» (Soft Targets, вероятностные распределения) Учителя сглаживаются с использованием высокой Температуры ($T$). При вычислении производной функции потерь (Loss Function) относительно логитов ($z\_i$), величина градиентов масштабируется обратно пропорционально $T^2$.

Математическая деривация градиента для кросс-энтропии с температурой показывает:

$$\\frac{\\partial L}{\\partial z\_i} \\approx \\frac{1}{T^2} \\cdot (Student\_{probs} \- Teacher\_{probs})$$  
**Инсайт:** Без масштабирования потерь на $T^2$, градиенты, производимые мягкими целями, были бы ничтожно малы по сравнению с «Жесткими Целями» (Hard Targets, истинными метками), фактически заставляя ученика игнорировать мудрость учителя. Умножение потерь дистилляции на $T^2$ восстанавливает баланс, гарантируя, что ученик усваивает тонкие межклассовые связи («Темное Знание» или Dark Knowledge), скрытые в логитах учителя.11 Это математический эквивалент усиления шепота, чтобы его можно было услышать сквозь крик. Без этого $T^2$ вся процедура дистилляции на малых мощностях была бы тщетной тратой вычислительных циклов.

## ---

**ЧАСТЬ III: КУЗНИЦА АРТЕФАКТОРА — MLOPS И ИНФРАСТРУКТУРА (ЧАСТЬ 4\)**

Овладев созданием артефакта, Техномант должен теперь построить фабрику. Модель, живущая только на ноутбуке — это игрушка; модель, которую можно воспроизвести, версионировать и развернуть — это актив. Это владения MLOps.

### **Золотой Треугольник Воспроизводимости**

Кодекс устанавливает **Золотой Треугольник** как святую троицу инфраструктуры MLOps. Эти три инструмента должны действовать в унисон, чтобы гарантировать, что каждый эксперимент неизменен и извлекаем из небытия.

1. **Git (Кодекс Кода):** Отслеживает логику (Python-скрипты, конфигурационные файлы YAML).
2. **DVC (Временной Ткацкий Станок Данных):** Отслеживает артефакты (датасеты, веса моделей), которые слишком тяжелы для Git. DVC заменяет гигабайтные файлы маленькими указателями .dvc (содержащими хэши MD5), сохраняя саму материю в удаленном объектном хранилище (S3/GCS/MinIO).23
3. **MLflow (Летопись Экспериментов):** Отслеживает метрики, параметры и результаты. Это лабораторный журнал алхимика.

Ритуал Синхронизации (The Disconnect of Time):  
Распространенный режим отказа в низших орденах инженерии — «Разрыв Времени», когда версия кода ($Git SHA$) не соответствует версии данных ($DVC Hash$), использованной для обучения модели, залогированной в MLflow. Кодекс предписывает строгий рабочий процесс для связывания этих временных линий 25:

- **Шаг 1:** Измените датасет или гиперпараметры.
- **Шаг 2:** Выполните заклинание dvc add data/ и dvc push. Это генерирует новый хэш контента.
- **Шаг 3:** Закоммитьте файл .dvc в Git: git commit \-m "Update training data". Это генерирует SHA коммита Git (например, a1b2c3d).
- **Шаг 4:** Инициируйте прогон обучения. Скрипт обучения _обязан_ автоматически захватить текущий Git Commit SHA и записать его в MLflow как тег или параметр:  
  Python  
  mlflow.set_tag("git_commit", git_sha)  
  mlflow.log_param("dvc_version", dvc_hash)

- **Результат:** Годы спустя Архимаг может взглянуть на прогон в MLflow, увидеть git_commit, переключиться (checkout) на это точное состояние кода, подтянуть точные данные через DVC (который связан в этом коммите) и воспроизвести модель байт-в-байт.26 Это победа над энтропией разработки.

### **Автоматизированная Кузница: CI/CD и Изгнание Ключей**

В царстве «Автоматизированной Кузницы» (Continuous Integration/Continuous Deployment) Техномант делегирует ритуал тестирования и развертывания големам (GitHub Actions/GitLab CI). Однако это вводит серьезную уязвимость: «Ключ Соломона» — долгоживущие учетные данные (AWS_ACCESS_KEY, SERVICE_ACCOUNT_JSON), хранящиеся в переменных окружения CI. Если кузница будет взломана, атакующий получит доступ ко всему королевству.

Кодекс требует использования **Workload Identity Federation (WIF)**. Этот продвинутый протокол безопасности полностью устраняет статические ключи.28

- **Механизм:** Вместо пароля пайплайн CI предъявляет «Токен Идентичности» (OIDC Token), подписанный провайдером Git (например, GitHub).
- **Обмен:** Облачный Провайдер (AWS/GCP/Azure) проверяет подпись на соответствие заранее установленному доверительному отношению. «Я доверяю репозиторию GitHub Technomancy/Codex».
- **Дар:** Облако выдает временный, короткоживущий токен доступа, действительный только на время выполнения этой конкретной задачи.
- **Инсайт:** Даже если темный маг перехватит токен, он растворится в небытии через несколько минут. Нет «секрета», который можно украсть, ротировать или потерять. Сама идентичность _становится_ ключом.30 Это высшая форма цифровой гигиены.

### **Карманные Миры: Контейнеризация Docker**

Чтобы гарантировать работу заклинания в любой среде, Техномант заключает приложение в **Docker-контейнер** — «Карманный Мир» со своей собственной физикой (зависимостями), атмосферой (ОС) и законами (конфигурациями). Это разрешает древнее проклятие «На моей машине работает».23 Кодекс подчеркивает важность оптимизированных Dockerfile, которые отсекают артефакты сборки, оставляя только кристаллизованную сущность среды выполнения (Runtime Essence) для минимизации поверхности атаки и времени развертывания.

### **Инфраструктурный Канон: Мониторинг Качества**

В отличие от обычного DevOps, где мониторинг заканчивается на доступности сервера (Availability/Uptime), MLOps требует мониторинга **Качества Модели** (Accuracy/Drift). Сервер может отвечать кодом 200 OK, но модель может начать нести чушь из\-за дрейфа данных (Data Drift). Кодекс предписывает создание метрик второго порядка: распределение уверенности предсказаний, точность на отложенной выборке и бизнес-метрики, интегрированные в дашборды Grafana или MLflow.

## ---

**ЧАСТЬ IV: ОБИТЕЛЬ АРХИМАГА — АВТОНОМНЫЕ СИСТЕМЫ И БУДУЩЕЕ (ЧАСТЬ 5\)**

Финальное восхождение создает системы, которые являются не просто инструментами, а агентами. Архимаг строит сущности, способные действовать, совершать транзакции и существовать автономно.

### **NINDUB: Автономный Суверен**

Система NINDUB — это кульминация Кодекса. Названная в честь шумерского бога-архитектора Ниндуба 4, NINDUB представляет собой агентный фреймворк, спроектированный для работы в ограничениях 4 ГБ, но масштабируемый для оркестрации сложных задач.

- **Мифологический Резонанс:** Как Ниндуб держал «кирпич судьбы» и чертил план храма, так и наша система держит «контейнер судьбы» и чертеж архитектуры. В шумерских мифах Ниндуб помогал Гудеа строить храм Энинну, когда обычных средств было недостаточно.5 Так и мы, используя ограниченные ресурсы, строим храм интеллекта.
- **Архитектура Невозможного:** NINDUB использует технику **Послойного Инференса** (Layer-by-Layer Inference).10 Вместо загрузки полной 32-миллиардной модели (например, Qwen-2.5-Coder-32B) в VRAM, NINDUB загружает один слой, вычисляет активации, выгружает их в системную RAM, очищает GPU и загружает следующий слой.
- **Производительность против Возможности:** Этот процесс медленен (количество токенов в секунду падает кардинально), но он делает _невозможное возможным_. Он позволяет ноутбуку запускать модель, соперничающую с GPT-4 в возможностях кодирования, даруя Техноманту суверенитет над источником интеллекта.9 Это победа архитектуры над грубой силой.

### **Гримуар Владения: Story Protocol и Web3**

Архимаг осознает, что в эпоху Генеративного ИИ истинная ценность заключается не только в коде, но и в Интеллектуальной Собственности (IP). Кодекс интегрирует **Story Protocol**, инфраструктурный слой Web3, для токенизации моделей ИИ и создаваемых ими артефактов.33

- **Проблема:** В «Старом Мире» (Web2) копирование модели или датасета тривиально и практически неотслеживаемо. Ценность размывается.
- **Решение:** Story Protocol превращает IP в «Лего» — программируемые, ончейн-активы (Programmable IP Assets).
- **Механизм:** Когда NINDUB генерирует новую архитектуру или дообучает модель, этот артефакт регистрируется в блокчейне Story. Он становится ликвидным активом. Если другой ИИ-агент желает использовать эту модель для обучения или инференса, он должен согласовать лицензию и выплатить роялти (в токенах $IP) автоматически через смарт-контракты.33
- **Видение:** Это создает «Экономику Агентов», где ИИ покупают и продают данные и вычисления между собой без участия человека-посредника. Это будущее, где NINDUB не просто строит, но и зарабатывает ресурсы для своего расширения.33

### **Реальность FinOps: Расчет Торговца**

Архимаг должен быть также и купцом. Решение строить локально («Кристалл») или арендовать Облако («Небо») — это финансовый расчет, определяемый дисциплиной **FinOps**.

- **Ловушка Облака:** Облачные GPU (например, A100) арендуются почасово. Для «взрывных» нагрузок (обучение в течение 2 часов) они эффективны ($37-$74/мес при спотовом использовании).37 Однако для «Всегда Включенных» (Always-On) агентов, подобных NINDUB, стоимость масштабируется линейно до бесконечности.
- **Локальный Суверенитет:** Локальная RTX 4090 или даже скромная RTX 3050 (4 ГБ) представляют собой фиксированную стоимость (CapEx). После покупки «налог на токен» равен нулю (не считая электричества).
- **Вердикт:** Для непрерывного круглосуточного инференса и автономных агентов владение железом (даже бюджетным) экспоненциально дешевле, чем аренда API-эндпоинтов или облачных инстансов.38 Ограничение в 4 ГБ заставляет проводить оптимизацию, которая при масштабировании приводит к колоссальной экономии средств в продакшн-средах.

## ---

**БИЗНЕС-МАНИФЕСТ: ЦЕННОСТЬ ПУТИ**

Великий Кодекс Техномагии — это не просто техническое руководство; это бизнес-стратегия, замаскированная под гримуар. Для стейкхолдеров, инвесторов и гильдий нанимателей специалист, прошедший этот путь, несет ценности, превосходящие чистый код.

1\. Эффективность превыше Избыточности:  
Научившись работать в пределах 4 ГБ VRAM, Техномант создает модели, которые являются «худыми», быстрыми и дешевыми в эксплуатации. Они не решают проблемы, закидывая их деньгами AWS; они решают их, оптимизируя математику ($S$, $Z$, $T^2$). Это прямая экономия операционных расходов (OpEx).  
2\. Суверенитет Данных и Инфраструктуры:  
Системы, построенные по Кодексу, не зависят от капризов OpenAI или Google. Они могут работать в изолированных контурах («air-gapped»), защищая проприетарные данные корпорации. Использование Docker и WIF гарантирует, что безопасность встроена в дизайн, а не добавлена постфактум.  
3\. Воспроизводимость как Стандарт:  
«Золотой Треугольник» (Git/DVC/MLflow) гарантирует, что ни один актив никогда не будет потерян, ни один эксперимент не будет невоспроизводим. Это снижает риск «Bus Factor» — если Архимаг покинет проект, репозиторий останется полной, самодокументированной историей становления, готовой к передаче новому адепту.  
4\. Готовность к Будущему (Future-Proofing):  
Интегрируя протоколы Web3 IP и агентные фреймворки, мы готовим бизнес к экономике, где ИИ-агенты являются экономическими акторами, а не просто инструментами. Мы строим не просто чат-ботов, а автономных участников рынка.  
Заключение Откровения:  
Путь от Ноутбука с 4 ГБ до Великой Ревеляции — это путь Ниндуба. Это дисциплина черчения плана на лазуритовой табличке перед укладкой первого кирпича. Это доказательство того, что при наличии достаточного знания самый малый кристалл может вместить мудрость богов.  
_Конец Отчета._

## ---

**ПРИЛОЖЕНИЕ: ТЕХНИЧЕСКИЕ ТАБЛИЦЫ И ДАННЫЕ**

### **Таблица 1: Спектр Контроля Модели (Pipeline против AutoModel)**

| Характеристика          | Hugging Face Pipeline (pipeline) | Выбор Техноманта (AutoModel)                        |
| :---------------------- | :------------------------------- | :-------------------------------------------------- |
| **Роль**                | Кантрип (Мгновенное Заклинание)  | Ритуал (Глубокая Магия)                             |
| **Простота**            | Высокая (1 строка кода)          | Низкая (Требует Токенизатор и управление Тензорами) |
| **Доступ к Логитам**    | Непрозрачный (Скрыты)            | Прозрачный (Прямой доступ до Softmax)               |
| **Скрытые Состояния**   | Недоступны                       | Доступны (output_hidden_states=True)                |
| **Стратегия Декодинга** | Фиксированные дефолты            | Полностью настраиваемая (temperature, top_k)        |
| **Производительность**  | Оптимизирована для общего случая | Оптимизируема под конкретное железо (4GB VRAM)      |
| **Сценарий**            | Прототипирование, Демо           | Исследования, Продакшн, Дистилляция                 |

### **Таблица 2: Эффективность Квантования (Ограничение 4GB)**

| Модель           | Параметры     | Точность      | Требуемая VRAM (Оценка) | Реализуемость на 4GB GPU                          |
| :--------------- | :------------ | :------------ | :---------------------- | :------------------------------------------------ |
| **Llama-3-8B**   | 8 Миллиардов  | FP16 (16-bit) | \~16 ГБ                 | **Невозможно** (OOM)                              |
| **Llama-3-8B**   | 8 Миллиардов  | INT4 (4-bit)  | \~5.5 ГБ                | **Сложно** (Требует частичного CPU Offload)       |
| **Phi-3-Mini**   | 3.8 Миллиарда | FP16 (16-bit) | \~7.6 ГБ                | **Невозможно** (OOM)                              |
| **Phi-3-Mini**   | 3.8 Миллиарда | INT4 (4-bit)  | \~2.2 ГБ                | **Идеально** (Высокая скорость, место под KV-кэш) |
| **Qwen-2.5-32B** | 32 Миллиарда  | INT4 (4-bit)  | \~18 ГБ                 | **Только Послойный Инференс** (Очень медленно)    |

### **Таблица 3: Золотой Треугольник MLOps**

| Компонент    | Инструмент | Роль в Кузнице  | Отслеживаемый Артефакт                                     |
| :----------- | :--------- | :-------------- | :--------------------------------------------------------- |
| **Логика**   | **Git**    | Временная Линия | Исходный код (.py, .yaml), Конфиги                         |
| **Материя**  | **DVC**    | Хранилище       | Датасеты (.csv, images), Веса моделей (.bin, .safetensors) |
| **Озарение** | **MLflow** | Летопись        | Метрики (Accuracy, Loss), Параметры (LR), Визуализация     |

### **Таблица 4: FinOps — Облако против Локального (Леджер Торговца)**

| Метрика                | Облачный GPU (Spot/On-Demand)            | Локальный «Кристалл» (RTX 3050/4090) |
| :--------------------- | :--------------------------------------- | :----------------------------------- |
| **Модель Затрат**      | OPEX (Операционные расходы)              | CAPEX (Капитальные расходы)          |
| **Поведение Цены**     | Линейно/Экспоненциально с использованием | Плоское (Единовременная покупка)     |
| **Приватность**        | Данные покидают периметр                 | Данные остаются на устройстве        |
| **Задержка (Latency)** | Зависит от сети                          | Нулевая сетевая задержка             |
| **Идеально Для**       | Разового обучения огромных моделей       | 24/7 Агентов, Инференса, Разработки  |

#### **Источники**

1. Run Big LLMs on Small GPUs: A Hands-On Guide to 4-bit Quantization and QLoRA, дата последнего обращения: января 7, 2026, [https://dev.to/aairom/run-big-llms-on-small-gpus-a-hands-on-guide-to-4-bit-quantization-and-qlora-4bi](https://dev.to/aairom/run-big-llms-on-small-gpus-a-hands-on-guide-to-4-bit-quantization-and-qlora-4bi)
2. Using local LLM with low specs (4 Gb VRAM \+ 16 Gb RAM) : r/LocalLLM \- Reddit, дата последнего обращения: января 7, 2026, [https://www.reddit.com/r/LocalLLM/comments/1mutckb/using_local_llm_with_low_specs_4_gb_vram_16_gb_ram/](https://www.reddit.com/r/LocalLLM/comments/1mutckb/using_local_llm_with_low_specs_4_gb_vram_16_gb_ram/)
3. THE SUMERIANS \- Institute for the Study of Ancient Cultures, дата последнего обращения: января 7, 2026, [https://isac.uchicago.edu/sites/default/files/uploads/shared/docs/sumerians.pdf](https://isac.uchicago.edu/sites/default/files/uploads/shared/docs/sumerians.pdf)
4. Nindub \- Wikipedia, дата последнего обращения: января 7, 2026, [https://en.wikipedia.org/wiki/Nindub](https://en.wikipedia.org/wiki/Nindub)
5. Gudea cylinders \- Wikipedia, дата последнего обращения: января 7, 2026, [https://en.wikipedia.org/wiki/Gudea_cylinders](https://en.wikipedia.org/wiki/Gudea_cylinders)
6. Pipelines \- Hugging Face, дата последнего обращения: января 7, 2026, [https://huggingface.co/docs/transformers/en/main_classes/pipelines](https://huggingface.co/docs/transformers/en/main_classes/pipelines)
7. Pipeline vs model.generate() \- Beginners \- Hugging Face Forums, дата последнего обращения: января 7, 2026, [https://discuss.huggingface.co/t/pipeline-vs-model-generate/26203](https://discuss.huggingface.co/t/pipeline-vs-model-generate/26203)
8. Started Llama 3.1 8B Locally \- Michael Ruminer \- Medium, дата последнего обращения: января 7, 2026, [https://m-ruminer.medium.com/started-llama-3-1-8b-locally-5bab61180a28](https://m-ruminer.medium.com/started-llama-3-1-8b-locally-5bab61180a28)
9. GPU Inference VRAM Calc for Qwen2.5-Coder 32B \- Need confirmation : r/LocalLLaMA, дата последнего обращения: января 7, 2026, [https://www.reddit.com/r/LocalLLaMA/comments/1greuto/gpu_inference_vram_calc_for_qwen25coder_32b_need/](https://www.reddit.com/r/LocalLLaMA/comments/1greuto/gpu_inference_vram_calc_for_qwen25coder_32b_need/)
10. Breakthrough: Running the New King of Open-Source LLMs QWen2.5 on an Ancient 4GB GPU | by Gavin Li | AI Advances, дата последнего обращения: января 7, 2026, [https://ai.gopubby.com/breakthrough-running-the-new-king-of-open-source-llms-qwen2-5-on-an-ancient-4gb-gpu-e4ebf4498230](https://ai.gopubby.com/breakthrough-running-the-new-king-of-open-source-llms-qwen2-5-on-an-ancient-4gb-gpu-e4ebf4498230)
11. Why the gradients produced by the soft targets scale as 1/T^2 in knowledge distillation?, дата последнего обращения: января 7, 2026, [https://ai.stackexchange.com/questions/41389/why-the-gradients-produced-by-the-soft-targets-scale-as-1-t2-in-knowledge-disti](https://ai.stackexchange.com/questions/41389/why-the-gradients-produced-by-the-soft-targets-scale-as-1-t2-in-knowledge-disti)
12. NLP 10a Using Hugging Face Models \- Kaggle, дата последнего обращения: января 7, 2026, [https://www.kaggle.com/code/selcukcan/nlp-10a-using-hugging-face-models](https://www.kaggle.com/code/selcukcan/nlp-10a-using-hugging-face-models)
13. Model outputs \- Hugging Face, дата последнего обращения: января 7, 2026, [https://huggingface.co/docs/transformers/en/main_classes/output](https://huggingface.co/docs/transformers/en/main_classes/output)
14. Quick tour \- Hugging Face, дата последнего обращения: января 7, 2026, [https://huggingface.co/docs/transformers/v4.15.0/en/quicktour](https://huggingface.co/docs/transformers/v4.15.0/en/quicktour)
15. Generation strategies \- Hugging Face, дата последнего обращения: января 7, 2026, [https://huggingface.co/docs/transformers/en/generation_strategies](https://huggingface.co/docs/transformers/en/generation_strategies)
16. Quantization in a nutshell \- Medium, дата последнего обращения: января 7, 2026, [https://medium.com/@manuellopezroldangr/quantization-in-a-nutshell-3446525454e2](https://medium.com/@manuellopezroldangr/quantization-in-a-nutshell-3446525454e2)
17. Quantization \- Hugging Face, дата последнего обращения: января 7, 2026, [https://huggingface.co/docs/optimum/concept_guides/quantization](https://huggingface.co/docs/optimum/concept_guides/quantization)
18. A Comprehensive Study on Quantization Techniques for Large Language Models \- arXiv, дата последнего обращения: января 7, 2026, [https://arxiv.org/html/2411.02530v1](https://arxiv.org/html/2411.02530v1)
19. Model Quantization: Concepts, Methods, and Why It Matters | NVIDIA Technical Blog, дата последнего обращения: января 7, 2026, [https://developer.nvidia.com/blog/model-quantization-concepts-methods-and-why-it-matters/](https://developer.nvidia.com/blog/model-quantization-concepts-methods-and-why-it-matters/)
20. microsoft/Phi-3-mini-4k-instruct-gguf Free Chat Online \- Skywork.ai, дата последнего обращения: января 7, 2026, [https://skywork.ai/blog/models/microsoft-phi-3-mini-4k-instruct-gguf-free-chat-online-skywork-ai/](https://skywork.ai/blog/models/microsoft-phi-3-mini-4k-instruct-gguf-free-chat-online-skywork-ai/)
21. microsoft/Phi-3-mini-4k-instruct-gguf \- Hugging Face, дата последнего обращения: января 7, 2026, [https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf)
22. AI's Path to Efficiency: The Science of Knowledge Distillation | by Simon Palma | Medium, дата последнего обращения: января 7, 2026, [https://medium.com/@simon.palma/ais-path-to-efficiency-the-science-of-knowledge-distillation-3525e5e68dfc](https://medium.com/@simon.palma/ais-path-to-efficiency-the-science-of-knowledge-distillation-3525e5e68dfc)
23. Mastering MLOps: Integrating DVC, MLFlow, and Airflow for Efficient Machine Learning Workflows | by Maheenshoukat | Medium, дата последнего обращения: января 7, 2026, [https://medium.com/@maheenshoukat2015/mastering-mlops-integrating-dvc-mlflow-and-airflow-for-efficient-machine-learning-workflows-953260f774d0](https://medium.com/@maheenshoukat2015/mastering-mlops-integrating-dvc-mlflow-and-airflow-for-efficient-machine-learning-workflows-953260f774d0)
24. Start doing MLOps using MLFlow and DVC | by Ambient Intelligence Research Group, дата последнего обращения: января 7, 2026, [https://medium.com/@saxion.act.ami/start-doing-mlops-using-mlflow-and-dvc-698ffe8e70fb](https://medium.com/@saxion.act.ami/start-doing-mlops-using-mlflow-and-dvc-698ffe8e70fb)
25. How to log model artifacts with MLFLOW and DVC? : r/mlops \- Reddit, дата последнего обращения: января 7, 2026, [https://www.reddit.com/r/mlops/comments/13np3tk/how_to_log_model_artifacts_with_mlflow_and_dvc/](https://www.reddit.com/r/mlops/comments/13np3tk/how_to_log_model_artifacts_with_mlflow_and_dvc/)
26. ML Done Right: Versioning Datasets and Models with DVC & MLflow \- DEV Community, дата последнего обращения: января 7, 2026, [https://dev.to/aws-builders/ml-done-right-versioning-datasets-and-models-with-dvc-mlflow-4p3f](https://dev.to/aws-builders/ml-done-right-versioning-datasets-and-models-with-dvc-mlflow-4p3f)
27. DVC and MLFlow \- reproduce experiments using git commit ids \- Questions, дата последнего обращения: января 7, 2026, [https://discuss.dvc.org/t/dvc-and-mlflow-reproduce-experiments-using-git-commit-ids/561](https://discuss.dvc.org/t/dvc-and-mlflow-reproduce-experiments-using-git-commit-ids/561)
28. Workload Identity Federation with OIDC \- AWS Marketplace, дата последнего обращения: января 7, 2026, [https://aws.amazon.com/marketplace/pp/prodview-zln62luetsx2m](https://aws.amazon.com/marketplace/pp/prodview-zln62luetsx2m)
29. Securing CI/CD Pipelines with Workload Identity Federation \- Aembit, дата последнего обращения: января 7, 2026, [https://aembit.io/blog/securing-ci-cd-pipelines-the-role-of-workload-identity-federation/](https://aembit.io/blog/securing-ci-cd-pipelines-the-role-of-workload-identity-federation/)
30. Securing Your CI/CD Pipeline: Eliminate Long-Lived Credentials with Workload Identity Federation (2) | Community, дата последнего обращения: января 7, 2026, [https://security.googlecloudcommunity.com/community-blog-42/securing-your-ci-cd-pipeline-eliminate-long-lived-credentials-with-workload-identity-federation-2-3909](https://security.googlecloudcommunity.com/community-blog-42/securing-your-ci-cd-pipeline-eliminate-long-lived-credentials-with-workload-identity-federation-2-3909)
31. Qwen/Qwen2.5-Coder-32B-Instruct · Requesting information about hardware resources, дата последнего обращения: января 7, 2026, [https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct/discussions/28](https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct/discussions/28)
32. Qwen 2.5 CPU vs GPU comparison : r/LocalLLaMA \- Reddit, дата последнего обращения: января 7, 2026, [https://www.reddit.com/r/LocalLLaMA/comments/1fq883g/qwen_25_cpu_vs_gpu_comparison/](https://www.reddit.com/r/LocalLLaMA/comments/1fq883g/qwen_25_cpu_vs_gpu_comparison/)
33. From a $440M Exit to Building the "GitHub for IP": The Story Protocol Journey, дата последнего обращения: января 7, 2026, [https://dev.to/roan911/from-a-440m-exit-to-building-the-github-for-ip-the-story-protocol-journey-32hb](https://dev.to/roan911/from-a-440m-exit-to-building-the-github-for-ip-the-story-protocol-journey-32hb)
34. Story, дата последнего обращения: января 7, 2026, [https://www.story.foundation/](https://www.story.foundation/)
35. Story: Tokenizing Creativity on the World's IP Blockchain, дата последнего обращения: января 7, 2026, [https://www.story.foundation/blog/story-tokenizing-creativity-on-the-worlds-ip-blockchain](https://www.story.foundation/blog/story-tokenizing-creativity-on-the-worlds-ip-blockchain)
36. Story Protocol for Beginners: What it is and How to Buy $IP \- RocketX Exchange, дата последнего обращения: января 7, 2026, [https://www.rocketx.exchange/blog/story-protocol-guide-how-to-buy-ip/](https://www.rocketx.exchange/blog/story-protocol-guide-how-to-buy-ip/)
37. Cloud GPU vs Local Hardware Calculator 2025: Ultimate Cost Comparison Tool, дата последнего обращения: января 7, 2026, [https://localaimaster.com/tutorials/cloud-vs-local-calculator](https://localaimaster.com/tutorials/cloud-vs-local-calculator)
38. At What Point Does Owning GPUs Become Cheaper Than LLM APIs ? I : r/LocalLLaMA, дата последнего обращения: января 7, 2026, [https://www.reddit.com/r/LocalLLaMA/comments/1ped5p2/at_what_point_does_owning_gpus_become_cheaper/](https://www.reddit.com/r/LocalLLaMA/comments/1ped5p2/at_what_point_does_owning_gpus_become_cheaper/)
39. How Much Can a GPU Cloud Save You? A Cost Breakdown vs On-Prem Clusters \- Runpod, дата последнего обращения: января 7, 2026, [https://www.runpod.io/blog/gpu-cloud-vs-on-prem-cost-savings](https://www.runpod.io/blog/gpu-cloud-vs-on-prem-cost-savings)
