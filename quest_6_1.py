# === quest_6_1.py ===
# Квест: 6.1 - Извлечение "эссенции голоса"
# Цель: Освоить магию Transfer Learning (Переноса Знаний) для аудио.
# Мы не будем решать задачу "в лоб". Вместо этого мы используем могущественного
# "духа", чтобы он извлек из звука его скрытую суть (эмбеддинг), которую
# мы сможем использовать для обучения в следующем квесте.

# --- Акт 1: Подготовка Гримуаров ---

# Призываем двух специалистов по аудио из гримуара transformers:
# 1. Wav2Vec2FeatureExtractor: "Настройщик Слуха", готовит аудио к анализу.
# 2. Wav2Vec2Model: Сам "Дух-Эмпат", который извлекает "эссенцию".
from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model
# "Библиотекарь" для призыва аудио-архивов.
from datasets import load_dataset
# Наш силовой гримуар PyTorch.
import torch

# --- Акт 2: Призыв Магических Существ ---

print("Призываю Духа-Эмпата (Wav2Vec2) и его инструменты...")
# Призываем "Настройщика Слуха", соответствующего модели 'wav2vec2-base'.
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained("facebook/wav2vec2-base")
# Призываем самого "Духа-Эмпата". 'base' - это его средняя по размеру и силе версия.
model = Wav2Vec2Model.from_pretrained("facebook/wav2vec2-base")
# Сразу отправляем "Духа" на Кристалл Маны для быстрой работы.
model.to("cuda")

# --- Акт 3: Призыв Аудио-Послания ---

print("\nПризываю аудио-послание из архива 'librispeech_asr'...")
# Используем наш надежный, проверенный в битвах архив.
# Мы знаем, что для него нужна версия datasets<3.0.0 и `trust_remote_code=True`.
# Мы также знаем, что правильное имя "тома" - 'test'.
dataset = load_dataset("librispeech_asr", "clean", split="test", streaming=True, trust_remote_code=True)
# Берем первый образец из потока.
sample = next(iter(dataset))

# Извлекаем всю аудио-информацию из образца.
audio_input = sample["audio"]
# Разбираем ее на "сырые вибрации" (массив чисел)...
audio_data = audio_input["array"]
# ...и "паспорт" звука (частоту).
sampling_rate = audio_input["sampling_rate"]
print("Послание призвано!")

# --- Акт 4: Ритуал Извлечения Эссенции ---

print("\nДух-Эмпат вслушивается в голос...")
# Передаем сырой звук и его паспорт "Настройщику Слуха".
# Он приводит звук к нужной частоте (16кГц), нормализует
# и упаковывает в Тензор PyTorch ('pt').
inputs = feature_extractor(audio_data, sampling_rate=sampling_rate, return_tensors="pt")
# Отправляем подготовленный звук на Кристалл Маны.
inputs = inputs.to("cuda")

# Используем защитное заклинание `torch.no_grad()` для экономии маны.
with torch.no_grad():
    # "Скармливаем" подготовленный звук "Духу-Эмпату".
    outputs = model(**inputs)

# Из сложного ответа 'outputs' мы извлекаем его самую ценную часть -
# 'last_hidden_state'. Это и есть многомерная "аура", или "эссенция" голоса.
voice_essence = outputs.last_hidden_state
print("Эссенция голоса успешно извлечена!")

# --- Акт 5: Анализ "Ауры" ---

print("\nАнализируем полученную 'ауру' (эмбеддинг):")
# .shape - это заклинание, которое показывает размеры тензора.
# Мы ожидаем увидеть 3 измерения: (батч, длина_последовательности, размер_эссенции).
# Например: (1, 175, 768).
print(f"  Форма ауры: {voice_essence.shape}")

# Для классификации всего аудио нам нужен один "отпечаток", а не 175.
# .mean(dim=1) - это заклинание "усреднения". Оно "схлопывает" измерение
# времени (dim=1), усредняя все 175 "мгновений" в одно.
aggregated_essence = voice_essence.mean(dim=1)
# Теперь форма тензора должна быть (батч, размер_эссенции), например (1, 768).
# Это и есть наш "золотой самородок" - финальный эмбеддинг.
print(f"  Форма единого отпечатка: {aggregated_essence.shape}")